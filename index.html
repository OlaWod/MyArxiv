<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-26T00:00:00Z">2023-10-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Matching Phones and Speech Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gene-Ping Yang, Hao Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning phone types from phone instances has been a long-standing problem,
while still being open. In this work, we revisit this problem in the context of
self-supervised learning, and pose it as the problem of matching cluster
centroids to phone embeddings. We study two key properties that enable
matching, namely, whether cluster centroids of self-supervised representations
reduce the variability of phone instances and respect the relationship among
phones. We then use the matching result to produce pseudo-labels and introduce
a new loss function for improving self-supervised representations. Our
experiments show that the matching result captures the relationship among
phones. Training the new loss function jointly with the regular self-supervised
losses, such as APC and CPC, significantly improves the downstream phone
classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ASRU 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Generation of Artificial Speaker Embeddings through
  Discovery of Principal Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Lux, Pascal Tilli, Sarina Meyer, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customizing voice and speaking style in a speech synthesis system with
intuitive and fine-grained controls is challenging, given that little data with
appropriate labels is available. Furthermore, editing an existing human's voice
also comes with ethical concerns. In this paper, we propose a method to
generate artificial speaker embeddings that cannot be linked to a real human
while offering intuitive and fine-grained control over the voice and speaking
style of the embeddings, without requiring any labels for speaker or style. The
artificial and controllable embeddings can be fed to a speech synthesis system,
conditioned on embeddings of real humans during training, without sacrificing
privacy during inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ISCA Interspeech 2023
  https://www.isca-speech.org/archive/interspeech_2023/lux23_interspeech.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Content-based Controls For Music Large Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liwei Lin, Gus Xia, Junyan Jiang, Yixiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a rapid growth of large-scale language models in
the domain of music audio. Such models enable end-to-end generation of
higher-quality music, and some allow conditioned generation using text
descriptions. However, the control power of text controls on music is
intrinsically limited, as they can only describe music indirectly through
meta-data (such as singers and instruments) or high-level representations (such
as genre and emotion). We aim to further equip the models with direct and
content-based controls on innate music languages such as pitch, chords and drum
track. To this end, we contribute Coco-Mulla, a content-based control method
for music large language modeling. It uses a parameter-efficient fine-tuning
(PEFT) method tailored for Transformer-based audio models. Experiments show
that our approach achieved high-quality music generation with low-resource
semi-supervised learning, tuning with less than 4% parameters compared to the
original model and training on a small dataset with fewer than 300 songs.
Moreover, our approach enables effective content-based controls, and we
illustrate the control power via chords and rhythms, two of the most salient
features of music audio. Furthermore, we show that by combining content-based
controls and text descriptions, our system achieves flexible music variation
generation and style transfer. Our source codes and demos are available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single channel speech enhancement by colored spectrograms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sania Gul, Muhammad Salman Khan, Muhammad Fazeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech enhancement concerns the processes required to remove unwanted
background sounds from the target speech to improve its quality and
intelligibility. In this paper, a novel approach for single-channel speech
enhancement is presented, using colored spectrograms. We propose the use of a
deep neural network (DNN) architecture adapted from the pix2pix generative
adversarial network (GAN) and train it over colored spectrograms of speech to
denoise them. After denoising, the colors of spectrograms are translated to
magnitudes of short-time Fourier transform (STFT) using a shallow regression
neural network. These estimated STFT magnitudes are later combined with the
noisy phases to obtain an enhanced speech. The results show an improvement of
almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1%
in the short-term objective intelligibility (STOI) over the unprocessed noisy
data. The gain in quality and intelligibility over the unprocessed signal is
almost equal to the gain achieved by the baseline methods used for comparison
with the proposed model, but at a much reduced computational cost. The proposed
solution offers a comparative PESQ score at almost 10 times reduced
computational cost than a similar baseline model that has generated the highest
PESQ score trained on grayscaled spectrograms, while it provides only a 1%
deficit in STOI at 28 times reduced computational cost when compared to another
baseline system based on convolutional neural network-GAN (CNN-GAN) that
produces the most intelligible speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Neonatal Chest Sound Separation using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yi Poh, Ethan Grooby, Kenneth Tan, Lindsay Zhou, Arrabella King, Ashwin Ramanathan, Atul Malhotra, Mehrtash Harandi, Faezeh Marzbanrad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auscultation for neonates is a simple and non-invasive method of providing
diagnosis for cardiovascular and respiratory disease. Such diagnosis often
requires high-quality heart and lung sounds to be captured during auscultation.
However, in most cases, obtaining such high-quality sounds is non-trivial due
to the chest sounds containing a mixture of heart, lung, and noise sounds. As
such, additional preprocessing is needed to separate the chest sounds into
heart and lung sounds. This paper proposes a novel deep-learning approach to
separate such chest sounds into heart and lung sounds. Inspired by the
Conv-TasNet model, the proposed model has an encoder, decoder, and mask
generator. The encoder consists of a 1D convolution model and the decoder
consists of a transposed 1D convolution. The mask generator is constructed
using stacked 1D convolutions and transformers. The proposed model outperforms
previous methods in terms of objective distortion measures by 2.01 dB to 5.06
dB in the artificial dataset, as well as computation time, with at least a
17-time improvement. Therefore, our proposed model could be a suitable
preprocessing step for any phonocardiogram-based health monitoring system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Multi-Speaker Expressive <span class="highlight-title">Speech Synthesis</span> via Semi-supervised
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfa Zhu, Yuke Li, Yi Lei, Ning Jiang, Guoqing Zhao, <span class="highlight-author">Lei Xie</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to build an expressive TTS system for multi-speakers,
synthesizing a target speaker's speech with multiple styles and emotions. To
this end, we propose a novel contrastive learning-based TTS approach to
transfer style and emotion across speakers. Specifically, we construct
positive-negative sample pairs at both utterance and category (such as
emotion-happy or style-poet or speaker A) levels and leverage contrastive
learning to better extract disentangled style, emotion, and speaker
representations from speech. Furthermore, we introduce a semi-supervised
training strategy to the proposed approach to effectively leverage multi-domain
data, including style-labeled data, emotion-labeled data, and unlabeled data.
We integrate the learned representations into an improved VITS model, enabling
it to synthesize expressive speech with diverse styles and emotions for a
target speaker. Experiments on multi-domain data demonstrate the good design of
our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A weighted-variance variational autoencoder model for speech enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Golmakani, Mostafa Sadeghi, Xavier Alameda-Pineda, Romain Serizel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address speech enhancement based on variational autoencoders, which
involves learning a speech prior distribution in the time-frequency (TF)
domain. A zero-mean complex-valued Gaussian distribution is usually assumed for
the generative model, where the speech information is encoded in the variance
as a function of a latent variable. In contrast to this commonly used approach,
we propose a weighted variance generative model, where the contribution of each
spectrogram time-frame in parameter learning is weighted. We impose a Gamma
prior distribution on the weights, which would effectively lead to a Student's
t-distribution instead of Gaussian for speech generative modeling. We develop
efficient training and speech enhancement algorithms based on the proposed
generative model. Our experimental results on spectrogram auto-encoding and
speech enhancement demonstrate the effectiveness and robustness of the proposed
approach compared to the standard unweighted variance model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TF-SepNet: An Efficient 1D Kernel Design in CNNs for Low-Complexity
  Acoustic Scene Classification <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqiang Cai, Peihong Zhang, Shengchen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies focus on developing efficient systems for acoustic scene
classification (ASC) using convolutional neural networks (CNNs), which
typically consist of consecutive kernels. This paper highlights the benefits of
using separate kernels as a more powerful and efficient design approach in ASC
tasks. Inspired by the time-frequency nature of audio signals, we propose
TF-SepNet, a CNN architecture that separates the feature processing along the
time and frequency dimensions. Features resulted from the separate paths are
then merged by channels and directly forwarded to the classifier. Instead of
the conventional two dimensional (2D) kernel, TF-SepNet incorporates one
dimensional (1D) kernels to reduce the computational costs. Experiments have
been conducted using the TAU Urban Acoustic Scene 2022 Mobile development
dataset. The results show that TF-SepNet outperforms similar state-of-the-arts
that use consecutive kernels. A further investigation reveals that the separate
kernels lead to a larger effective receptive field (ERF), which enables
TF-SepNet to capture more time-frequency features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2024 IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Matching Phones and Speech Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gene-Ping Yang, Hao Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning phone types from phone instances has been a long-standing problem,
while still being open. In this work, we revisit this problem in the context of
self-supervised learning, and pose it as the problem of matching cluster
centroids to phone embeddings. We study two key properties that enable
matching, namely, whether cluster centroids of self-supervised representations
reduce the variability of phone instances and respect the relationship among
phones. We then use the matching result to produce pseudo-labels and introduce
a new loss function for improving self-supervised representations. Our
experiments show that the matching result captures the relationship among
phones. Training the new loss function jointly with the regular self-supervised
losses, such as APC and CPC, significantly improves the downstream phone
classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ASRU 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Generation of Artificial Speaker Embeddings through
  Discovery of Principal Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Lux, Pascal Tilli, Sarina Meyer, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customizing voice and speaking style in a speech synthesis system with
intuitive and fine-grained controls is challenging, given that little data with
appropriate labels is available. Furthermore, editing an existing human's voice
also comes with ethical concerns. In this paper, we propose a method to
generate artificial speaker embeddings that cannot be linked to a real human
while offering intuitive and fine-grained control over the voice and speaking
style of the embeddings, without requiring any labels for speaker or style. The
artificial and controllable embeddings can be fed to a speech synthesis system,
conditioned on embeddings of real humans during training, without sacrificing
privacy during inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ISCA Interspeech 2023
  https://www.isca-speech.org/archive/interspeech_2023/lux23_interspeech.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The IMS Toucan System for the Blizzard Challenge 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Lux, Julia Koch, Sarina Meyer, Thomas Bott, Nadja Schauffler, Pavel Denisov, Antje Schweitzer, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For our contribution to the Blizzard Challenge 2023, we improved on the
system we submitted to the Blizzard Challenge 2021. Our approach entails a
rule-based text-to-phoneme processing system that includes rule-based
disambiguation of homographs in the French language. It then transforms the
phonemes to spectrograms as intermediate representations using a fast and
efficient non-autoregressive synthesis architecture based on Conformer and
Glow. A GAN based neural vocoder that combines recent state-of-the-art
approaches converts the spectrogram to the final wave. We carefully designed
the data processing, training, and inference procedures for the challenge data.
Our system identifier is G. Open source code and demo are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the Blizzard Challenge Workshop 2023, colocated with the
  Speech Synthesis Workshop 2023, a sattelite event of the Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialect Adaptation and Data Augmentation for Low-Resource ASR: TalTech
  Systems for the MADASR 2023 Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanel Alumäe, Jiaming Kong, Daniil Robnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes Tallinn University of Technology (TalTech) systems
developed for the ASRU MADASR 2023 Challenge. The challenge focuses on
automatic speech recognition of dialect-rich Indian languages with limited
training audio and text data. TalTech participated in two tracks of the
challenge: Track 1 that allowed using only the provided training data and Track
3 which allowed using additional audio data. In both tracks, we relied on
wav2vec2.0 models. Our methodology diverges from the traditional procedure of
finetuning pretrained wav2vec2.0 models in two key points: firstly, through the
implementation of the aligned data augmentation technique to enhance the
linguistic diversity of the training data, and secondly, via the application of
deep prefix tuning for dialect adaptation of wav2vec2.0 models. In both tracks,
our approach yielded significant improvements over the provided baselines,
achieving the lowest word error rates across all participating teams.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-preserving Representation Learning for Speech Understanding <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Tran, Mohammad Soleymani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing privacy-preserving speech representation learning methods target a
single application domain. In this paper, we present a novel framework to
anonymize utterance-level speech embeddings generated by pre-trained encoders
and show its effectiveness for a range of speech classification tasks.
Specifically, given the representations from a pre-trained encoder, we train a
Transformer to estimate the representations for the same utterances spoken by
other speakers. During inference, the extracted representations can be
converted into different identities to preserve privacy. We compare the results
with the voice anonymization baselines from the VoicePrivacy 2022 challenge. We
evaluate our framework on speaker identification for privacy and emotion
recognition, depression classification, and intent classification for utility.
Our method outperforms the baselines on privacy and utility in paralinguistic
tasks and achieves comparable performance for intent classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Content-based Controls For Music Large Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liwei Lin, Gus Xia, Junyan Jiang, Yixiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a rapid growth of large-scale language models in
the domain of music audio. Such models enable end-to-end generation of
higher-quality music, and some allow conditioned generation using text
descriptions. However, the control power of text controls on music is
intrinsically limited, as they can only describe music indirectly through
meta-data (such as singers and instruments) or high-level representations (such
as genre and emotion). We aim to further equip the models with direct and
content-based controls on innate music languages such as pitch, chords and drum
track. To this end, we contribute Coco-Mulla, a content-based control method
for music large language modeling. It uses a parameter-efficient fine-tuning
(PEFT) method tailored for Transformer-based audio models. Experiments show
that our approach achieved high-quality music generation with low-resource
semi-supervised learning, tuning with less than 4% parameters compared to the
original model and training on a small dataset with fewer than 300 songs.
Moreover, our approach enables effective content-based controls, and we
illustrate the control power via chords and rhythms, two of the most salient
features of music audio. Furthermore, we show that by combining content-based
controls and text descriptions, our system achieves flexible music variation
generation and style transfer. Our source codes and demos are available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single channel speech enhancement by colored spectrograms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sania Gul, Muhammad Salman Khan, Muhammad Fazeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech enhancement concerns the processes required to remove unwanted
background sounds from the target speech to improve its quality and
intelligibility. In this paper, a novel approach for single-channel speech
enhancement is presented, using colored spectrograms. We propose the use of a
deep neural network (DNN) architecture adapted from the pix2pix generative
adversarial network (GAN) and train it over colored spectrograms of speech to
denoise them. After denoising, the colors of spectrograms are translated to
magnitudes of short-time Fourier transform (STFT) using a shallow regression
neural network. These estimated STFT magnitudes are later combined with the
noisy phases to obtain an enhanced speech. The results show an improvement of
almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1%
in the short-term objective intelligibility (STOI) over the unprocessed noisy
data. The gain in quality and intelligibility over the unprocessed signal is
almost equal to the gain achieved by the baseline methods used for comparison
with the proposed model, but at a much reduced computational cost. The proposed
solution offers a comparative PESQ score at almost 10 times reduced
computational cost than a similar baseline model that has generated the highest
PESQ score trained on grayscaled spectrograms, while it provides only a 1%
deficit in STOI at 28 times reduced computational cost when compared to another
baseline system based on convolutional neural network-GAN (CNN-GAN) that
produces the most intelligible speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Neonatal Chest Sound Separation using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yi Poh, Ethan Grooby, Kenneth Tan, Lindsay Zhou, Arrabella King, Ashwin Ramanathan, Atul Malhotra, Mehrtash Harandi, Faezeh Marzbanrad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auscultation for neonates is a simple and non-invasive method of providing
diagnosis for cardiovascular and respiratory disease. Such diagnosis often
requires high-quality heart and lung sounds to be captured during auscultation.
However, in most cases, obtaining such high-quality sounds is non-trivial due
to the chest sounds containing a mixture of heart, lung, and noise sounds. As
such, additional preprocessing is needed to separate the chest sounds into
heart and lung sounds. This paper proposes a novel deep-learning approach to
separate such chest sounds into heart and lung sounds. Inspired by the
Conv-TasNet model, the proposed model has an encoder, decoder, and mask
generator. The encoder consists of a 1D convolution model and the decoder
consists of a transposed 1D convolution. The mask generator is constructed
using stacked 1D convolutions and transformers. The proposed model outperforms
previous methods in terms of objective distortion measures by 2.01 dB to 5.06
dB in the artificial dataset, as well as computation time, with at least a
17-time improvement. Therefore, our proposed model could be a suitable
preprocessing step for any phonocardiogram-based health monitoring system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Multi-Speaker Expressive <span class="highlight-title">Speech Synthesis</span> via Semi-supervised
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfa Zhu, Yuke Li, Yi Lei, Ning Jiang, Guoqing Zhao, <span class="highlight-author">Lei Xie</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to build an expressive TTS system for multi-speakers,
synthesizing a target speaker's speech with multiple styles and emotions. To
this end, we propose a novel contrastive learning-based TTS approach to
transfer style and emotion across speakers. Specifically, we construct
positive-negative sample pairs at both utterance and category (such as
emotion-happy or style-poet or speaker A) levels and leverage contrastive
learning to better extract disentangled style, emotion, and speaker
representations from speech. Furthermore, we introduce a semi-supervised
training strategy to the proposed approach to effectively leverage multi-domain
data, including style-labeled data, emotion-labeled data, and unlabeled data.
We integrate the learned representations into an improved VITS model, enabling
it to synthesize expressive speech with diverse styles and emotions for a
target speaker. Experiments on multi-domain data demonstrate the good design of
our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Five ways to recover the symbol of a non-binary localization operator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Halvdansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Five constructive methods for recovering the symbol of a time-frequency
localization operator with non-binary symbol are presented, two based on
earlier work and three novel methods. For the two derivative methods which have
previously been applied to binary symbols, we propose a changed symbol
estimator and provide additional estimates that show how we can recover
non-binary symbols. The three novel methods each have their own advantages and
are all applicable to non-binary symbols. Two of them rely on prescribing the
input of the localization operator and examining the output, allowing for
targeting of the part of the symbol one wishes to recover while the last one
relies on spectral information about the operator. All five methods are also
implemented numerically and evaluated with the code available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A weighted-variance variational autoencoder model for speech enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Golmakani, Mostafa Sadeghi, Xavier Alameda-Pineda, Romain Serizel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address speech enhancement based on variational autoencoders, which
involves learning a speech prior distribution in the time-frequency (TF)
domain. A zero-mean complex-valued Gaussian distribution is usually assumed for
the generative model, where the speech information is encoded in the variance
as a function of a latent variable. In contrast to this commonly used approach,
we propose a weighted variance generative model, where the contribution of each
spectrogram time-frame in parameter learning is weighted. We impose a Gamma
prior distribution on the weights, which would effectively lead to a Student's
t-distribution instead of Gaussian for speech generative modeling. We develop
efficient training and speech enhancement algorithms based on the proposed
generative model. Our experimental results on spectrogram auto-encoding and
speech enhancement demonstrate the effectiveness and robustness of the proposed
approach compared to the standard unweighted variance model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recycle-and-Distill: Universal Compression Strategy for
  Transformer-based Speech SSL Models with Attention Map Reusing and Masking
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangwook Jang, Sungnyun Kim, Se-Young Yun, Hoirin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based speech self-supervised learning (SSL) models, such as
HuBERT, show surprising performance in various speech processing tasks.
However, huge number of parameters in speech SSL models necessitate the
compression to a more compact model for wider usage in academia or small
companies. In this study, we suggest to reuse attention maps across the
Transformer layers, so as to remove key and query parameters while retaining
the number of layers. Furthermore, we propose a novel masking distillation
strategy to improve the student model's speech representation quality. We
extend the distillation loss to utilize both masked and unmasked speech frames
to fully leverage the teacher model's high-quality representation. Our
universal compression strategy yields the student model that achieves phoneme
error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB
benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of Interspeech 2023. Code URL:
  https://github.com/sungnyun/ARMHuBERT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TF-SepNet: An Efficient 1D Kernel Design in CNNs for Low-Complexity
  Acoustic Scene Classification <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqiang Cai, Peihong Zhang, Shengchen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies focus on developing efficient systems for acoustic scene
classification (ASC) using convolutional neural networks (CNNs), which
typically consist of consecutive kernels. This paper highlights the benefits of
using separate kernels as a more powerful and efficient design approach in ASC
tasks. Inspired by the time-frequency nature of audio signals, we propose
TF-SepNet, a CNN architecture that separates the feature processing along the
time and frequency dimensions. Features resulted from the separate paths are
then merged by channels and directly forwarded to the classifier. Instead of
the conventional two dimensional (2D) kernel, TF-SepNet incorporates one
dimensional (1D) kernels to reduce the computational costs. Experiments have
been conducted using the TAU Urban Acoustic Scene 2022 Mobile development
dataset. The results show that TF-SepNet outperforms similar state-of-the-arts
that use consecutive kernels. A further investigation reveals that the separate
kernels lead to a larger effective receptive field (ERF), which enables
TF-SepNet to capture more time-frequency features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2024 IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-25T00:00:00Z">2023-10-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArTST: Arabic Text and Speech Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hawau Olamide Toyin, Amirbek Djanibekov, Ajinkya Kulkarni, Hanan Aldarmaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ArTST, a pre-trained Arabic text and speech transformer for
supporting open-source speech technologies for the Arabic language. The model
architecture follows the unified-modal framework, SpeechT5, that was recently
released for English, and is focused on Modern Standard Arabic (MSA), with
plans to extend the model for dialectal and code-switched Arabic in future
editions. We pre-trained the model from scratch on MSA speech and text data,
and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR),
Text-To-Speech synthesis (TTS), and spoken dialect identification. In our
experiments comparing ArTST with SpeechT5, as well as with previously reported
results in these tasks, ArTST performs on a par with or exceeding the current
state-of-the-art in all three tasks. Moreover, we find that our pre-training is
conducive for generalization, which is particularly evident in the low-resource
TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models
are released for research use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure, SIGARAB ArabicNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Back Transcription as a Method for Evaluating Robustness of Natural
  Language Understanding Models to Speech Recognition Errors <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Kubis, Paweł Skórzewski, Marcin Sowański, Tomasz Ziętkiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a spoken dialogue system, an NLU model is preceded by a speech recognition
system that can deteriorate the performance of natural language understanding.
This paper proposes a method for investigating the impact of speech recognition
errors on the performance of natural language understanding models. The
proposed method combines the back transcription procedure with a fine-grained
technique for categorizing the errors that affect the performance of NLU
models. The method relies on the usage of synthesized speech for NLU
evaluation. We show that the use of synthesized speech in place of audio
recording does not change the outcomes of the presented technique in a
significant way.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Processing Neural Network Architecture For Hearing Loss
  Compensation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Szymon Drgas, Lars Bramsløw, Archontis Politis, Gaurav Naithani, Tuomas Virtanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes neural networks for compensating sensorineural hearing
loss. The aim of the hearing loss compensation task is to transform a speech
signal to increase speech intelligibility after further processing by a person
with a hearing impairment, which is modeled by a hearing loss model. We propose
an interpretable model called dynamic processing network, which has a structure
similar to band-wise dynamic compressor. The network is differentiable, and
therefore allows to learn its parameters to maximize speech intelligibility.
More generic models based on convolutional layers were tested as well. The
performance of the tested architectures was assessed using spectro-temporal
objective index (STOI) with hearing-threshold noise and hearing aid speech
intelligibility (HASPI) metrics. The dynamic processing network gave a
significant improvement of STOI and HASPI in comparison to popular compressive
gain prescription rule Camfit. A large enough convolutional network could
outperform the interpretable model with the cost of larger computational load.
Finally, a combination of the dynamic processing network with convolutional
neural network gave the best results in terms of STOI and HASPI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Approach for Object Based Audio Broadcasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Hasanabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object Based Audio (OBA) provides a new kind of audio experience, delivered
to the audience to personalize and customize their experience of listening and
to give them choice of what and how to hear their audio content. OBA can be
applied to different platforms such as broadcasting, streaming and cinema
sound. This paper presents a novel approach for creating object-based audio on
the production side. The approach here presents Sample-by-Sample Object Based
Audio (SSOBA) embedding. SSOBA places audio object samples in such a way that
allows audiences to easily individualize their chosen audio sources according
to their interests and needs. SSOBA is an extra service and not an alternative,
so it is also compliant with legacy audio players. The biggest advantage of
SSOBA is that it does not require any special additional hardware in the
broadcasting chain and it is therefore easy to implement and equip legacy
players and decoders with enhanced ability. Input audio objects, number of
output channels and sampling rates are three important factors affecting SSOBA
performance and specifying it to be lossless or lossy. SSOBA adopts
interpolation at the decoder side to compensate for eliminated samples. Both
subjective and objective experiments are carried out to evaluate the output
results at each step. MUSHRA subjective experiments conducted after the
encoding step shows good-quality performance of SSOBA with up to five objects.
SNR measurements and objective experiments, performed after decoding and
interpolation, show significant successful recovery and separation of audio
objects. Experimental results show that a minimum sampling rate of 96 kHz is
indicated to encode up to five objects in a Stereo-mode channel to acquire good
subjective and objective results simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ABU Technical Review Journal 2020/9</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Generative Pre-training for Speech with Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Liu, Matt Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, <span class="highlight-author">Wei-Ning Hsu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have gained more and more attention in recent years for
their remarkable success in tasks that required estimating and sampling data
distribution to generate high-fidelity synthetic data. In speech,
text-to-speech synthesis and neural vocoder are good examples where generative
models have shined. While generative models have been applied to different
applications in speech, there exists no general-purpose generative model that
models speech directly. In this work, we take a step toward this direction by
showing a single pre-trained generative model can be adapted to different
downstream tasks with strong performance. Specifically, we pre-trained a
generative model, named SpeechFlow, on 60k hours of untranscribed speech with
Flow Matching and masked conditions. Experiment results show the pre-trained
generative model can be fine-tuned with task-specific data to match or surpass
existing expert models on speech enhancement, separation, and synthesis. Our
work suggested a foundational model for generation tasks in speech can be built
with generative pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style
  Transfer and Multi-Track Function Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Zhao, Gus Xia, Ye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose AccoMontage-3, a symbolic music automation system capable of
generating multi-track, full-band accompaniment based on the input of a lead
melody with chords (i.e., a lead sheet). The system contains three modular
components, each modelling a vital aspect of full-band composition. The first
component is a piano arranger that generates piano accompaniment for the lead
sheet by transferring texture styles to the chords using latent chord-texture
disentanglement and heuristic retrieval of texture donors. The second component
orchestrates the piano accompaniment score into full-band arrangement according
to the orchestration style encoded by individual track functions. The third
component, which connects the previous two, is a prior model characterizing the
global structure of orchestration style over the whole piece of music. From end
to end, the system learns to generate full-band accompaniment in a
self-supervised fashion, applying style transfer at two levels of polyphonic
composition: texture and orchestration. Experiments show that our system
outperforms the baselines significantly, and the modular design offers
effective controls in a musically meaningful way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Streaming Speech-to-Avatar Synthesis <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejas S. Prabhune, Peter Wu, Bohan Yu, Gopala K. Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Streaming speech-to-avatar synthesis creates real-time animations for a
virtual character from audio data. Accurate avatar representations of speech
are important for the visualization of sound in linguistics, phonetics, and
phonology, visual feedback to assist second language acquisition, and virtual
embodiment for paralyzed patients. Previous works have highlighted the
capability of deep articulatory inversion to perform high-quality avatar
animation using electromagnetic articulography (EMA) features. However, these
models focus on offline avatar synthesis with recordings rather than real-time
audio, which is necessary for live avatar visualization or embodiment. To
address this issue, we propose a method using articulatory inversion for
streaming high quality facial and inner-mouth avatar animation from real-time
audio. Our approach achieves 130ms average streaming latency for every 0.1
seconds of audio with a 0.792 correlation with ground truth articulations.
Finally, we show generated mouth and tongue animations to demonstrate the
efficacy of our methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Repeatable Speech Embeddings Using An Intra-class Correlation
  Regularizer <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Zhang, Suren Jayasuriya, Visar Berisha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A good supervised embedding for a specific machine learning task is only
sensitive to changes in the label of interest and is invariant to other
confounding factors. We leverage the concept of repeatability from measurement
theory to describe this property and propose to use the intra-class correlation
coefficient (ICC) to evaluate the repeatability of embeddings. We then propose
a novel regularizer, the ICC regularizer, as a complementary component for
contrastive losses to guide deep neural networks to produce embeddings with
higher repeatability. We use simulated data to explain why the ICC regularizer
works better on minimizing the intra-class variance than the contrastive loss
alone. We implement the ICC regularizer and apply it to three speech tasks:
speaker verification, voice style conversion, and a clinical application for
detecting dysphonic voice. The experimental results demonstrate that adding an
ICC regularizer can improve the repeatability of learned embeddings compared to
only using the contrastive loss; further, these embeddings lead to improved
performance in these downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Panning on Non-Equidistant Loudspeakers with Direct Sound Level
  Compensation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Hendrik Hanschke, Daniel Arteaga, Giulio Cengarle, Joshua Lando, Mark R. P. Thomas, Alan Seefeldt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loudspeaker rendering techniques that create phantom sound sources often
assume an equidistant loudspeaker layout. Typical home setups might not fulfill
this condition as loudspeakers deviate from canonical positions, thus requiring
a corresponding calibration. The standard approach is to compensate for delays
and to match the loudness of each loudspeaker at the listener's location.It was
found that a shift of the phantom image occurs when this calibration procedure
is applied and one of a pair of loudspeakers is significantly closer to the
listener than the other. In this paper, a novel approach to panning on
non-equidistant loudspeaker layouts is presented whereby the panning position
is governed by the direct sound and the perceived loudness is governed by the
full impulse response. Subjective listening tests are presented that validate
the approach and quantify the perceived effect of the compensation. In a setup
where the standard calibration leads to an average error of 10 degrees, the
proposed direct sound compensation largely returns the phantom source to its
intended position.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. Accepted for presentation in AES Convention 155 (2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System <span class="chip">EMNLP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11069v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11069v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Waheed, Bashar Talafha, Peter Suvellin, AbdelRahim Elmadany, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arabic is a complex language with many varieties and dialects spoken by over
450 millions all around the world. Due to the linguistic diversity and
variations, it is challenging to build a robust and generalized ASR system for
Arabic. In this work, we address this gap by developing and demoing a system,
dubbed VoxArabica, for dialect identification (DID) as well as automatic speech
recognition (ASR) of Arabic. We train a wide range of models such as HuBERT
(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR
tasks. Our DID models are trained to identify 17 different dialects in addition
to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.
Additionally, for the remaining dialects in ASR, we provide the option to
choose various models such as Whisper and MMS in a zero-shot setting. We
integrate these models into a single web interface with diverse features such
as audio recording, file upload, model selection, and the option to raise flags
for incorrect outputs. Overall, we believe VoxArabica will be useful for a wide
range of audiences concerned with Arabic research. Our system is currently
running at https://cdce-206-12-100-168.ngrok.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ArabicNLP conference co-located with EMNLP'23. First
  three authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global, and Local Optimization Beamforming for Broadband Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04921v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04921v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Goudarzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an alternative energy function for Global Optimization
(GO) beamforming, tailored to acoustic broadband sources. Given, that
properties such as the source location, multipole rotation, or flow conditions
are parameterized over the frequency, a CSM-fitting can be performed for all
frequencies at once. A numerical analysis shows that the nonlinear energy
function for the standard GO problem is equivalent to the source's Point Spread
Function (PSF) and contains local minima at the grating- and side lobes'
locations. The energy function is improved with the proposed broadband energy,
as it averages the PSF. Further, it simplifies the process of identifying
sources and reconstructing their spectra from the results. The paper shows that
the method is superior on synthetic monopoles compared to standard GO and
CLEAN-SC. For real-world data the results of the proposed method and CLEAN-SC
are similar, and outperform standard GO. The main difference is that source
assumption violations cause noisy maps for CLEAN-SC and cause wrong spectral
estimations of the proposed method. By using reasonable initial values, the GO
problem reduces to a Local Optimization problem with similar results. Further,
the proposed method is able to identify synthetic multipoles with different
pole amplitudes and unknown pole rotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to JASA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Attention always needed? A Case Study on Language Identification from
  Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03427v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03427v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atanu Mandal, Santanu Pal, Indranil Dutta, Mahidas Bhattacharya, Sudip Kumar Naskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Identification (LID) is a crucial preliminary process in the field
of Automatic Speech Recognition (ASR) that involves the identification of a
spoken language from audio samples. Contemporary systems that can process
speech in multiple languages require users to expressly designate one or more
languages prior to utilization. The LID task assumes a significant role in
scenarios where ASR systems are unable to comprehend the spoken language in
multilingual settings, leading to unsuccessful speech recognition outcomes. The
present study introduces convolutional recurrent neural network (CRNN) based
LID, designed to operate on the Mel-frequency Cepstral Coefficient (MFCC)
characteristics of audio samples. Furthermore, we replicate certain
state-of-the-art methodologies, specifically the Convolutional Neural Network
(CNN) and Attention-based Convolutional Recurrent Neural Network (CRNN with
attention), and conduct a comparative analysis with our CRNN-based approach. We
conducted comprehensive evaluations on thirteen distinct Indian languages and
our model resulted in over 98\% classification accuracy. The LID model exhibits
high-performance levels ranging from 97% to 100% for languages that are
linguistically similar. The proposed LID model exhibits a high degree of
extensibility to additional languages and demonstrates a strong resistance to
noise, achieving 91.2% accuracy in a noisy setting when applied to a European
Language (EU) dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Natural Language Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video
  Understanding <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02858v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02858v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhang, Xin Li, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Video-LLaMA a multi-modal framework that empowers Large Language
Models (LLMs) with the capability of understanding both visual and auditory
content in the video. Video-LLaMA bootstraps cross-modal training from the
frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike
previous works that complement LLMs to process the visual or audio signals
only, Video-LLaMA enables video comprehension by tackling two challenges: (1)
capturing the temporal changes in visual scenes, (2) integrating audio-visual
signals. To counter the first challenge, we propose a Video Q-former to
assemble a pre-trained image encoder into our video encoder and introduce a
video-to-text generation task to learn video-language correspondence. For the
second challenge, we leverage ImageBind, a universal embedding model aligning
multiple modalities, as the pre-trained audio encoder and introduce an Audio
Q-former on top of ImageBind to learn reasonable auditory query embeddings for
the LLM module. To align the output of both visual and audio encoders with
LLM's embedding space, we first train Video-LLaMA on massive
video/image-caption pairs and then tune our model with visual-instruction
datasets of moderate amount but higher quality. We found Video-LLaMA shows the
ability to perceive and comprehend video content and generate meaningful
responses grounded in the visual and auditory information presented in the
videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2023's demo track; Code, Pretrained Model, and
  Dataset: https://github.com/DAMO-NLP-SG/Video-LLaMA</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArTST: Arabic Text and Speech Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hawau Olamide Toyin, Amirbek Djanibekov, Ajinkya Kulkarni, Hanan Aldarmaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ArTST, a pre-trained Arabic text and speech transformer for
supporting open-source speech technologies for the Arabic language. The model
architecture follows the unified-modal framework, SpeechT5, that was recently
released for English, and is focused on Modern Standard Arabic (MSA), with
plans to extend the model for dialectal and code-switched Arabic in future
editions. We pre-trained the model from scratch on MSA speech and text data,
and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR),
Text-To-Speech synthesis (TTS), and spoken dialect identification. In our
experiments comparing ArTST with SpeechT5, as well as with previously reported
results in these tasks, ArTST performs on a par with or exceeding the current
state-of-the-art in all three tasks. Moreover, we find that our pre-training is
conducive for generalization, which is particularly evident in the low-resource
TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models
are released for research use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure, SIGARAB ArabicNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Back Transcription as a Method for Evaluating Robustness of Natural
  Language Understanding Models to Speech Recognition Errors <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Kubis, Paweł Skórzewski, Marcin Sowański, Tomasz Ziętkiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a spoken dialogue system, an NLU model is preceded by a speech recognition
system that can deteriorate the performance of natural language understanding.
This paper proposes a method for investigating the impact of speech recognition
errors on the performance of natural language understanding models. The
proposed method combines the back transcription procedure with a fine-grained
technique for categorizing the errors that affect the performance of NLU
models. The method relies on the usage of synthesized speech for NLU
evaluation. We show that the use of synthesized speech in place of audio
recording does not change the outcomes of the presented technique in a
significant way.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Processing Neural Network Architecture For Hearing Loss
  Compensation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Szymon Drgas, Lars Bramsløw, Archontis Politis, Gaurav Naithani, Tuomas Virtanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes neural networks for compensating sensorineural hearing
loss. The aim of the hearing loss compensation task is to transform a speech
signal to increase speech intelligibility after further processing by a person
with a hearing impairment, which is modeled by a hearing loss model. We propose
an interpretable model called dynamic processing network, which has a structure
similar to band-wise dynamic compressor. The network is differentiable, and
therefore allows to learn its parameters to maximize speech intelligibility.
More generic models based on convolutional layers were tested as well. The
performance of the tested architectures was assessed using spectro-temporal
objective index (STOI) with hearing-threshold noise and hearing aid speech
intelligibility (HASPI) metrics. The dynamic processing network gave a
significant improvement of STOI and HASPI in comparison to popular compressive
gain prescription rule Camfit. A large enough convolutional network could
outperform the interpretable model with the cost of larger computational load.
Finally, a combination of the dynamic processing network with convolutional
neural network gave the best results in terms of STOI and HASPI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Approach for Object Based Audio Broadcasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Hasanabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object Based Audio (OBA) provides a new kind of audio experience, delivered
to the audience to personalize and customize their experience of listening and
to give them choice of what and how to hear their audio content. OBA can be
applied to different platforms such as broadcasting, streaming and cinema
sound. This paper presents a novel approach for creating object-based audio on
the production side. The approach here presents Sample-by-Sample Object Based
Audio (SSOBA) embedding. SSOBA places audio object samples in such a way that
allows audiences to easily individualize their chosen audio sources according
to their interests and needs. SSOBA is an extra service and not an alternative,
so it is also compliant with legacy audio players. The biggest advantage of
SSOBA is that it does not require any special additional hardware in the
broadcasting chain and it is therefore easy to implement and equip legacy
players and decoders with enhanced ability. Input audio objects, number of
output channels and sampling rates are three important factors affecting SSOBA
performance and specifying it to be lossless or lossy. SSOBA adopts
interpolation at the decoder side to compensate for eliminated samples. Both
subjective and objective experiments are carried out to evaluate the output
results at each step. MUSHRA subjective experiments conducted after the
encoding step shows good-quality performance of SSOBA with up to five objects.
SNR measurements and objective experiments, performed after decoding and
interpolation, show significant successful recovery and separation of audio
objects. Experimental results show that a minimum sampling rate of 96 kHz is
indicated to encode up to five objects in a Stereo-mode channel to acquire good
subjective and objective results simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ABU Technical Review Journal 2020/9</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> UniX-Encoder: A Universal $X$-Channel Speech Encoder for Ad-Hoc
  Microphone Array Speech Processing <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zili Huang, Yiwen Shao, Shi-Xiong Zhang, <span class="highlight-author">Dong Yu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The speech field is evolving to solve more challenging scenarios, such as
multi-channel recordings with multiple simultaneous talkers. Given the many
types of microphone setups out there, we present the UniX-Encoder. It's a
universal encoder designed for multiple tasks, and worked with any microphone
array, in both solo and multi-talker environments. Our research enhances
previous multi-channel speech processing efforts in four key areas: 1)
Adaptability: Contrasting traditional models constrained to certain microphone
array configurations, our encoder is universally compatible. 2) Multi-Task
Capability: Beyond the single-task focus of previous systems, UniX-Encoder acts
as a robust upstream model, adeptly extracting features for diverse tasks
including ASR and speaker recognition. 3) Self-Supervised Training: The encoder
is trained without requiring labeled multi-channel data. 4) End-to-End
Integration: In contrast to models that first beamform then process
single-channels, our encoder offers an end-to-end solution, bypassing explicit
beamforming or separation. To validate its effectiveness, we tested the
UniX-Encoder on a synthetic multi-channel dataset from the LibriSpeech corpus.
Across tasks like speech recognition and speaker diarization, our encoder
consistently outperformed combinations like the WavLM model with the BeamformIt
frontend.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Generative Pre-training for Speech with Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Liu, Matt Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, <span class="highlight-author">Wei-Ning Hsu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have gained more and more attention in recent years for
their remarkable success in tasks that required estimating and sampling data
distribution to generate high-fidelity synthetic data. In speech,
text-to-speech synthesis and neural vocoder are good examples where generative
models have shined. While generative models have been applied to different
applications in speech, there exists no general-purpose generative model that
models speech directly. In this work, we take a step toward this direction by
showing a single pre-trained generative model can be adapted to different
downstream tasks with strong performance. Specifically, we pre-trained a
generative model, named SpeechFlow, on 60k hours of untranscribed speech with
Flow Matching and masked conditions. Experiment results show the pre-trained
generative model can be fine-tuned with task-specific data to match or surpass
existing expert models on speech enhancement, separation, and synthesis. Our
work suggested a foundational model for generation tasks in speech can be built
with generative pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style
  Transfer and Multi-Track Function Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Zhao, Gus Xia, Ye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose AccoMontage-3, a symbolic music automation system capable of
generating multi-track, full-band accompaniment based on the input of a lead
melody with chords (i.e., a lead sheet). The system contains three modular
components, each modelling a vital aspect of full-band composition. The first
component is a piano arranger that generates piano accompaniment for the lead
sheet by transferring texture styles to the chords using latent chord-texture
disentanglement and heuristic retrieval of texture donors. The second component
orchestrates the piano accompaniment score into full-band arrangement according
to the orchestration style encoded by individual track functions. The third
component, which connects the previous two, is a prior model characterizing the
global structure of orchestration style over the whole piece of music. From end
to end, the system learns to generate full-band accompaniment in a
self-supervised fashion, applying style transfer at two levels of polyphonic
composition: texture and orchestration. Experiments show that our system
outperforms the baselines significantly, and the modular design offers
effective controls in a musically meaningful way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Covariance Blocking and Whitening Method for Successive Relative
  Transfer Function Vector Estimation in Multi-Speaker Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henri Gode, Simon Doclo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of estimating the relative transfer
function (RTF) vectors of multiple speakers in a noisy and reverberant
environment. More specifically, we consider a scenario where two speakers
activate successively. In this scenario, the RTF vector of the first speaker
can be estimated in a straightforward way and the main challenge lies in
estimating the RTF vector of the second speaker during segments where both
speakers are simultaneously active. To estimate the RTF vector of the second
speaker the so-called blind oblique projection (BOP) method determines the
oblique projection operator that optimally blocks the second speaker. Instead
of blocking the second speaker, in this paper we propose a covariance blocking
and whitening (CBW) method, which first blocks the first speaker and applies
whitening using the estimated noise covariance matrix and then estimates the
RTF vector of the second speaker based on a singular value decomposition. When
using the estimated RTF vectors of both speakers in a linearly constrained
minimum variance beamformer, simulation results using real-world recordings for
multiple speaker positions demonstrate that the proposed CBW method outperforms
the conventional BOP and covariance whitening methods in terms of
signal-to-interferer-and-noise ratio improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Workshop on Applications of Signal Processing to Audio and
  Acoustics (WASPAA), New Paltz, NY, USA, Oct 22-25, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Streaming Speech-to-Avatar Synthesis <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejas S. Prabhune, Peter Wu, Bohan Yu, Gopala K. Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Streaming speech-to-avatar synthesis creates real-time animations for a
virtual character from audio data. Accurate avatar representations of speech
are important for the visualization of sound in linguistics, phonetics, and
phonology, visual feedback to assist second language acquisition, and virtual
embodiment for paralyzed patients. Previous works have highlighted the
capability of deep articulatory inversion to perform high-quality avatar
animation using electromagnetic articulography (EMA) features. However, these
models focus on offline avatar synthesis with recordings rather than real-time
audio, which is necessary for live avatar visualization or embodiment. To
address this issue, we propose a method using articulatory inversion for
streaming high quality facial and inner-mouth avatar animation from real-time
audio. Our approach achieves 130ms average streaming latency for every 0.1
seconds of audio with a 0.792 correlation with ground truth articulations.
Finally, we show generated mouth and tongue animations to demonstrate the
efficacy of our methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Repeatable Speech Embeddings Using An Intra-class Correlation
  Regularizer <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Zhang, Suren Jayasuriya, Visar Berisha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A good supervised embedding for a specific machine learning task is only
sensitive to changes in the label of interest and is invariant to other
confounding factors. We leverage the concept of repeatability from measurement
theory to describe this property and propose to use the intra-class correlation
coefficient (ICC) to evaluate the repeatability of embeddings. We then propose
a novel regularizer, the ICC regularizer, as a complementary component for
contrastive losses to guide deep neural networks to produce embeddings with
higher repeatability. We use simulated data to explain why the ICC regularizer
works better on minimizing the intra-class variance than the contrastive loss
alone. We implement the ICC regularizer and apply it to three speech tasks:
speaker verification, voice style conversion, and a clinical application for
detecting dysphonic voice. The experimental results demonstrate that adding an
ICC regularizer can improve the repeatability of learned embeddings compared to
only using the contrastive loss; further, these embeddings lead to improved
performance in these downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Panning on Non-Equidistant Loudspeakers with Direct Sound Level
  Compensation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Hendrik Hanschke, Daniel Arteaga, Giulio Cengarle, Joshua Lando, Mark R. P. Thomas, Alan Seefeldt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loudspeaker rendering techniques that create phantom sound sources often
assume an equidistant loudspeaker layout. Typical home setups might not fulfill
this condition as loudspeakers deviate from canonical positions, thus requiring
a corresponding calibration. The standard approach is to compensate for delays
and to match the loudness of each loudspeaker at the listener's location.It was
found that a shift of the phantom image occurs when this calibration procedure
is applied and one of a pair of loudspeakers is significantly closer to the
listener than the other. In this paper, a novel approach to panning on
non-equidistant loudspeaker layouts is presented whereby the panning position
is governed by the direct sound and the perceived loudness is governed by the
full impulse response. Subjective listening tests are presented that validate
the approach and quantify the perceived effect of the compensation. In a setup
where the standard calibration leads to an average error of 10 degrees, the
proposed direct sound compensation largely returns the phantom source to its
intended position.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. Accepted for presentation in AES Convention 155 (2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System <span class="chip">EMNLP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11069v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11069v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Waheed, Bashar Talafha, Peter Suvellin, AbdelRahim Elmadany, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arabic is a complex language with many varieties and dialects spoken by over
450 millions all around the world. Due to the linguistic diversity and
variations, it is challenging to build a robust and generalized ASR system for
Arabic. In this work, we address this gap by developing and demoing a system,
dubbed VoxArabica, for dialect identification (DID) as well as automatic speech
recognition (ASR) of Arabic. We train a wide range of models such as HuBERT
(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR
tasks. Our DID models are trained to identify 17 different dialects in addition
to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.
Additionally, for the remaining dialects in ASR, we provide the option to
choose various models such as Whisper and MMS in a zero-shot setting. We
integrate these models into a single web interface with diverse features such
as audio recording, file upload, model selection, and the option to raise flags
for incorrect outputs. Overall, we believe VoxArabica will be useful for a wide
range of audiences concerned with Arabic research. Our system is currently
running at https://cdce-206-12-100-168.ngrok.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ArabicNLP conference co-located with EMNLP'23. First
  three authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> MusicAgent: An AI Agent for Music Understanding and Generation with
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, <span class="highlight-author">Xu Tan</span>, Wei Ye, Shikun Zhang, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-empowered music processing is a diverse field that encompasses dozens of
tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension
tasks (e.g., music classification). For developers and amateurs, it is very
difficult to grasp all of these task to satisfy their requirements in music
processing, especially considering the huge differences in the representations
of music data and the model applicability across platforms among various tasks.
Consequently, it is necessary to build a system to organize and integrate these
tasks, and thus help practitioners to automatically analyze their demand and
call suitable tools as solutions to fulfill their requirements. Inspired by the
recent success of large language models (LLMs) in task automation, we develop a
system, named MusicAgent, which integrates numerous music-related tools and an
autonomous workflow to address user requirements. More specifically, we build
1) toolset that collects tools from diverse sources, including Hugging Face,
GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g.,
ChatGPT) to organize these tools and automatically decompose user requests into
multiple sub-tasks and invoke corresponding music tools. The primary goal of
this system is to free users from the intricacies of AI-music tools, enabling
them to concentrate on the creative aspect. By granting users the freedom to
effortlessly combine tools, the system offers a seamless and enriching music
experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global, and Local Optimization Beamforming for Broadband Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04921v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04921v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Goudarzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an alternative energy function for Global Optimization
(GO) beamforming, tailored to acoustic broadband sources. Given, that
properties such as the source location, multipole rotation, or flow conditions
are parameterized over the frequency, a CSM-fitting can be performed for all
frequencies at once. A numerical analysis shows that the nonlinear energy
function for the standard GO problem is equivalent to the source's Point Spread
Function (PSF) and contains local minima at the grating- and side lobes'
locations. The energy function is improved with the proposed broadband energy,
as it averages the PSF. Further, it simplifies the process of identifying
sources and reconstructing their spectra from the results. The paper shows that
the method is superior on synthetic monopoles compared to standard GO and
CLEAN-SC. For real-world data the results of the proposed method and CLEAN-SC
are similar, and outperform standard GO. The main difference is that source
assumption violations cause noisy maps for CLEAN-SC and cause wrong spectral
estimations of the proposed method. By using reasonable initial values, the GO
problem reduces to a Local Optimization problem with similar results. Further,
the proposed method is able to identify synthetic multipoles with different
pole amplitudes and unknown pole rotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to JASA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Attention always needed? A Case Study on Language Identification from
  Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03427v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03427v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atanu Mandal, Santanu Pal, Indranil Dutta, Mahidas Bhattacharya, Sudip Kumar Naskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Identification (LID) is a crucial preliminary process in the field
of Automatic Speech Recognition (ASR) that involves the identification of a
spoken language from audio samples. Contemporary systems that can process
speech in multiple languages require users to expressly designate one or more
languages prior to utilization. The LID task assumes a significant role in
scenarios where ASR systems are unable to comprehend the spoken language in
multilingual settings, leading to unsuccessful speech recognition outcomes. The
present study introduces convolutional recurrent neural network (CRNN) based
LID, designed to operate on the Mel-frequency Cepstral Coefficient (MFCC)
characteristics of audio samples. Furthermore, we replicate certain
state-of-the-art methodologies, specifically the Convolutional Neural Network
(CNN) and Attention-based Convolutional Recurrent Neural Network (CRNN with
attention), and conduct a comparative analysis with our CRNN-based approach. We
conducted comprehensive evaluations on thirteen distinct Indian languages and
our model resulted in over 98\% classification accuracy. The LID model exhibits
high-performance levels ranging from 97% to 100% for languages that are
linguistically similar. The proposed LID model exhibits a high degree of
extensibility to additional languages and demonstrates a strong resistance to
noise, achieving 91.2% accuracy in a noisy setting when applied to a European
Language (EU) dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Natural Language Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video
  Understanding <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02858v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02858v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhang, Xin Li, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Video-LLaMA a multi-modal framework that empowers Large Language
Models (LLMs) with the capability of understanding both visual and auditory
content in the video. Video-LLaMA bootstraps cross-modal training from the
frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike
previous works that complement LLMs to process the visual or audio signals
only, Video-LLaMA enables video comprehension by tackling two challenges: (1)
capturing the temporal changes in visual scenes, (2) integrating audio-visual
signals. To counter the first challenge, we propose a Video Q-former to
assemble a pre-trained image encoder into our video encoder and introduce a
video-to-text generation task to learn video-language correspondence. For the
second challenge, we leverage ImageBind, a universal embedding model aligning
multiple modalities, as the pre-trained audio encoder and introduce an Audio
Q-former on top of ImageBind to learn reasonable auditory query embeddings for
the LLM module. To align the output of both visual and audio encoders with
LLM's embedding space, we first train Video-LLaMA on massive
video/image-caption pairs and then tune our model with visual-instruction
datasets of moderate amount but higher quality. We found Video-LLaMA shows the
ability to perceive and comprehend video content and generate meaningful
responses grounded in the visual and auditory information presented in the
videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2023's demo track; Code, Pretrained Model, and
  Dataset: https://github.com/DAMO-NLP-SG/Video-LLaMA</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-24T00:00:00Z">2023-10-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CDSD: Chinese Dysarthria Speech Database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyi Sun, Ming Gao, Xinchen Kang, Shiru Wang, Jun Du, Dengfeng Yao, Su-Jing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Chinese Dysarthria Speech Database (CDSD) as a valuable
resource for dysarthria research. This database comprises speech data from 24
participants with dysarthria. Among these participants, one recorded an
additional 10 hours of speech data, while each recorded one hour, resulting in
34 hours of speech material. To accommodate participants with varying cognitive
levels, our text pool primarily consists of content from the AISHELL-1 dataset
and speeches by primary and secondary school students. When participants read
these texts, they must use a mobile device or the ZOOM F8n multi-track field
recorder to record their speeches. In this paper, we elucidate the data
collection and annotation processes and present an approach for establishing a
baseline for dysarthric speech recognition. Furthermore, we conducted a
speaker-dependent dysarthric speech recognition experiment using an additional
10 hours of speech data from one of our participants. Our research findings
indicate that, through extensive data-driven model training, fine-tuning
limited quantities of specific individual data yields commendable results in
speaker-dependent dysarthric speech recognition. However, we observe
significant variations in recognition results among different dysarthric
speakers. These insights provide valuable reference points for
speaker-dependent dysarthric speech recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Much Context Does My Attention-Based ASR System Need? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Flynn, Anton Ragni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the task of speech recognition, the use of more than 30 seconds of
acoustic context during training is uncommon, and under-investigated in
literature. In this work, we examine the effect of scaling the sequence length
used to train/evaluate (dense-attention based) acoustic and language models on
speech recognition performance. For these experiments a dataset of roughly
100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5
seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets
Earnings-22 and Tedlium demonstrate a benefit from training with around 80
seconds of acoustic context, showing up to a 14.9% relative improvement from a
limited context baseline. Furthermore, we perform a system combination with
long-context transformer language models via beam search for a fully
long-context ASR system, with results that are competitive with the current
state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOLEY-VAE: Generación de efectos de audio para cine con inteligencia
  artificial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateo Cámara, José Luis Blanco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we present an interface based on Variational Autoencoders
trained with a wide range of natural sounds for the innovative creation of
Foley effects. The model can transfer new sound features to prerecorded audio
or microphone-captured speech in real time. In addition, it allows interactive
modification of latent variables, facilitating precise and customized artistic
adjustments. Taking as a starting point our previous study on Variational
Autoencoders presented at this same congress last year, we analyzed an existing
implementation: RAVE [1]. This model has been specifically trained for audio
effects production. Various audio effects have been successfully generated,
ranging from electromagnetic, science fiction, and water sounds, among others
published with this work. This innovative approach has been the basis for the
artistic creation of the first Spanish short film with sound effects assisted
by artificial intelligence. This milestone illustrates palpably the
transformative potential of this technology in the film industry, opening the
door to new possibilities for sound creation and the improvement of artistic
quality in film productions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, in Spanish, Tecniac\'ustica</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Schmid, Khaled Koutini, Gerhard Widmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of large-scale audio datasets, such as AudioSet, paved the
way for Transformers to conquer the audio domain and replace CNNs as the
state-of-the-art neural network architecture for many tasks. Audio Spectrogram
Transformers are excellent at exploiting large datasets, creating powerful
pre-trained models that surpass CNNs when fine-tuned on downstream tasks.
However, current popular Audio Spectrogram Transformers are demanding in terms
of computational complexity compared to CNNs. Recently, we have shown that, by
employing Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch
up with and even outperform Transformers on large datasets. In this work, we
extend this line of research and increase the capacity of efficient CNNs by
introducing dynamic CNN blocks, constructed of dynamic non-linearities, dynamic
convolutions and attention mechanisms. We show that these dynamic CNNs
outperform traditional efficient CNNs, in terms of the performance-complexity
trade-off and parameter efficiency, at the task of audio tagging on the
large-scale AudioSet. Our experiments further indicate that the introduced
dynamic CNNs achieve better performance on downstream tasks and scale up well,
attaining Transformer performance and even outperforming them on AudioSet and
several downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/ACM Transactions on Audio, Speech, and Language
  Processing. Source Code available at:
  https://github.com/fschmid56/EfficientAT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Mason-Alberta Phonetic Segmenter: A forced alignment system based on
  deep neural networks and interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew C. Kelley, Scott James Perry, Benjamin V. Tucker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forced alignment systems automatically determine boundaries between segments
in speech data, given an orthographic transcription. These tools are
commonplace in phonetics to facilitate the use of speech data that would be
infeasible to manually transcribe and segment. In the present paper, we
describe a new neural network-based forced alignment system, the Mason-Alberta
Phonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two
possible improvements we pursue for forced alignment systems. The first is
treating the acoustic model in a forced aligner as a tagging task, rather than
a classification task, motivated by the common understanding that segments in
speech are not truly discrete and commonly overlap. The second is an
interpolation technique to allow boundaries more precise than the common 10 ms
limit in modern forced alignment systems. We compare configurations of our
system to a state-of-the-art system, the Montreal Forced Aligner. The tagging
approach did not generally yield improved results over the Montreal Forced
Aligner. However, a system with the interpolation technique had a 27.92%
increase relative to the Montreal Forced Aligner in the amount of boundaries
within 10 ms of the target on the test set. We also reflect on the task and
training process for acoustic modeling in forced alignment, highlighting how
the output targets for these models do not match phoneticians' conception of
similarity between phones and that reconciliation of this tension may require
rethinking the task and output targets or how speech itself should be
segmented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IA Para el Mantenimiento Predictivo en Canteras: Modelado 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Marcos, Rodrigo Tamaki, Mateo Cámara, Virginia Yagüe, José Luis Blanco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dependence on raw materials, especially in the mining sector, is a key part
of today's economy. Aggregates are vital, being the second most used raw
material after water. Digitally transforming this sector is key to optimizing
operations. However, supervision and maintenance (predictive and corrective)
are challenges little explored in this sector, due to the particularities of
the sector, machinery and environmental conditions. All this, despite the
successes achieved in other scenarios in monitoring with acoustic and contact
sensors. We present an unsupervised learning scheme that trains a variational
autoencoder model on a set of sound records. This is the first such dataset
collected during processing plant operations, containing information from
different points of the processing line. Our results demonstrate the model's
ability to reconstruct and represent in latent space the recorded sounds, the
differences in operating conditions and between different equipment. In the
future, this should facilitate the classification of sounds, as well as the
detection of anomalies and degradation patterns in the operation of the
machinery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, in Spanish language, 5 figures. Presented in Tecniacustica
  2023 conference (Cuenca, Spain)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complex Image Generation SwinTransformer Network for Audio Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youshan Zhang, Jialu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-performance audio denoising is still a challenging task in
real-world applications. Existing time-frequency methods often ignore the
quality of generated frequency domain images. This paper converts the audio
denoising problem into an image generation task. We first develop a complex
image generation SwinTransformer network to capture more information from the
complex Fourier domain. We then impose structure similarity and detailed loss
functions to generate high-quality images and develop an SDR loss to minimize
the difference between denoised and clean audios. Extensive experiments on two
benchmark datasets demonstrate that our proposed model is better than
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-Based Adversarial Purification for Speaker Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Bai, Xiao-Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, automatic speaker verification (ASV) based on deep learning is
easily contaminated by adversarial attacks, which is a new type of attack that
injects imperceptible perturbations to audio signals so as to make ASV produce
wrong decisions. This poses a significant threat to the security and
reliability of ASV systems. To address this issue, we propose a Diffusion-Based
Adversarial Purification (DAP) method that enhances the robustness of ASV
systems against such adversarial attacks. Our method leverages a conditional
denoising diffusion probabilistic model to effectively purify the adversarial
examples and mitigate the impact of perturbations. DAP first introduces
controlled noise into adversarial examples, and then performs a reverse
denoising process to reconstruct clean audio. Experimental results demonstrate
the efficacy of the proposed DAP in enhancing the security of ASV and meanwhile
minimizing the distortion of the purified audio signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving End-to-End Speech Processing by Efficient Text Data
  Utilization with Latent Synthesis <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqiao Lu, Wenyong Huang, Nianzu Zheng, Xingshan Zeng, Yu Ting Yeung, Xiao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a high performance end-to-end speech (E2E) processing model requires
an enormous amount of labeled speech data, especially in the era of
data-centric artificial intelligence. However, labeled speech data are usually
scarcer and more expensive for collection, compared to textual data. We propose
Latent Synthesis (LaSyn), an efficient textual data utilization framework for
E2E speech processing models. We train a latent synthesizer to convert textual
data into an intermediate latent representation of a pre-trained speech model.
These pseudo acoustic representations of textual data augment acoustic data for
model training. We evaluate LaSyn on low-resource automatic speech recognition
(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an
E2E baseline trained on LibriSpeech train-clean-100, with relative word error
rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our
E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for
slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)
and EM-Tree accuracies on STOP respectively. With fewer parameters, the results
of LaSyn are competitive to published state-of-the-art works. The results
demonstrate the quality of the augmented training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 8 tables, Accepted to EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for Modality-Agnostic Deepfakes Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.14491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.14491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cai Yu, Peng Chen, Jiahe Tian, Jin Liu, Jiao Dai, Xi Wang, Yesheng Chai, Shan Jia, Siwei Lyu, Jizhong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI-generated content (AIGC) thrives, deepfakes have expanded from
single-modality falsification to cross-modal fake content creation, where
either audio or visual components can be manipulated. While using two unimodal
detectors can detect audio-visual deepfakes, cross-modal forgery clues could be
overlooked. Existing multimodal deepfake detection methods typically establish
correspondence between the audio and visual modalities for binary real/fake
classification, and require the co-occurrence of both modalities. However, in
real-world multi-modal applications, missing modality scenarios may occur where
either modality is unavailable. In such cases, audio-visual detection methods
are less practical than two independent unimodal methods. Consequently, the
detector can not always obtain the number or type of manipulated modalities
beforehand, necessitating a fake-modality-agnostic audio-visual detector. In
this work, we introduce a comprehensive framework that is agnostic to fake
modalities, which facilitates the identification of multimodal deepfakes and
handles situations with missing modalities, regardless of the manipulations
embedded in audio, video, or even cross-modal forms. To enhance the modeling of
cross-modal forgery clues, we employ audio-visual speech recognition (AVSR) as
a preliminary task. This efficiently extracts speech correlations across
modalities, a feature challenging for deepfakes to replicate. Additionally, we
propose a dual-label detection approach that follows the structure of AVSR to
support the independent detection of each modality. Extensive experiments on
three audio-visual datasets show that our scheme outperforms state-of-the-art
detection methods with promising performance on modality-agnostic audio/video
deepfakes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised
  Features for Audio-Visual Speech Enhancement <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju-Chieh Chou, Chung-Ming Chien, Karen Livescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech enhancement systems are typically trained using pairs of clean and
noisy speech. In audio-visual speech enhancement (AVSE), there is not as much
ground-truth clean data available; most audio-visual datasets are collected in
real-world environments with background noise and reverberation, hampering the
development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based
audio-visual speech enhancement approach that can generate clean speech despite
the challenges of real-world training data. We obtain a subset of nearly clean
speech from an audio-visual corpus using a neural quality estimator, and then
train a diffusion model on this subset to generate waveforms conditioned on
continuous speech representations from AV-HuBERT with noise-robust training. We
use continuous rather than discrete representations to retain prosody and
speaker information. With this vocoding task alone, the model can perform
speech enhancement better than a masking-based baseline. We further fine-tune
the diffusion model on clean/noisy utterance pairs to improve the performance.
Our approach outperforms a masking-based baseline in terms of both automatic
metrics and a human listening test and is close in quality to the target speech
in the listening test. Audio samples can be found at
https://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Novel-View Acoustic Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08730v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08730v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changan Chen, Alexander Richard, Roman Shapovalov, Vamsi Krishna Ithapu, Natalia Neverova, Kristen Grauman, Andrea Vedaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the novel-view acoustic synthesis (NVAS) task: given the sight
and sound observed at a source viewpoint, can we synthesize the sound of that
scene from an unseen target viewpoint? We propose a neural rendering approach:
Visually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize
the sound of an arbitrary point in space by analyzing the input audio-visual
cues. To benchmark this task, we collect two first-of-their-kind large-scale
multi-view audio-visual datasets, one synthetic and one real. We show that our
model successfully reasons about the spatial cues and synthesizes faithful
audio on both datasets. To our knowledge, this work represents the very first
formulation, dataset, and approach to solve the novel-view acoustic synthesis
task, which has exciting potential applications ranging from AR/VR to art and
design. Unlocked by this work, we believe that the future of novel-view
synthesis is in multi-modal learning from videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023. Project page:
  https://vision.cs.utexas.edu/projects/nvas</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Reproducing Whisper-Style Training Using an Open-Source Toolkit and
  Publicly Available Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13876v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13876v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, <span class="highlight-author">Shinji Watanabe</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training speech models on large volumes of data has achieved remarkable
success. OpenAI Whisper is a multilingual multitask model trained on 680k hours
of supervised speech data. It generalizes well to various speech recognition
and translation benchmarks even in a zero-shot setup. However, the full
pipeline for developing such models (from data collection to training) is not
publicly accessible, which makes it difficult for researchers to further
improve its performance and address training-related issues such as efficiency,
robustness, fairness, and bias. This work presents an Open Whisper-style Speech
Model (OWSM), which reproduces Whisper-style training using an open-source
toolkit and publicly available data. OWSM even supports more translation
directions and can be more efficient to train. We will publicly release all
scripts used for data preparation, training, inference, and scoring as well as
pre-trained models and training logs to promote open science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ASRU 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CDSD: Chinese Dysarthria Speech Database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyi Sun, Ming Gao, Xinchen Kang, Shiru Wang, Jun Du, Dengfeng Yao, Su-Jing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Chinese Dysarthria Speech Database (CDSD) as a valuable
resource for dysarthria research. This database comprises speech data from 24
participants with dysarthria. Among these participants, one recorded an
additional 10 hours of speech data, while each recorded one hour, resulting in
34 hours of speech material. To accommodate participants with varying cognitive
levels, our text pool primarily consists of content from the AISHELL-1 dataset
and speeches by primary and secondary school students. When participants read
these texts, they must use a mobile device or the ZOOM F8n multi-track field
recorder to record their speeches. In this paper, we elucidate the data
collection and annotation processes and present an approach for establishing a
baseline for dysarthric speech recognition. Furthermore, we conducted a
speaker-dependent dysarthric speech recognition experiment using an additional
10 hours of speech data from one of our participants. Our research findings
indicate that, through extensive data-driven model training, fine-tuning
limited quantities of specific individual data yields commendable results in
speaker-dependent dysarthric speech recognition. However, we observe
significant variations in recognition results among different dysarthric
speakers. These insights provide valuable reference points for
speaker-dependent dysarthric speech recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pre-training Music Classification Models via Music Source Separation <span class="chip">ICASSP-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Garoufis, Athanasia Zlatintsi, Petros Maragos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study whether music source separation can be used as a
pre-training strategy for music representation learning, targeted at music
classification tasks. To this end, we first pre-train U-Net networks under
various music source separation objectives, such as the isolation of vocal or
instrumental sources from a musical piece; afterwards, we attach a
convolutional tail network to the pre-trained U-Net and jointly finetune the
whole network. The features learned by the separation network are also
propagated to the tail network through skip connections. Experimental results
in two widely used and publicly available datasets indicate that pre-training
the U-Nets with a music source separation objective can improve performance
compared to both training the whole network from scratch and using the tail
network as a standalone in two music classification tasks: music auto-tagging,
when vocal separation is used, and music genre classification for the case of
multi-source separation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages (4+references), 3 figures. ICASSP-24 submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Much Context Does My Attention-Based ASR System Need? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Flynn, Anton Ragni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the task of speech recognition, the use of more than 30 seconds of
acoustic context during training is uncommon, and under-investigated in
literature. In this work, we examine the effect of scaling the sequence length
used to train/evaluate (dense-attention based) acoustic and language models on
speech recognition performance. For these experiments a dataset of roughly
100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5
seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets
Earnings-22 and Tedlium demonstrate a benefit from training with around 80
seconds of acoustic context, showing up to a 14.9% relative improvement from a
limited context baseline. Furthermore, we perform a system combination with
long-context transformer language models via beam search for a fully
long-context ASR system, with results that are competitive with the current
state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOLEY-VAE: Generación de efectos de audio para cine con inteligencia
  artificial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateo Cámara, José Luis Blanco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we present an interface based on Variational Autoencoders
trained with a wide range of natural sounds for the innovative creation of
Foley effects. The model can transfer new sound features to prerecorded audio
or microphone-captured speech in real time. In addition, it allows interactive
modification of latent variables, facilitating precise and customized artistic
adjustments. Taking as a starting point our previous study on Variational
Autoencoders presented at this same congress last year, we analyzed an existing
implementation: RAVE [1]. This model has been specifically trained for audio
effects production. Various audio effects have been successfully generated,
ranging from electromagnetic, science fiction, and water sounds, among others
published with this work. This innovative approach has been the basis for the
artistic creation of the first Spanish short film with sound effects assisted
by artificial intelligence. This milestone illustrates palpably the
transformative potential of this technology in the film industry, opening the
door to new possibilities for sound creation and the improvement of artistic
quality in film productions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, in Spanish, Tecniac\'ustica</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Schmid, Khaled Koutini, Gerhard Widmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of large-scale audio datasets, such as AudioSet, paved the
way for Transformers to conquer the audio domain and replace CNNs as the
state-of-the-art neural network architecture for many tasks. Audio Spectrogram
Transformers are excellent at exploiting large datasets, creating powerful
pre-trained models that surpass CNNs when fine-tuned on downstream tasks.
However, current popular Audio Spectrogram Transformers are demanding in terms
of computational complexity compared to CNNs. Recently, we have shown that, by
employing Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch
up with and even outperform Transformers on large datasets. In this work, we
extend this line of research and increase the capacity of efficient CNNs by
introducing dynamic CNN blocks, constructed of dynamic non-linearities, dynamic
convolutions and attention mechanisms. We show that these dynamic CNNs
outperform traditional efficient CNNs, in terms of the performance-complexity
trade-off and parameter efficiency, at the task of audio tagging on the
large-scale AudioSet. Our experiments further indicate that the introduced
dynamic CNNs achieve better performance on downstream tasks and scale up well,
attaining Transformer performance and even outperforming them on AudioSet and
several downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/ACM Transactions on Audio, Speech, and Language
  Processing. Source Code available at:
  https://github.com/fschmid56/EfficientAT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Mason-Alberta Phonetic Segmenter: A forced alignment system based on
  deep neural networks and interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew C. Kelley, Scott James Perry, Benjamin V. Tucker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forced alignment systems automatically determine boundaries between segments
in speech data, given an orthographic transcription. These tools are
commonplace in phonetics to facilitate the use of speech data that would be
infeasible to manually transcribe and segment. In the present paper, we
describe a new neural network-based forced alignment system, the Mason-Alberta
Phonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two
possible improvements we pursue for forced alignment systems. The first is
treating the acoustic model in a forced aligner as a tagging task, rather than
a classification task, motivated by the common understanding that segments in
speech are not truly discrete and commonly overlap. The second is an
interpolation technique to allow boundaries more precise than the common 10 ms
limit in modern forced alignment systems. We compare configurations of our
system to a state-of-the-art system, the Montreal Forced Aligner. The tagging
approach did not generally yield improved results over the Montreal Forced
Aligner. However, a system with the interpolation technique had a 27.92%
increase relative to the Montreal Forced Aligner in the amount of boundaries
within 10 ms of the target on the test set. We also reflect on the task and
training process for acoustic modeling in forced alignment, highlighting how
the output targets for these models do not match phoneticians' conception of
similarity between phones and that reconciliation of this tension may require
rethinking the task and output targets or how speech itself should be
segmented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IA Para el Mantenimiento Predictivo en Canteras: Modelado 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Marcos, Rodrigo Tamaki, Mateo Cámara, Virginia Yagüe, José Luis Blanco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dependence on raw materials, especially in the mining sector, is a key part
of today's economy. Aggregates are vital, being the second most used raw
material after water. Digitally transforming this sector is key to optimizing
operations. However, supervision and maintenance (predictive and corrective)
are challenges little explored in this sector, due to the particularities of
the sector, machinery and environmental conditions. All this, despite the
successes achieved in other scenarios in monitoring with acoustic and contact
sensors. We present an unsupervised learning scheme that trains a variational
autoencoder model on a set of sound records. This is the first such dataset
collected during processing plant operations, containing information from
different points of the processing line. Our results demonstrate the model's
ability to reconstruct and represent in latent space the recorded sounds, the
differences in operating conditions and between different equipment. In the
future, this should facilitate the classification of sounds, as well as the
detection of anomalies and degradation patterns in the operation of the
machinery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, in Spanish language, 5 figures. Presented in Tecniacustica
  2023 conference (Cuenca, Spain)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complex Image Generation SwinTransformer Network for Audio Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youshan Zhang, Jialu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-performance audio denoising is still a challenging task in
real-world applications. Existing time-frequency methods often ignore the
quality of generated frequency domain images. This paper converts the audio
denoising problem into an image generation task. We first develop a complex
image generation SwinTransformer network to capture more information from the
complex Fourier domain. We then impose structure similarity and detailed loss
functions to generate high-quality images and develop an SDR loss to minimize
the difference between denoised and clean audios. Extensive experiments on two
benchmark datasets demonstrate that our proposed model is better than
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPRING-INX: A Multilingual Indian Language Speech Corpus by SPRING Lab,
  IIT Madras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nithya R, Malavika S, Jordan F, Arjun Gangwar, Metilda N J, S Umesh, Rithik Sarab, Akhilesh Kumar Dubey, Govind Divakaran, Samudra Vijaya K, Suryakanth V Gangashetty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  India is home to a multitude of languages of which 22 languages are
recognised by the Indian Constitution as official. Building speech based
applications for the Indian population is a difficult problem owing to limited
data and the number of languages and accents to accommodate. To encourage the
language technology community to build speech based applications in Indian
languages, we are open sourcing SPRING-INX data which has about 2000 hours of
legally sourced and manually transcribed speech data for ASR system building in
Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi
and Tamil. This endeavor is by SPRING Lab , Indian Institute of Technology
Madras and is a part of National Language Translation Mission (NLTM), funded by
the Indian Ministry of Electronics and Information Technology (MeitY),
Government of India. We describe the data collection and data cleaning process
along with the data statistics in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, About SPRING-INX Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-Based Adversarial Purification for Speaker Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Bai, Xiao-Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, automatic speaker verification (ASV) based on deep learning is
easily contaminated by adversarial attacks, which is a new type of attack that
injects imperceptible perturbations to audio signals so as to make ASV produce
wrong decisions. This poses a significant threat to the security and
reliability of ASV systems. To address this issue, we propose a Diffusion-Based
Adversarial Purification (DAP) method that enhances the robustness of ASV
systems against such adversarial attacks. Our method leverages a conditional
denoising diffusion probabilistic model to effectively purify the adversarial
examples and mitigate the impact of perturbations. DAP first introduces
controlled noise into adversarial examples, and then performs a reverse
denoising process to reconstruct clean audio. Experimental results demonstrate
the efficacy of the proposed DAP in enhancing the security of ASV and meanwhile
minimizing the distortion of the purified audio signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving End-to-End Speech Processing by Efficient Text Data
  Utilization with Latent Synthesis <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqiao Lu, Wenyong Huang, Nianzu Zheng, Xingshan Zeng, Yu Ting Yeung, Xiao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a high performance end-to-end speech (E2E) processing model requires
an enormous amount of labeled speech data, especially in the era of
data-centric artificial intelligence. However, labeled speech data are usually
scarcer and more expensive for collection, compared to textual data. We propose
Latent Synthesis (LaSyn), an efficient textual data utilization framework for
E2E speech processing models. We train a latent synthesizer to convert textual
data into an intermediate latent representation of a pre-trained speech model.
These pseudo acoustic representations of textual data augment acoustic data for
model training. We evaluate LaSyn on low-resource automatic speech recognition
(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an
E2E baseline trained on LibriSpeech train-clean-100, with relative word error
rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our
E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for
slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)
and EM-Tree accuracies on STOP respectively. With fewer parameters, the results
of LaSyn are competitive to published state-of-the-art works. The results
demonstrate the quality of the augmented training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 8 tables, Accepted to EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for Modality-Agnostic Deepfakes Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.14491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.14491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cai Yu, Peng Chen, Jiahe Tian, Jin Liu, Jiao Dai, Xi Wang, Yesheng Chai, Shan Jia, Siwei Lyu, Jizhong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI-generated content (AIGC) thrives, deepfakes have expanded from
single-modality falsification to cross-modal fake content creation, where
either audio or visual components can be manipulated. While using two unimodal
detectors can detect audio-visual deepfakes, cross-modal forgery clues could be
overlooked. Existing multimodal deepfake detection methods typically establish
correspondence between the audio and visual modalities for binary real/fake
classification, and require the co-occurrence of both modalities. However, in
real-world multi-modal applications, missing modality scenarios may occur where
either modality is unavailable. In such cases, audio-visual detection methods
are less practical than two independent unimodal methods. Consequently, the
detector can not always obtain the number or type of manipulated modalities
beforehand, necessitating a fake-modality-agnostic audio-visual detector. In
this work, we introduce a comprehensive framework that is agnostic to fake
modalities, which facilitates the identification of multimodal deepfakes and
handles situations with missing modalities, regardless of the manipulations
embedded in audio, video, or even cross-modal forms. To enhance the modeling of
cross-modal forgery clues, we employ audio-visual speech recognition (AVSR) as
a preliminary task. This efficiently extracts speech correlations across
modalities, a feature challenging for deepfakes to replicate. Additionally, we
propose a dual-label detection approach that follows the structure of AVSR to
support the independent detection of each modality. Extensive experiments on
three audio-visual datasets show that our scheme outperforms state-of-the-art
detection methods with promising performance on modality-agnostic audio/video
deepfakes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised
  Features for Audio-Visual Speech Enhancement <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju-Chieh Chou, Chung-Ming Chien, Karen Livescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech enhancement systems are typically trained using pairs of clean and
noisy speech. In audio-visual speech enhancement (AVSE), there is not as much
ground-truth clean data available; most audio-visual datasets are collected in
real-world environments with background noise and reverberation, hampering the
development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based
audio-visual speech enhancement approach that can generate clean speech despite
the challenges of real-world training data. We obtain a subset of nearly clean
speech from an audio-visual corpus using a neural quality estimator, and then
train a diffusion model on this subset to generate waveforms conditioned on
continuous speech representations from AV-HuBERT with noise-robust training. We
use continuous rather than discrete representations to retain prosody and
speaker information. With this vocoding task alone, the model can perform
speech enhancement better than a masking-based baseline. We further fine-tune
the diffusion model on clean/noisy utterance pairs to improve the performance.
Our approach outperforms a masking-based baseline in terms of both automatic
metrics and a human listening test and is close in quality to the target speech
in the listening test. Audio samples can be found at
https://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai-Wei Chang, Ming-Hsin Chen, Yun-Ping Lin, Jing Neng Hsu, Paul Kuo-Ming Huang, Chien-yu Huang, Shang-Wen Li, <span class="highlight-author">Hung-yi Lee</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting and adapter tuning have emerged as efficient alternatives to
fine-tuning (FT) methods. However, existing studies on speech prompting focused
on classification tasks and failed on more complex sequence generation tasks.
Besides, adapter tuning is primarily applied with a focus on encoder-only
self-supervised models. Our experiments show that prompting on Wav2Seq, a
self-supervised encoder-decoder model, surpasses previous works in sequence
generation tasks. It achieves a remarkable 53% relative improvement in word
error rate for ASR and a 27% in F1 score for slot filling. Additionally,
prompting competes with the FT method in the low-resource scenario. Moreover,
we show the transferability of prompting and adapter tuning on Wav2Seq in
cross-lingual ASR. When limited trainable parameters are involved, prompting
and adapter tuning consistently outperform conventional FT across 7 languages.
Notably, in the low-resource scenario, prompting consistently outperforms
adapter tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ASRU 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling Voice and Content with Self-Supervision for Speaker
  Recognition <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchi Liu, Kong Aik Lee, Qiongqiong Wang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For speaker recognition, it is difficult to extract an accurate speaker
representation from speech because of its mixture of speaker traits and
content. This paper proposes a disentanglement framework that simultaneously
models speaker traits and content variability in speech. It is realized with
the use of three Gaussian inference layers, each consisting of a learnable
transition model that extracts distinct speech components. Notably, a
strengthened transition model is specifically designed to model complex speech
dynamics. We also propose a self-supervision method to dynamically disentangle
content without the use of labels other than speaker identities. The efficacy
of the proposed framework is validated via experiments conducted on the
VoxCeleb and SITW datasets with 9.56% and 8.24% average reductions in EER and
minDCF, respectively. Since neither additional model training nor data is
specifically needed, it is easily applicable in practical use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023 (main track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Novel-View Acoustic Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08730v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08730v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changan Chen, Alexander Richard, Roman Shapovalov, Vamsi Krishna Ithapu, Natalia Neverova, Kristen Grauman, Andrea Vedaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the novel-view acoustic synthesis (NVAS) task: given the sight
and sound observed at a source viewpoint, can we synthesize the sound of that
scene from an unseen target viewpoint? We propose a neural rendering approach:
Visually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize
the sound of an arbitrary point in space by analyzing the input audio-visual
cues. To benchmark this task, we collect two first-of-their-kind large-scale
multi-view audio-visual datasets, one synthetic and one real. We show that our
model successfully reasons about the spatial cues and synthesizes faithful
audio on both datasets. To our knowledge, this work represents the very first
formulation, dataset, and approach to solve the novel-view acoustic synthesis
task, which has exciting potential applications ranging from AR/VR to art and
design. Unlocked by this work, we believe that the future of novel-view
synthesis is in multi-modal learning from videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023. Project page:
  https://vision.cs.utexas.edu/projects/nvas</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Reproducing Whisper-Style Training Using an Open-Source Toolkit and
  Publicly Available Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13876v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13876v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, <span class="highlight-author">Shinji Watanabe</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training speech models on large volumes of data has achieved remarkable
success. OpenAI Whisper is a multilingual multitask model trained on 680k hours
of supervised speech data. It generalizes well to various speech recognition
and translation benchmarks even in a zero-shot setup. However, the full
pipeline for developing such models (from data collection to training) is not
publicly accessible, which makes it difficult for researchers to further
improve its performance and address training-related issues such as efficiency,
robustness, fairness, and bias. This work presents an Open Whisper-style Speech
Model (OWSM), which reproduces Whisper-style training using an open-source
toolkit and publicly available data. OWSM even supports more translation
directions and can be more efficient to train. We will publicly release all
scripts used for data preparation, training, inference, and scoring as well as
pre-trained models and training logs to promote open science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ASRU 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-23T00:00:00Z">2023-10-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel-View Acoustic Synthesis from 3D Reconstructed Rooms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeongjoo Ahn, Karren Yang, Brian Hamilton, Jonathan Sheaffer, Anurag Ranjan, Miguel Sarabia, Oncel Tuzel, Jen-Hao Rick Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the benefit of combining blind audio recordings with 3D scene
information for novel-view acoustic synthesis. Given audio recordings from 2-4
microphones and the 3D geometry and material of a scene containing multiple
unknown sound sources, we estimate the sound anywhere in the scene. We identify
the main challenges of novel-view acoustic synthesis as sound source
localization, separation, and dereverberation. While naively training an
end-to-end network fails to produce high-quality results, we show that
incorporating room impulse responses (RIRs) derived from 3D reconstructed rooms
enables the same network to jointly tackle these tasks. Our method outperforms
existing methods designed for the individual tasks, demonstrating its
effectiveness at utilizing 3D visual information. In a simulated study on the
Matterport3D-NVAS dataset, our model achieves near-perfect accuracy on source
localization, a PSNR of 26.44 dB and a SDR of 14.23 dB for source separation
and dereverberation, resulting in a PSNR of 25.55 dB and a SDR of 14.20 dB on
novel-view acoustic synthesis. Code, pretrained model, and video results are
available on the project webpage (https://github.com/apple/ml-nvas3d).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Key Frame Mechanism For Efficient Conformer Based End-to-end Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Fan, Changhao Shan, Jianwei Zhang, Sining Sun, Qing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Conformer as a backbone network for end-to-end automatic speech
recognition achieved state-of-the-art performance. The Conformer block
leverages a self-attention mechanism to capture global information, along with
a convolutional neural network to capture local information, resulting in
improved performance. However, the Conformer-based model encounters an issue
with the self-attention mechanism, as computational complexity grows
quadratically with the length of the input sequence. Inspired by previous
Connectionist Temporal Classification (CTC) guided blank skipping during
decoding, we introduce intermediate CTC outputs as guidance into the
downsampling procedure of the Conformer encoder. We define the frame with
non-blank output as key frame. Specifically, we introduce the key frame-based
self-attention (KFSA) mechanism, a novel method to reduce the computation of
the self-attention mechanism using key frames. The structure of our proposed
approach comprises two encoders. Following the initial encoder, we introduce an
intermediate CTC loss function to compute the label frame, enabling us to
extract the key frames and blank frames for KFSA. Furthermore, we introduce the
key frame-based downsampling (KFDS) mechanism to operate on high-dimensional
acoustic features directly and drop the frames corresponding to blank labels,
which results in new acoustic feature sequences as input to the second encoder.
By using the proposed method, which achieves comparable or higher performance
than vanilla Conformer and other similar work such as Efficient Conformer.
Meantime, our proposed method can discard more than 60\% useless frames during
model training and inference, which will accelerate the inference speed
significantly. This work code is available in
{https://github.com/scufan1990/Key-Frame-Mechanism-For-Efficient-Conformer}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted by IEEE Signal Processing Letters
  for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 8+8=4: Formalizing Time Units to Handle Symbolic Music Durations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanouil Karystinaios, Francesco Foscarin, Florent Jacquemard, Masahiko Sakai, Satoshi Tojo, Gerhard Widmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on the nominal durations of musical events (notes and
rests) in a symbolic musical score, and on how to conveniently handle these in
computer applications. We propose the usage of a temporal unit that is directly
related to the graphical symbols in musical scores and pair this with a set of
operations that cover typical computations in music applications. We formalize
this time unit and the more commonly used approach in a single mathematical
framework, as semirings, algebraic structures that enable an abstract
description of algorithms/processing pipelines. We then discuss some practical
use cases and highlight when our system can improve such pipelines by making
them more efficient in terms of data type used and the number of computations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the International Symposium on Computer Music
  Multidisciplinary Research (CMMR 2023), Tokyo, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intuitive Multilingual Audio-Visual Speech Recognition with a
  Single-Trained Model <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Hong, Se Jin Park, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to multilingual audio-visual speech recognition
tasks by introducing a single model on a multilingual dataset. Motivated by a
human cognitive system where humans can intuitively distinguish different
languages without any conscious effort or guidance, we propose a model that can
capture which language is given as an input speech by distinguishing the
inherent similarities and differences between languages. To do so, we design a
prompt fine-tuning technique into the largely pre-trained audio-visual
representation model so that the network can recognize the language class as
well as the speech with the corresponding language. Our work contributes to
developing robust and efficient multilingual audio-visual speech recognition
systems, reducing the need for language-specific models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Timestamp Information for Serialized Joint Streaming
  Recognition and Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Papi, Peidong Wang, Junkun Chen, Jian Xue, Naoyuki Kanda, Jinyu Li, Yashesh Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing need for instant spoken language transcription and translation is
driven by increased global communication and cross-lingual interactions. This
has made offering translations in multiple languages essential for user
applications. Traditional approaches to automatic speech recognition (ASR) and
speech translation (ST) have often relied on separate systems, leading to
inefficiencies in computational resources, and increased synchronization
complexity in real time. In this paper, we propose a streaming
Transformer-Transducer (T-T) model able to jointly produce many-to-one and
one-to-many transcription and translation using a single decoder. We introduce
a novel method for joint token-level serialized output training based on
timestamp information to effectively produce ASR and ST outputs in the
streaming setting. Experiments on {it,es,de}->en prove the effectiveness of our
approach, enabling the generation of one-to-many joint outputs with a single
decoder for the first time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-Visual Speaker Tracking: Progress, Challenges, and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinzheng Zhao, Yong Xu, Xinyuan Qian, Davide Berghi, Peipei Wu, Meng Cui, Jianyuan Sun, Philip J. B. Jackson, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual speaker tracking has drawn increasing attention over the past
few years due to its academic values and wide application. Audio and visual
modalities can provide complementary information for localization and tracking.
With audio and visual information, the Bayesian-based filter can solve the
problem of data association, audio-visual fusion and track management. In this
paper, we conduct a comprehensive overview of audio-visual speaker tracking. To
our knowledge, this is the first extensive survey over the past five years. We
introduce the family of Bayesian filters and summarize the methods for
obtaining audio-visual measurements. In addition, the existing trackers and
their performance on AV16.3 dataset are summarized. In the past few years, deep
learning techniques have thrived, which also boosts the development of audio
visual speaker tracking. The influence of deep learning techniques in terms of
measurement extraction and state estimation is also discussed. At last, we
discuss the connections between audio-visual speaker tracking and other areas
such as speech separation and distributed speaker tracking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Acoustic BPE for Speech Generation with Discrete Tokens <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feiyu Shen, Yiwei Guo, Chenpeng Du, Xie Chen, <span class="highlight-author">Kai Yu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete audio tokens derived from self-supervised learning models have
gained widespread usage in speech generation. However, current practice of
directly utilizing audio tokens poses challenges for sequence modeling due to
the length of the token sequence. Additionally, this approach places the burden
on the model to establish correlations between tokens, further complicating the
modeling process. To address this issue, we propose acoustic BPE which encodes
frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE
effectively reduces the sequence length and leverages the prior morphological
information present in token sequence, which alleviates the modeling challenges
of token correlation. Through comprehensive investigations on a speech language
model trained with acoustic BPE, we confirm the notable advantages it offers,
including faster inference and improved syntax capturing capabilities. In
addition, we propose a novel rescore method to select the optimal synthetic
speech among multiple candidates generated by rich-diversity TTS system.
Experiments prove that rescore selection aligns closely with human preference,
which highlights acoustic BPE's potential to other speech generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures; submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GESI: Gammachirp Envelope Similarity Index for Predicting
  Intelligibility of Simulated Hearing Loss Sounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayako Yamamoto, Toshio Irino, Fuki Miyazaki, Honoka Tamaru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We proposed a new objective intelligibility measure (OIM), called the
Gammachirp Envelope Similarity Index (GESI), which can predict the speech
intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing
(NH) listeners. GESI is an intrusive method that computes the SI metric using
the gammachirp filterbank (GCFB), the modulation filterbank, and the extended
cosine similarity measure. GESI can accept the level asymmetry of the reference
and test sounds and reflect the HI listener's hearing level as it appears on
the audiogram. A unique feature of GESI is its ability to incorporate an
individual participant's listening condition into the SI prediction. We
conducted four SI experiments on male and female speech sounds in both
laboratory and crowdsourced remote environments. We then evaluated GESI and the
conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI, for their ability to predict
mean and individual SI values with and without the use of simulated HL sounds.
GESI outperformed the other OIMs in all evaluations. STOI, ESTOI, and MBSTOI
did not predict SI at all, even when using the simulated HL sounds. HASPI did
not predict the difference between the laboratory and remote experiments on
male speech sounds and the individual SI values. GESI may provide a first step
toward SI prediction for individual HI listeners whose HL is caused solely by
peripheral dysfunction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was submitted to Speech Communication on September 14,
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality Dropout for Multimodal Device Directed Speech Detection using
  Verbal and Non-Verbal Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gautam Krishna, Sameer Dharur, Oggi Rudovic, Pranay Dighe, Saurabh Adya, Ahmed Hussen Abdelaziz, Ahmed H Tewfik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Device-directed speech detection (DDSD) is the binary classification task of
distinguishing between queries directed at a voice assistant versus side
conversation or background speech. State-of-the-art DDSD systems use verbal
cues, e.g acoustic, text and/or automatic speech recognition system (ASR)
features, to classify speech as device-directed or otherwise, and often have to
contend with one or more of these modalities being unavailable when deployed in
real-world settings. In this paper, we investigate fusion schemes for DDSD
systems that can be made more robust to missing modalities. Concurrently, we
study the use of non-verbal cues, specifically prosody features, in addition to
verbal cues for DDSD. We present different approaches to combine scores and
embeddings from prosody with the corresponding verbal cues, finding that
prosody improves DDSD performance by upto 8.5% in terms of false acceptance
rate (FA) at a given fixed operating point via non-linear intermediate fusion,
while our use of modality dropout techniques improves the performance of these
models by 7.4% in terms of FA when evaluated with missing modalities during
inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Comunità, Riccardo F. Gramaccioni, Emilian Postolache, Emanuele Rodolà, Danilo Comminiello, Joshua D. Reiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound design involves creatively selecting, recording, and editing sound
effects for various media like cinema, video games, and virtual/augmented
reality. One of the most time-consuming steps when designing sound is
synchronizing audio with video. In some cases, environmental recordings from
video shoots are available, which can aid in the process. However, in video
games and animations, no reference audio exists, requiring manual annotation of
event timings from the video. We propose a system to extract repetitive actions
onsets from a video, which are then used - in conjunction with audio or textual
embeddings - to condition a diffusion model trained to generate a new
synchronized sound effects audio track. In this way, we leave complete creative
control to the sound designer while removing the burden of synchronization with
video. Furthermore, editing the onset track or changing the conditioning
embedding requires much less effort than editing the audio track itself,
simplifying the sonification process. We provide sound examples, source code,
and pretrained models to faciliate reproducibility
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11757v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11757v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavio Schneider, Ojasv Kamal, Zhijing Jin, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen the rapid development of large generative models for
text; however, much less research has explored the connection between text and
another "language" of communication -- music. Music, much like text, can convey
emotions, stories, and ideas, and has its own unique structure and syntax. In
our work, we bridge text and music via a text-to-music generation model that is
highly efficient, expressive, and can handle long-term structure. Specifically,
we develop Mo\^usai, a cascading two-stage latent diffusion model that can
generate multiple minutes of high-quality stereo music at 48kHz from textual
descriptions. Moreover, our model features high efficiency, which enables
real-time inference on a single consumer GPU with a reasonable speed. Through
experiments and property analyses, we show our model's competence over a
variety of criteria compared with existing music generation models. Lastly, to
promote the open-source culture, we provide a collection of open-source
libraries with the hope of facilitating future work in the field. We
open-source the following: Codes:
https://github.com/archinetai/audio-diffusion-pytorch; music samples for this
paper: http://bit.ly/44ozWDH; all music samples for all models:
https://bit.ly/audio-diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel-View Acoustic Synthesis from 3D Reconstructed Rooms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeongjoo Ahn, Karren Yang, Brian Hamilton, Jonathan Sheaffer, Anurag Ranjan, Miguel Sarabia, Oncel Tuzel, Jen-Hao Rick Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the benefit of combining blind audio recordings with 3D scene
information for novel-view acoustic synthesis. Given audio recordings from 2-4
microphones and the 3D geometry and material of a scene containing multiple
unknown sound sources, we estimate the sound anywhere in the scene. We identify
the main challenges of novel-view acoustic synthesis as sound source
localization, separation, and dereverberation. While naively training an
end-to-end network fails to produce high-quality results, we show that
incorporating room impulse responses (RIRs) derived from 3D reconstructed rooms
enables the same network to jointly tackle these tasks. Our method outperforms
existing methods designed for the individual tasks, demonstrating its
effectiveness at utilizing 3D visual information. In a simulated study on the
Matterport3D-NVAS dataset, our model achieves near-perfect accuracy on source
localization, a PSNR of 26.44 dB and a SDR of 14.23 dB for source separation
and dereverberation, resulting in a PSNR of 25.55 dB and a SDR of 14.20 dB on
novel-view acoustic synthesis. Code, pretrained model, and video results are
available on the project webpage (https://github.com/apple/ml-nvas3d).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delayed Memory Unit: Modelling Temporal Dependency Through Delay Gate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Sun, Jibin Wu, Malu Zhang, Paul Devos, Dick Botteldooren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent Neural Networks (RNNs) are renowned for their adeptness in modeling
temporal dependencies, a trait that has driven their widespread adoption for
sequential data processing. Nevertheless, vanilla RNNs are confronted with the
well-known issue of gradient vanishing and exploding, posing a significant
challenge for learning and establishing long-range dependencies. Additionally,
gated RNNs tend to be over-parameterized, resulting in poor network
generalization. To address these challenges, we propose a novel Delayed Memory
Unit (DMU) in this paper, wherein a delay line structure, coupled with delay
gates, is introduced to facilitate temporal interaction and temporal credit
assignment, so as to enhance the temporal modeling capabilities of vanilla
RNNs. Particularly, the DMU is designed to directly distribute the input
information to the optimal time instant in the future, rather than aggregating
and redistributing it over time through intricate network dynamics. Our
proposed DMU demonstrates superior temporal modeling capabilities across a
broad range of sequential modeling tasks, utilizing considerably fewer
parameters than other state-of-the-art gated RNN models in applications such as
speech recognition, radar gesture recognition, ECG waveform segmentation, and
permuted sequential image classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Key Frame Mechanism For Efficient Conformer Based End-to-end Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Fan, Changhao Shan, Jianwei Zhang, Sining Sun, Qing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Conformer as a backbone network for end-to-end automatic speech
recognition achieved state-of-the-art performance. The Conformer block
leverages a self-attention mechanism to capture global information, along with
a convolutional neural network to capture local information, resulting in
improved performance. However, the Conformer-based model encounters an issue
with the self-attention mechanism, as computational complexity grows
quadratically with the length of the input sequence. Inspired by previous
Connectionist Temporal Classification (CTC) guided blank skipping during
decoding, we introduce intermediate CTC outputs as guidance into the
downsampling procedure of the Conformer encoder. We define the frame with
non-blank output as key frame. Specifically, we introduce the key frame-based
self-attention (KFSA) mechanism, a novel method to reduce the computation of
the self-attention mechanism using key frames. The structure of our proposed
approach comprises two encoders. Following the initial encoder, we introduce an
intermediate CTC loss function to compute the label frame, enabling us to
extract the key frames and blank frames for KFSA. Furthermore, we introduce the
key frame-based downsampling (KFDS) mechanism to operate on high-dimensional
acoustic features directly and drop the frames corresponding to blank labels,
which results in new acoustic feature sequences as input to the second encoder.
By using the proposed method, which achieves comparable or higher performance
than vanilla Conformer and other similar work such as Efficient Conformer.
Meantime, our proposed method can discard more than 60\% useless frames during
model training and inference, which will accelerate the inference speed
significantly. This work code is available in
{https://github.com/scufan1990/Key-Frame-Mechanism-For-Efficient-Conformer}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted by IEEE Signal Processing Letters
  for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 8+8=4: Formalizing Time Units to Handle Symbolic Music Durations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanouil Karystinaios, Francesco Foscarin, Florent Jacquemard, Masahiko Sakai, Satoshi Tojo, Gerhard Widmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on the nominal durations of musical events (notes and
rests) in a symbolic musical score, and on how to conveniently handle these in
computer applications. We propose the usage of a temporal unit that is directly
related to the graphical symbols in musical scores and pair this with a set of
operations that cover typical computations in music applications. We formalize
this time unit and the more commonly used approach in a single mathematical
framework, as semirings, algebraic structures that enable an abstract
description of algorithms/processing pipelines. We then discuss some practical
use cases and highlight when our system can improve such pipelines by making
them more efficient in terms of data type used and the number of computations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the International Symposium on Computer Music
  Multidisciplinary Research (CMMR 2023), Tokyo, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intuitive Multilingual Audio-Visual Speech Recognition with a
  Single-Trained Model <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Hong, Se Jin Park, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to multilingual audio-visual speech recognition
tasks by introducing a single model on a multilingual dataset. Motivated by a
human cognitive system where humans can intuitively distinguish different
languages without any conscious effort or guidance, we propose a model that can
capture which language is given as an input speech by distinguishing the
inherent similarities and differences between languages. To do so, we design a
prompt fine-tuning technique into the largely pre-trained audio-visual
representation model so that the network can recognize the language class as
well as the speech with the corresponding language. Our work contributes to
developing robust and efficient multilingual audio-visual speech recognition
systems, reducing the need for language-specific models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prompt-driven Target Speech Diarization <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidi Jiang, Zhengyang Chen, Ruijie Tao, Liqun Deng, Yanmin Qian, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel task named `target speech diarization', which seeks to
determine `when target event occurred' within an audio signal. We devise a
neural architecture called Prompt-driven Target Speech Diarization (PTSD), that
works with diverse prompts that specify the target speech events of interest.
We train and evaluate PTSD using sim2spk, sim3spk and sim4spk datasets, which
are derived from the Librispeech. We show that the proposed framework
accurately localizes target speech events. Furthermore, our framework exhibits
versatility through its impressive performance in three diarization-related
tasks: target speaker voice activity detection, overlapped speech detection and
gender diarization. In particular, PTSD achieves comparable performance to
specialized models across these tasks on both real and simulated data. This
work serves as a reference benchmark and provides valuable insights into
prompt-driven target speech processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Timestamp Information for Serialized Joint Streaming
  Recognition and Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Papi, Peidong Wang, Junkun Chen, Jian Xue, Naoyuki Kanda, Jinyu Li, Yashesh Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing need for instant spoken language transcription and translation is
driven by increased global communication and cross-lingual interactions. This
has made offering translations in multiple languages essential for user
applications. Traditional approaches to automatic speech recognition (ASR) and
speech translation (ST) have often relied on separate systems, leading to
inefficiencies in computational resources, and increased synchronization
complexity in real time. In this paper, we propose a streaming
Transformer-Transducer (T-T) model able to jointly produce many-to-one and
one-to-many transcription and translation using a single decoder. We introduce
a novel method for joint token-level serialized output training based on
timestamp information to effectively produce ASR and ST outputs in the
streaming setting. Experiments on {it,es,de}->en prove the effectiveness of our
approach, enabling the generation of one-to-many joint outputs with a single
decoder for the first time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-Visual Speaker Tracking: Progress, Challenges, and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinzheng Zhao, Yong Xu, Xinyuan Qian, Davide Berghi, Peipei Wu, Meng Cui, Jianyuan Sun, Philip J. B. Jackson, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual speaker tracking has drawn increasing attention over the past
few years due to its academic values and wide application. Audio and visual
modalities can provide complementary information for localization and tracking.
With audio and visual information, the Bayesian-based filter can solve the
problem of data association, audio-visual fusion and track management. In this
paper, we conduct a comprehensive overview of audio-visual speaker tracking. To
our knowledge, this is the first extensive survey over the past five years. We
introduce the family of Bayesian filters and summarize the methods for
obtaining audio-visual measurements. In addition, the existing trackers and
their performance on AV16.3 dataset are summarized. In the past few years, deep
learning techniques have thrived, which also boosts the development of audio
visual speaker tracking. The influence of deep learning techniques in terms of
measurement extraction and state estimation is also discussed. At last, we
discuss the connections between audio-visual speaker tracking and other areas
such as speech separation and distributed speaker tracking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DPP-TTS: Diversifying prosodic features of speech via determinantal
  point processes <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongho Joo, Hyukhun Koh, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement in deep generative models, recent neural
Text-To-Speech(TTS) models have succeeded in synthesizing human-like speech.
There have been some efforts to generate speech with various prosody beyond
monotonous prosody patterns. However, previous works have several limitations.
First, typical TTS models depend on the scaled sampling temperature for
boosting the diversity of prosody. Speech samples generated at high sampling
temperatures often lack perceptual prosodic diversity, which can adversely
affect the naturalness of the speech. Second, the diversity among samples is
neglected since the sampling procedure often focuses on a single speech sample
rather than multiple ones. In this paper, we propose DPP-TTS: a text-to-speech
model based on Determinantal Point Processes (DPPs) with a prosody diversifying
module. Our TTS model is capable of generating speech samples that
simultaneously consider perceptual diversity in each sample and among multiple
samples. We demonstrate that DPP-TTS generates speech samples with more
diversified prosody than baselines in the side-by-side comparison test
considering the naturalness of speech at the same time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Acoustic BPE for Speech Generation with Discrete Tokens <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feiyu Shen, Yiwei Guo, Chenpeng Du, Xie Chen, <span class="highlight-author">Kai Yu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete audio tokens derived from self-supervised learning models have
gained widespread usage in speech generation. However, current practice of
directly utilizing audio tokens poses challenges for sequence modeling due to
the length of the token sequence. Additionally, this approach places the burden
on the model to establish correlations between tokens, further complicating the
modeling process. To address this issue, we propose acoustic BPE which encodes
frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE
effectively reduces the sequence length and leverages the prior morphological
information present in token sequence, which alleviates the modeling challenges
of token correlation. Through comprehensive investigations on a speech language
model trained with acoustic BPE, we confirm the notable advantages it offers,
including faster inference and improved syntax capturing capabilities. In
addition, we propose a novel rescore method to select the optimal synthetic
speech among multiple candidates generated by rich-diversity TTS system.
Experiments prove that rescore selection aligns closely with human preference,
which highlights acoustic BPE's potential to other speech generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures; submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GESI: Gammachirp Envelope Similarity Index for Predicting
  Intelligibility of Simulated Hearing Loss Sounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayako Yamamoto, Toshio Irino, Fuki Miyazaki, Honoka Tamaru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We proposed a new objective intelligibility measure (OIM), called the
Gammachirp Envelope Similarity Index (GESI), which can predict the speech
intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing
(NH) listeners. GESI is an intrusive method that computes the SI metric using
the gammachirp filterbank (GCFB), the modulation filterbank, and the extended
cosine similarity measure. GESI can accept the level asymmetry of the reference
and test sounds and reflect the HI listener's hearing level as it appears on
the audiogram. A unique feature of GESI is its ability to incorporate an
individual participant's listening condition into the SI prediction. We
conducted four SI experiments on male and female speech sounds in both
laboratory and crowdsourced remote environments. We then evaluated GESI and the
conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI, for their ability to predict
mean and individual SI values with and without the use of simulated HL sounds.
GESI outperformed the other OIMs in all evaluations. STOI, ESTOI, and MBSTOI
did not predict SI at all, even when using the simulated HL sounds. HASPI did
not predict the difference between the laboratory and remote experiments on
male speech sounds and the individual SI values. GESI may provide a first step
toward SI prediction for individual HI listeners whose HL is caused solely by
peripheral dysfunction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was submitted to Speech Communication on September 14,
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality Dropout for Multimodal Device Directed Speech Detection using
  Verbal and Non-Verbal Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gautam Krishna, Sameer Dharur, Oggi Rudovic, Pranay Dighe, Saurabh Adya, Ahmed Hussen Abdelaziz, Ahmed H Tewfik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Device-directed speech detection (DDSD) is the binary classification task of
distinguishing between queries directed at a voice assistant versus side
conversation or background speech. State-of-the-art DDSD systems use verbal
cues, e.g acoustic, text and/or automatic speech recognition system (ASR)
features, to classify speech as device-directed or otherwise, and often have to
contend with one or more of these modalities being unavailable when deployed in
real-world settings. In this paper, we investigate fusion schemes for DDSD
systems that can be made more robust to missing modalities. Concurrently, we
study the use of non-verbal cues, specifically prosody features, in addition to
verbal cues for DDSD. We present different approaches to combine scores and
embeddings from prosody with the corresponding verbal cues, finding that
prosody improves DDSD performance by upto 8.5% in terms of false acceptance
rate (FA) at a given fixed operating point via non-linear intermediate fusion,
while our use of modality dropout techniques improves the performance of these
models by 7.4% in terms of FA when evaluated with missing modalities during
inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Comunità, Riccardo F. Gramaccioni, Emilian Postolache, Emanuele Rodolà, Danilo Comminiello, Joshua D. Reiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound design involves creatively selecting, recording, and editing sound
effects for various media like cinema, video games, and virtual/augmented
reality. One of the most time-consuming steps when designing sound is
synchronizing audio with video. In some cases, environmental recordings from
video shoots are available, which can aid in the process. However, in video
games and animations, no reference audio exists, requiring manual annotation of
event timings from the video. We propose a system to extract repetitive actions
onsets from a video, which are then used - in conjunction with audio or textual
embeddings - to condition a diffusion model trained to generate a new
synchronized sound effects audio track. In this way, we leave complete creative
control to the sound designer while removing the burden of synchronization with
video. Furthermore, editing the onset track or changing the conditioning
embedding requires much less effort than editing the audio track itself,
simplifying the sonification process. We provide sound examples, source code,
and pretrained models to faciliate reproducibility
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11757v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11757v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavio Schneider, Ojasv Kamal, Zhijing Jin, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen the rapid development of large generative models for
text; however, much less research has explored the connection between text and
another "language" of communication -- music. Music, much like text, can convey
emotions, stories, and ideas, and has its own unique structure and syntax. In
our work, we bridge text and music via a text-to-music generation model that is
highly efficient, expressive, and can handle long-term structure. Specifically,
we develop Mo\^usai, a cascading two-stage latent diffusion model that can
generate multiple minutes of high-quality stereo music at 48kHz from textual
descriptions. Moreover, our model features high efficiency, which enables
real-time inference on a single consumer GPU with a reasonable speed. Through
experiments and property analyses, we show our model's competence over a
variety of criteria compared with existing music generation models. Lastly, to
promote the open-source culture, we provide a collection of open-source
libraries with the hope of facilitating future work in the field. We
open-source the following: Codes:
https://github.com/archinetai/audio-diffusion-pytorch; music samples for this
paper: http://bit.ly/44ozWDH; all music samples for all models:
https://bit.ly/audio-diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-22T00:00:00Z">2023-10-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An <span class="highlight-title">overview</span> of <span class="highlight-title">text-to-speech</span> systems and media applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Hasanabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Producing synthetic voice, similar to human-like sound, is an emerging
novelty of modern interactive media systems. Text-To-Speech (TTS) systems try
to generate synthetic and authentic voices via text input. Besides, well known
and familiar dubbing, announcing and narrating voices, as valuable possessions
of any media organization, can be kept forever by utilizing TTS and Voice
Conversion (VC) algorithms . The emergence of deep learning approaches has made
such TTS systems more accurate and accessible. To understand TTS systems
better, this paper investigates the key components of such systems including
text analysis, acoustic modelling and vocoding. The paper then provides details
of important state-of-the-art TTS systems based on deep learning. Finally, a
comparison is made between recently released systems in term of backbone
architecture, type of input and conversion, vocoder used and subjective
assessment (MOS). Accordingly, Tacotron 2, Transformer TTS, WaveNet and
FastSpeech 1 are among the most successful TTS systems ever released. In the
discussion section, some suggestions are made to develop a TTS system with
regard to the intended application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ABU Technical Review journal 2023/6</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MFCC-GAN Codec: A New AI-based Audio Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Hasanabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we proposed AI-based audio coding using MFCC features in an
adversarial setting. We combined a conventional encoder with an adversarial
learning decoder to better reconstruct the original waveform. Since GAN gives
implicit density estimation, therefore, such models are less prone to
overfitting. We compared our work with five well-known codecs namely AAC, AC3,
Opus, Vorbis, and Speex, performing on bitrates from 2kbps to 128kbps.
MFCCGAN_36k achieved the state-of-the-art result in terms of SNR despite a
lower bitrate in comparison to AC3_128k, AAC_112k, Vorbis_48k, Opus_48k, and
Speex_48K. On the other hand, MFCCGAN_13k also achieved high SNR=27 which is
equal to that of AC3_128k, and AAC_112k while having a significantly lower
bitrate (13 kbps). MFCCGAN_36k achieved higher NISQA-MOS results compared to
AAC_48k while having a 20% lower bitrate. Furthermore, MFCCGAN_13k obtained
NISQAMOS= 3.9 which is much higher than AAC_24k, AAC_32k, AC3_32k, and AAC_48k.
For future work, we finally suggest adopting loss functions optimizing
intelligibility and perceptual metrics in the MFCCGAN structure to improve
quality and intelligibility simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ABU Technical Review journal 2023/3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Conversational Speech Recognition by Learning Audio-textual Cross-modal
  Contextual Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Wei, Bei Li, Hang Lv, Quan Lu, Ning Jiang, <span class="highlight-author">Lei Xie</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) in conversational settings presents unique
challenges, including extracting relevant contextual information from previous
conversational turns. Due to irrelevant content, error propagation, and
redundancy, existing methods struggle to extract longer and more effective
contexts. To address this issue, we introduce a novel Conversational ASR
system, extending the Conformer encoder-decoder model with cross-modal
conversational representation. Our approach leverages a cross-modal extractor
that combines pre-trained speech and text models through a specialized encoder
and a modal-level mask input. This enables the extraction of richer historical
speech context without explicit error propagation. We also incorporate
conditional latent variational modules to learn conversational level attributes
such as role preference and topic coherence. By introducing both cross-modal
and conversational representations into the decoder, our model retains context
over longer sentences without information loss, achieving relative accuracy
improvements of 8.8% and 23% on Mandarin conversation datasets HKUST and
MagicData-RAMC, respectively, compared to the standard Conformer model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TASLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on Prosodic Entrainment in Relation to Therapist Empathy in
  Counseling Conversation <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dehua Tao, Tan Lee, Harold Chui, Sarah Luk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counseling is carried out as spoken conversation between a therapist and a
client. The empathy level expressed by the therapist is considered an important
index of the quality of counseling and often assessed by an observer or the
client. This research investigates the entrainment of speech prosody in
relation to subjectively rated empathy. Experimental results show that the
entrainment of intensity is more influential to empathy observation than that
of pitch or speech rate in client-therapist interaction. The observer and the
client have different perceptions of therapist empathy with the same entrained
phenomena in pitch and intensity. The client's intention to make adjustment on
pitch variation and intensity of speech is considered an indicator of the
client's perception of counseling quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Intrapersonal and Interpersonal Influences for Automatic
  Estimation of Therapist Empathy in Counseling Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dehua Tao, Tan Lee, Harold Chui, Sarah Luk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counseling is usually conducted through spoken conversation between a
therapist and a client. The empathy level of therapist is a key indicator of
outcomes. Presuming that therapist's empathy expression is shaped by their past
behavior and their perception of the client's behavior, we propose a model to
estimate the therapist empathy by considering both intrapersonal and
interpersonal influences. These dynamic influences are captured by applying an
attention mechanism to the therapist turn and the historical turns of both
therapist and client. Our findings suggest that the integration of dynamic
influences enhances empathy level estimation. The influence-derived embedding
should constitute a minor portion in the target turn representation for optimal
empathy estimation. The client's turns (interpersonal influence) appear to
slightly surpass the therapist's own turns (intrapersonal influence) in empathy
estimation effectiveness. It is noted that concentrating exclusively on recent
historical turns can significantly impact the estimation of therapist empathy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First-Shot Unsupervised Anomalous Sound Detection With Unknown Anomalies
  Estimated by Metadata-Assisted Audio Generation <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hejing Zhang, Qiaoxi Zhu, Jian Guan, Haohe Liu, Feiyang Xiao, Jiantong Tian, Xinhao Mei, Xubo Liu, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  First-shot (FS) unsupervised anomalous sound detection (ASD) is a brand-new
task introduced in DCASE 2023 Challenge Task 2, where the anomalous sounds for
the target machine types are unseen in training. Existing methods often rely on
the availability of normal and abnormal sound data from the target machines.
However, due to the lack of anomalous sound data for the target machine types,
it becomes challenging when adapting the existing ASD methods to the first-shot
task. In this paper, we propose a new framework for the first-shot unsupervised
ASD, where metadata-assisted audio generation is used to estimate unknown
anomalies, by utilising the available machine information (i.e., metadata and
sound data) to fine-tune a text-to-audio generation model for generating the
anomalous sounds that contain unique acoustic characteristics accounting for
each different machine types. We then use the method of Time-Weighted Frequency
domain audio Representation with Gaussian Mixture Model (TWFR-GMM) as the
backbone to achieve the first-shot unsupervised ASD. Our proposed FS-TWFR-GMM
method achieves competitive performance amongst top systems in DCASE 2023
Challenge Task 2, while requiring only 1% model parameters for detection, as
validated in our experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Beamforming for Speech Enhancement and Speaker Localization with an
  Array Response-Aware Loss Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsinyu Chang, Yicheng Hsu, Mingsian R. Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research advances in deep neural network (DNN)-based beamformers have
shown great promise for speech enhancement under adverse acoustic conditions.
Different network architectures and input features have been explored in
estimating beamforming weights. In this paper, we propose a deep beamformer
based on an efficient convolutional recurrent network (CRN) trained with a
novel ARray RespOnse-aWare (ARROW) loss function. The ARROW loss exploits the
array responses of the target and interferer by using the ground truth relative
transfer functions (RTFs). The DNN-based beamforming system, trained with ARROW
loss through supervised learning, is able to perform speech enhancement and
speaker localization jointly. Experimental results have shown that the proposed
deep beamformer, trained with the linearly weighted scale-invariant
source-to-noise ratio (SI-SNR) and ARROW loss functions, achieves superior
performance in speech enhancement and speaker localization compared to two
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Source Separation of Unknown Numbers of Single-Channel Underwater
  Acoustic Signals Based on Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11749v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11749v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Sun, Kejun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The separation of single-channel underwater acoustic signals is a challenging
problem with practical significance. Few existing studies focus on the source
separation problem with unknown numbers of signals, and how to evaluate the
performances of the systems is not yet clear. We propose a solution with a
fixed number of output channels to address these two problems, enabling it to
avoid the dimensional disaster caused by the permutation problem induced by the
alignment of outputs to targets. Specifically, we propose a two-step algorithm
based on autoencoders and a new performance evaluation method for situations
with mute channels. Experiments conducted on simulated mixtures of radiated
ship noise show that the proposed solution can achieve similar separation
performance to that attained with a known number of signals. The proposed
algorithm achieved competitive performance as two algorithms developed for
known numbers of signals, which is highly explainable and extensible and get
the state of the art under this framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 3 tables. For codes, see
  https://github.com/QinggangSUN/unknown_number_source_separation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Effective Distillation of Self-Supervised Speech Models for
  Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15631v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15631v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Wang, Changli Tang, Ziyang Ma, Zhisheng Zheng, Xie Chen, Wei-Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed great strides in self-supervised learning (SSL)
on the speech processing. The SSL model is normally pre-trained on a great
variety of unlabelled data and a large model size is preferred to increase the
modeling capacity. However, this might limit its potential applications due to
the expensive computation and memory costs introduced by the oversize model.
Miniaturization for SSL models has become an important research direction of
practical value. To this end, we explore the effective distillation of
HuBERT-based SSL models for automatic speech recognition (ASR). First, in order
to establish a strong baseline, a comprehensive study on different student
model structures is conducted. On top of this, as a supplement to the
regression loss widely adopted in previous works, a discriminative loss is
introduced for HuBERT to enhance the distillation performance, especially in
low-resource scenarios. In addition, we design a simple and effective algorithm
to distill the front-end input from waveform to Fbank feature, resulting in 17%
parameter reduction and doubling inference speed, at marginal performance
degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ASRU 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALCAP: Alignment-Augmented Music Captioner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao He, Weituo Hao, Wei-Tsung Lu, Changyou Chen, Kristina Lerman, Xuchen Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music captioning has gained significant attention in the wake of the rising
prominence of streaming media platforms. Traditional approaches often
prioritize either the audio or lyrics aspect of the music, inadvertently
ignoring the intricate interplay between the two. However, a comprehensive
understanding of music necessitates the integration of both these elements. In
this study, we delve into this overlooked realm by introducing a method to
systematically learn multimodal alignment between audio and lyrics through
contrastive learning. This not only recognizes and emphasizes the synergy
between audio and lyrics but also paves the way for models to achieve deeper
cross-modal coherence, thereby producing high-quality captions. We provide both
theoretical and empirical results demonstrating the advantage of the proposed
method, which achieves new state-of-the-art on two music captioning datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre
  Transfer <span class="chip">ICLR 2019</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1811.09620v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1811.09620v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Huang, Qiyang Li, Cem Anil, Xuchan Bao, Sageev Oore, Roger B. Grosse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the problem of musical timbre transfer, where the
goal is to manipulate the timbre of a sound sample from one instrument to match
another instrument while preserving other musical content, such as pitch,
rhythm, and loudness. In principle, one could apply image-based style transfer
techniques to a time-frequency representation of an audio signal, but this
depends on having a representation that allows independent manipulation of
timbre as well as high-quality waveform generation. We introduce TimbreTron, a
method for musical timbre transfer which applies "image" domain style transfer
to a time-frequency representation of the audio signal, and then produces a
high-quality waveform using a conditional WaveNet synthesizer. We show that the
Constant Q Transform (CQT) representation is particularly well-suited to
convolutional architectures due to its approximate pitch equivariance. Based on
human perceptual evaluations, we confirmed that TimbreTron recognizably
transferred the timbre while otherwise preserving the musical content, for both
monophonic and polyphonic samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, published as a conference paper at ICLR 2019</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An <span class="highlight-title">overview</span> of <span class="highlight-title">text-to-speech</span> systems and media applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Hasanabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Producing synthetic voice, similar to human-like sound, is an emerging
novelty of modern interactive media systems. Text-To-Speech (TTS) systems try
to generate synthetic and authentic voices via text input. Besides, well known
and familiar dubbing, announcing and narrating voices, as valuable possessions
of any media organization, can be kept forever by utilizing TTS and Voice
Conversion (VC) algorithms . The emergence of deep learning approaches has made
such TTS systems more accurate and accessible. To understand TTS systems
better, this paper investigates the key components of such systems including
text analysis, acoustic modelling and vocoding. The paper then provides details
of important state-of-the-art TTS systems based on deep learning. Finally, a
comparison is made between recently released systems in term of backbone
architecture, type of input and conversion, vocoder used and subjective
assessment (MOS). Accordingly, Tacotron 2, Transformer TTS, WaveNet and
FastSpeech 1 are among the most successful TTS systems ever released. In the
discussion section, some suggestions are made to develop a TTS system with
regard to the intended application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ABU Technical Review journal 2023/6</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MFCC-GAN Codec: A New AI-based Audio Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Hasanabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we proposed AI-based audio coding using MFCC features in an
adversarial setting. We combined a conventional encoder with an adversarial
learning decoder to better reconstruct the original waveform. Since GAN gives
implicit density estimation, therefore, such models are less prone to
overfitting. We compared our work with five well-known codecs namely AAC, AC3,
Opus, Vorbis, and Speex, performing on bitrates from 2kbps to 128kbps.
MFCCGAN_36k achieved the state-of-the-art result in terms of SNR despite a
lower bitrate in comparison to AC3_128k, AAC_112k, Vorbis_48k, Opus_48k, and
Speex_48K. On the other hand, MFCCGAN_13k also achieved high SNR=27 which is
equal to that of AC3_128k, and AAC_112k while having a significantly lower
bitrate (13 kbps). MFCCGAN_36k achieved higher NISQA-MOS results compared to
AAC_48k while having a 20% lower bitrate. Furthermore, MFCCGAN_13k obtained
NISQAMOS= 3.9 which is much higher than AAC_24k, AAC_32k, AC3_32k, and AAC_48k.
For future work, we finally suggest adopting loss functions optimizing
intelligibility and perceptual metrics in the MFCCGAN structure to improve
quality and intelligibility simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ABU Technical Review journal 2023/3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Conversational Speech Recognition by Learning Audio-textual Cross-modal
  Contextual Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Wei, Bei Li, Hang Lv, Quan Lu, Ning Jiang, <span class="highlight-author">Lei Xie</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) in conversational settings presents unique
challenges, including extracting relevant contextual information from previous
conversational turns. Due to irrelevant content, error propagation, and
redundancy, existing methods struggle to extract longer and more effective
contexts. To address this issue, we introduce a novel Conversational ASR
system, extending the Conformer encoder-decoder model with cross-modal
conversational representation. Our approach leverages a cross-modal extractor
that combines pre-trained speech and text models through a specialized encoder
and a modal-level mask input. This enables the extraction of richer historical
speech context without explicit error propagation. We also incorporate
conditional latent variational modules to learn conversational level attributes
such as role preference and topic coherence. By introducing both cross-modal
and conversational representations into the decoder, our model retains context
over longer sentences without information loss, achieving relative accuracy
improvements of 8.8% and 23% on Mandarin conversation datasets HKUST and
MagicData-RAMC, respectively, compared to the standard Conformer model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TASLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First-Shot Unsupervised Anomalous Sound Detection With Unknown Anomalies
  Estimated by Metadata-Assisted Audio Generation <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hejing Zhang, Qiaoxi Zhu, Jian Guan, Haohe Liu, Feiyang Xiao, Jiantong Tian, Xinhao Mei, Xubo Liu, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  First-shot (FS) unsupervised anomalous sound detection (ASD) is a brand-new
task introduced in DCASE 2023 Challenge Task 2, where the anomalous sounds for
the target machine types are unseen in training. Existing methods often rely on
the availability of normal and abnormal sound data from the target machines.
However, due to the lack of anomalous sound data for the target machine types,
it becomes challenging when adapting the existing ASD methods to the first-shot
task. In this paper, we propose a new framework for the first-shot unsupervised
ASD, where metadata-assisted audio generation is used to estimate unknown
anomalies, by utilising the available machine information (i.e., metadata and
sound data) to fine-tune a text-to-audio generation model for generating the
anomalous sounds that contain unique acoustic characteristics accounting for
each different machine types. We then use the method of Time-Weighted Frequency
domain audio Representation with Gaussian Mixture Model (TWFR-GMM) as the
backbone to achieve the first-shot unsupervised ASD. Our proposed FS-TWFR-GMM
method achieves competitive performance amongst top systems in DCASE 2023
Challenge Task 2, while requiring only 1% model parameters for detection, as
validated in our experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Source Separation of Unknown Numbers of Single-Channel Underwater
  Acoustic Signals Based on Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11749v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11749v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Sun, Kejun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The separation of single-channel underwater acoustic signals is a challenging
problem with practical significance. Few existing studies focus on the source
separation problem with unknown numbers of signals, and how to evaluate the
performances of the systems is not yet clear. We propose a solution with a
fixed number of output channels to address these two problems, enabling it to
avoid the dimensional disaster caused by the permutation problem induced by the
alignment of outputs to targets. Specifically, we propose a two-step algorithm
based on autoencoders and a new performance evaluation method for situations
with mute channels. Experiments conducted on simulated mixtures of radiated
ship noise show that the proposed solution can achieve similar separation
performance to that attained with a known number of signals. The proposed
algorithm achieved competitive performance as two algorithms developed for
known numbers of signals, which is highly explainable and extensible and get
the state of the art under this framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 3 tables. For codes, see
  https://github.com/QinggangSUN/unknown_number_source_separation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Effective Distillation of Self-Supervised Speech Models for
  Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15631v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15631v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Wang, Changli Tang, Ziyang Ma, Zhisheng Zheng, Xie Chen, Wei-Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed great strides in self-supervised learning (SSL)
on the speech processing. The SSL model is normally pre-trained on a great
variety of unlabelled data and a large model size is preferred to increase the
modeling capacity. However, this might limit its potential applications due to
the expensive computation and memory costs introduced by the oversize model.
Miniaturization for SSL models has become an important research direction of
practical value. To this end, we explore the effective distillation of
HuBERT-based SSL models for automatic speech recognition (ASR). First, in order
to establish a strong baseline, a comprehensive study on different student
model structures is conducted. On top of this, as a supplement to the
regression loss widely adopted in previous works, a discriminative loss is
introduced for HuBERT to enhance the distillation performance, especially in
low-resource scenarios. In addition, we design a simple and effective algorithm
to distill the front-end input from waveform to Fbank feature, resulting in 17%
parameter reduction and doubling inference speed, at marginal performance
degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ASRU 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALCAP: Alignment-Augmented Music Captioner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao He, Weituo Hao, Wei-Tsung Lu, Changyou Chen, Kristina Lerman, Xuchen Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music captioning has gained significant attention in the wake of the rising
prominence of streaming media platforms. Traditional approaches often
prioritize either the audio or lyrics aspect of the music, inadvertently
ignoring the intricate interplay between the two. However, a comprehensive
understanding of music necessitates the integration of both these elements. In
this study, we delve into this overlooked realm by introducing a method to
systematically learn multimodal alignment between audio and lyrics through
contrastive learning. This not only recognizes and emphasizes the synergy
between audio and lyrics but also paves the way for models to achieve deeper
cross-modal coherence, thereby producing high-quality captions. We provide both
theoretical and empirical results demonstrating the advantage of the proposed
method, which achieves new state-of-the-art on two music captioning datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre
  Transfer <span class="chip">ICLR 2019</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1811.09620v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1811.09620v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Huang, Qiyang Li, Cem Anil, Xuchan Bao, Sageev Oore, Roger B. Grosse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the problem of musical timbre transfer, where the
goal is to manipulate the timbre of a sound sample from one instrument to match
another instrument while preserving other musical content, such as pitch,
rhythm, and loudness. In principle, one could apply image-based style transfer
techniques to a time-frequency representation of an audio signal, but this
depends on having a representation that allows independent manipulation of
timbre as well as high-quality waveform generation. We introduce TimbreTron, a
method for musical timbre transfer which applies "image" domain style transfer
to a time-frequency representation of the audio signal, and then produces a
high-quality waveform using a conditional WaveNet synthesizer. We show that the
Constant Q Transform (CQT) representation is particularly well-suited to
convolutional architectures due to its approximate pitch equivariance. Based on
human perceptual evaluations, we confirmed that TimbreTron recognizably
transferred the timbre while otherwise preserving the musical content, for both
monophonic and polyphonic samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, published as a conference paper at ICLR 2019</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-21T00:00:00Z">2023-10-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composer Style-specific Symbolic Music Generation Using Vector Quantized
  Discrete Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Zhang, Jingjing Tang, Charalampos Saitis, György Fazekas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging Denoising Diffusion Probabilistic Models (DDPM) have become
increasingly utilised because of promising results they have achieved in
diverse generative tasks with continuous data, such as image and sound
synthesis. Nonetheless, the success of diffusion models has not been fully
extended to discrete symbolic music. We propose to combine a vector quantized
variational autoencoder (VQ-VAE) and discrete diffusion models for the
generation of symbolic music with desired composer styles. The trained VQ-VAE
can represent symbolic music as a sequence of indexes that correspond to
specific entries in a learned codebook. Subsequently, a discrete diffusion
model is used to model the VQ-VAE's discrete latent space. The diffusion model
is trained to generate intermediate music sequences consisting of codebook
indexes, which are then decoded to symbolic music using the VQ-VAE's decoder.
The results demonstrate our model can generate symbolic music with target
composer styles that meet the given conditions with a high accuracy of 72.36%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Diffusion GAN Model for Symbolic Music Generation Controlled by
  Emotions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Zhang, György Fazekas, Charalampos Saitis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown promising results for a wide range of generative
tasks with continuous data, such as image and audio synthesis. However, little
progress has been made on using diffusion models to generate discrete symbolic
music because this new class of generative models are not well suited for
discrete data while its iterative sampling process is computationally
expensive. In this work, we propose a diffusion model combined with a
Generative Adversarial Network, aiming to (i) alleviate one of the remaining
challenges in algorithmic music generation which is the control of generation
towards a target emotion, and (ii) mitigate the slow sampling drawback of
diffusion models applied to symbolic music generation. We first used a trained
Variational Autoencoder to obtain embeddings of a symbolic music dataset with
emotion labels and then used those to train a diffusion model. Our results
demonstrate the successful control of our diffusion model to generate symbolic
music with a desired emotion. Our model achieves several orders of magnitude
improvement in computational cost, requiring merely four time steps to denoise
while the steps required by current state-of-the-art diffusion models for
symbolic music generation is in the order of thousands.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal convolutional neural networks to generate a head-related
  impulse response from one direction to another 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuki Kobayashi, Yoshiko Maruyama, Isao Nambu, Shohei Yano, Yasuhiro Wada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual sound synthesis is a technology that allows users to perceive spatial
sound through headphones or earphones. However, accurate virtual sound requires
an individual head-related transfer function (HRTF), which can be difficult to
measure due to the need for a specialized environment. In this study, we
proposed a method to generate HRTFs from one direction to the other. To this
end, we used temporal convolutional neural networks (TCNs) to generate
head-related impulse responses (HRIRs). To train the TCNs, publicly available
datasets in the horizontal plane were used. Using the trained networks, we
successfully generated HRIRs for directions other than the front direction in
the dataset. We found that the proposed method successfully generated HRIRs for
publicly available datasets. To test the generalization of the method, we
measured the HRIRs of a new dataset and tested whether the trained networks
could be used for this new dataset. Although the similarity evaluated by
spectral distortion was slightly degraded, behavioral experiments with human
participants showed that the generated HRIRs were equivalent to the measured
ones. These results suggest that the proposed TCNs can be used to generate
personalized HRIRs from one direction to another, which could contribute to the
personalization of virtual sound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Pronunciation Assessment -- A <span class="highlight-title">Review</span> <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassine El Kheir, Ahmed Ali, Shammur Absar Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pronunciation assessment and its application in computer-aided pronunciation
training (CAPT) have seen impressive progress in recent years. With the rapid
growth in language processing and deep learning over the past few years, there
is a need for an updated review. In this paper, we review methods employed in
pronunciation assessment for both phonemic and prosodic. We categorize the main
challenges observed in prominent research trends, and highlight existing
limitations, and available resources. This is followed by a discussion of the
remaining challenges and possible directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, accepted to EMNLP Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System <span class="chip">EMNLP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Waheed, Bashar Talafha, Peter Suvellin, Abdelrahman Elmadney, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arabic is a complex language with many varieties and dialects spoken by over
450 millions all around the world. Due to the linguistic diversity and
variations, it is challenging to build a robust and generalized ASR system for
Arabic. In this work, we address this gap by developing and demoing a system,
dubbed VoxArabica, for dialect identification (DID) as well as automatic speech
recognition (ASR) of Arabic. We train a wide range of models such as HuBERT
(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR
tasks. Our DID models are trained to identify 17 different dialects in addition
to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.
Additionally, for the remaining dialects in ASR, we provide the option to
choose various models such as Whisper and MMS in a zero-shot setting. We
integrate these models into a single web interface with diverse features such
as audio recording, file upload, model selection, and the option to raise flags
for incorrect outputs. Overall, we believe VoxArabica will be useful for a wide
range of audiences concerned with Arabic research. Our system is currently
running at https://cdce-206-12-100-168.ngrok.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ArabicNLP conference co-located with EMNLP'23. First
  three authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instabilities in Convnets for Raw Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Haider, Vincent Lostanlen, Martin Ehler, Peter Balazs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What makes waveform-based deep learning so hard? Despite numerous attempts at
training convolutional neural networks (convnets) for filterbank design, they
often fail to outperform hand-crafted baselines. These baselines are linear
time-invariant systems: as such, they can be approximated by convnets with wide
receptive fields. Yet, in practice, gradient-based optimization leads to
suboptimal approximations. In our article, we approach this phenomenon from the
perspective of initialization. We present a theory of large deviations for the
energy response of FIR filterbanks with random Gaussian weights. We find that
deviations worsen for large filters and locally periodic input signals, which
are both typical for audio signal processing applications. Numerical
simulations align with our theory and suggest that the condition number of a
convolutional layer follows a logarithmic scaling law between the number and
length of the filters, which is reminiscent of discrete wavelet bases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures, 1 page appendix, under review for IEEE SPL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on
  Respiratory Sound Classification <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14032v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14032v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangmin Bae, June-Woo Kim, Won-Yang Cho, Hyerim Baek, Soyoun Son, Byungjo Lee, Changwan Ha, Kyongpil Tae, Sungnyun Kim, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Respiratory sound contains crucial information for the early diagnosis of
fatal lung diseases. Since the COVID-19 pandemic, there has been a growing
interest in contact-free medical care based on electronic stethoscopes. To this
end, cutting-edge deep learning models have been developed to diagnose lung
diseases; however, it is still challenging due to the scarcity of medical data.
In this study, we demonstrate that the pretrained model on large-scale visual
and audio datasets can be generalized to the respiratory sound classification
task. In addition, we introduce a straightforward Patch-Mix augmentation, which
randomly mixes patches between different samples, with Audio Spectrogram
Transformer (AST). We further propose a novel and effective Patch-Mix
Contrastive Learning to distinguish the mixed representations in the latent
space. Our method achieves state-of-the-art performance on the ICBHI dataset,
outperforming the prior leading score by an improvement of 4.08%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INTERSPEECH 2023, Code URL:
  https://github.com/raymin0223/patch-mix_contrastive_learning</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composer Style-specific Symbolic Music Generation Using Vector Quantized
  Discrete Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Zhang, Jingjing Tang, Charalampos Saitis, György Fazekas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging Denoising Diffusion Probabilistic Models (DDPM) have become
increasingly utilised because of promising results they have achieved in
diverse generative tasks with continuous data, such as image and sound
synthesis. Nonetheless, the success of diffusion models has not been fully
extended to discrete symbolic music. We propose to combine a vector quantized
variational autoencoder (VQ-VAE) and discrete diffusion models for the
generation of symbolic music with desired composer styles. The trained VQ-VAE
can represent symbolic music as a sequence of indexes that correspond to
specific entries in a learned codebook. Subsequently, a discrete diffusion
model is used to model the VQ-VAE's discrete latent space. The diffusion model
is trained to generate intermediate music sequences consisting of codebook
indexes, which are then decoded to symbolic music using the VQ-VAE's decoder.
The results demonstrate our model can generate symbolic music with target
composer styles that meet the given conditions with a high accuracy of 72.36%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Diffusion GAN Model for Symbolic Music Generation Controlled by
  Emotions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Zhang, György Fazekas, Charalampos Saitis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown promising results for a wide range of generative
tasks with continuous data, such as image and audio synthesis. However, little
progress has been made on using diffusion models to generate discrete symbolic
music because this new class of generative models are not well suited for
discrete data while its iterative sampling process is computationally
expensive. In this work, we propose a diffusion model combined with a
Generative Adversarial Network, aiming to (i) alleviate one of the remaining
challenges in algorithmic music generation which is the control of generation
towards a target emotion, and (ii) mitigate the slow sampling drawback of
diffusion models applied to symbolic music generation. We first used a trained
Variational Autoencoder to obtain embeddings of a symbolic music dataset with
emotion labels and then used those to train a diffusion model. Our results
demonstrate the successful control of our diffusion model to generate symbolic
music with a desired emotion. Our model achieves several orders of magnitude
improvement in computational cost, requiring merely four time steps to denoise
while the steps required by current state-of-the-art diffusion models for
symbolic music generation is in the order of thousands.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal convolutional neural networks to generate a head-related
  impulse response from one direction to another 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuki Kobayashi, Yoshiko Maruyama, Isao Nambu, Shohei Yano, Yasuhiro Wada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual sound synthesis is a technology that allows users to perceive spatial
sound through headphones or earphones. However, accurate virtual sound requires
an individual head-related transfer function (HRTF), which can be difficult to
measure due to the need for a specialized environment. In this study, we
proposed a method to generate HRTFs from one direction to the other. To this
end, we used temporal convolutional neural networks (TCNs) to generate
head-related impulse responses (HRIRs). To train the TCNs, publicly available
datasets in the horizontal plane were used. Using the trained networks, we
successfully generated HRIRs for directions other than the front direction in
the dataset. We found that the proposed method successfully generated HRIRs for
publicly available datasets. To test the generalization of the method, we
measured the HRIRs of a new dataset and tested whether the trained networks
could be used for this new dataset. Although the similarity evaluated by
spectral distortion was slightly degraded, behavioral experiments with human
participants showed that the generated HRIRs were equivalent to the measured
ones. These results suggest that the proposed TCNs can be used to generate
personalized HRIRs from one direction to another, which could contribute to the
personalization of virtual sound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwG-former: Sliding-window Graph Convolutional Network Integrated with
  Conformer for Sound Event Localization and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiming Huang, Qinghua Huang, Liyan Ma, Zhengyu Chen, Chuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound event localization and detection (SELD) is a joint task of sound event
detection (SED) and direction of arrival (DoA) estimation. SED mainly relies on
temporal dependencies to distinguish different sound classes, while DoA
estimation depends on spatial correlations to estimate source directions. To
jointly optimize two subtasks, the SELD system should extract spatial
correlations and model temporal dependencies simultaneously. However, numerous
models mainly extract spatial correlations and model temporal dependencies
separately. In this paper, the interdependence of spatial-temporal information
in audio signals is exploited for simultaneous extraction to enhance the model
performance. In response, a novel graph representation leveraging graph
convolutional network (GCN) in non-Euclidean space is developed to extract
spatial-temporal information concurrently. A sliding-window graph (SwG) module
is designed based on the graph representation. It exploits sliding-windows with
different sizes to learn temporal context information and dynamically
constructs graph vertices in the frequency-channel (F-C) domain to capture
spatial correlations. Furthermore, as the cornerstone of message passing, a
robust Conv2dAgg function is proposed and embedded into the SwG module to
aggregate the features of neighbor vertices. To improve the performance of SELD
in a natural spatial acoustic environment, a general and efficient SwG-former
model is proposed by integrating the SwG module with the Conformer. It exhibits
superior performance in comparison to recent advanced SELD models. To further
validate the generality and efficiency of the SwG-former, it is seamlessly
integrated into the event-independent network version 2 (EINV2) called
SwG-EINV2. The SwG-EINV2 surpasses the state-of-the-art (SOTA) methods under
the same acoustic environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Pronunciation Assessment -- A <span class="highlight-title">Review</span> <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassine El Kheir, Ahmed Ali, Shammur Absar Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pronunciation assessment and its application in computer-aided pronunciation
training (CAPT) have seen impressive progress in recent years. With the rapid
growth in language processing and deep learning over the past few years, there
is a need for an updated review. In this paper, we review methods employed in
pronunciation assessment for both phonemic and prosodic. We categorize the main
challenges observed in prominent research trends, and highlight existing
limitations, and available resources. This is followed by a discussion of the
remaining challenges and possible directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, accepted to EMNLP Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System <span class="chip">EMNLP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Waheed, Bashar Talafha, Peter Suvellin, Abdelrahman Elmadney, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arabic is a complex language with many varieties and dialects spoken by over
450 millions all around the world. Due to the linguistic diversity and
variations, it is challenging to build a robust and generalized ASR system for
Arabic. In this work, we address this gap by developing and demoing a system,
dubbed VoxArabica, for dialect identification (DID) as well as automatic speech
recognition (ASR) of Arabic. We train a wide range of models such as HuBERT
(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR
tasks. Our DID models are trained to identify 17 different dialects in addition
to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.
Additionally, for the remaining dialects in ASR, we provide the option to
choose various models such as Whisper and MMS in a zero-shot setting. We
integrate these models into a single web interface with diverse features such
as audio recording, file upload, model selection, and the option to raise flags
for incorrect outputs. Overall, we believe VoxArabica will be useful for a wide
range of audiences concerned with Arabic research. Our system is currently
running at https://cdce-206-12-100-168.ngrok.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ArabicNLP conference co-located with EMNLP'23. First
  three authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instabilities in Convnets for Raw Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Haider, Vincent Lostanlen, Martin Ehler, Peter Balazs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What makes waveform-based deep learning so hard? Despite numerous attempts at
training convolutional neural networks (convnets) for filterbank design, they
often fail to outperform hand-crafted baselines. These baselines are linear
time-invariant systems: as such, they can be approximated by convnets with wide
receptive fields. Yet, in practice, gradient-based optimization leads to
suboptimal approximations. In our article, we approach this phenomenon from the
perspective of initialization. We present a theory of large deviations for the
energy response of FIR filterbanks with random Gaussian weights. We find that
deviations worsen for large filters and locally periodic input signals, which
are both typical for audio signal processing applications. Numerical
simulations align with our theory and suggest that the condition number of a
convolutional layer follows a logarithmic scaling law between the number and
length of the filters, which is reminiscent of discrete wavelet bases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures, 1 page appendix, under review for IEEE SPL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on
  Respiratory Sound Classification <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14032v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14032v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangmin Bae, June-Woo Kim, Won-Yang Cho, Hyerim Baek, Soyoun Son, Byungjo Lee, Changwan Ha, Kyongpil Tae, Sungnyun Kim, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Respiratory sound contains crucial information for the early diagnosis of
fatal lung diseases. Since the COVID-19 pandemic, there has been a growing
interest in contact-free medical care based on electronic stethoscopes. To this
end, cutting-edge deep learning models have been developed to diagnose lung
diseases; however, it is still challenging due to the scarcity of medical data.
In this study, we demonstrate that the pretrained model on large-scale visual
and audio datasets can be generalized to the respiratory sound classification
task. In addition, we introduce a straightforward Patch-Mix augmentation, which
randomly mixes patches between different samples, with Audio Spectrogram
Transformer (AST). We further propose a novel and effective Patch-Mix
Contrastive Learning to distinguish the mixed representations in the latent
space. Our method achieves state-of-the-art performance on the ICBHI dataset,
outperforming the prior leading score by an improvement of 4.08%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INTERSPEECH 2023, Code URL:
  https://github.com/raymin0223/patch-mix_contrastive_learning</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-20T00:00:00Z">2023-10-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural domain alignment for spoken language recognition based on optimal
  transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain shift poses a significant challenge in cross-domain spoken language
recognition (SLR) by reducing its effectiveness. Unsupervised domain adaptation
(UDA) algorithms have been explored to address domain shifts in SLR without
relying on class labels in the target domain. One successful UDA approach
focuses on learning domain-invariant representations to align feature
distributions between domains. However, disregarding the class structure during
the learning process of domain-invariant representations can result in
over-alignment, negatively impacting the classification task. To overcome this
limitation, we propose an optimal transport (OT)-based UDA algorithm for a
cross-domain SLR, leveraging the distribution geometry structure-aware property
of OT. An OT-based discrepancy measure on a joint distribution over feature and
label information is considered during domain alignment in OT-based UDA. Our
previous study discovered that completely aligning the distributions between
the source and target domains can introduce a negative transfer, where classes
or irrelevant classes from the source domain map to a different class in the
target domain during distribution alignment. This negative transfer degrades
the performance of the adaptive model. To mitigate this issue, we introduce
coupling-weighted partial optimal transport (POT) within our UDA framework for
SLR, where soft weighting on the OT coupling based on transport cost is
adaptively set during domain alignment. A cross-domain SLR task was used in the
experiments to evaluate the proposed UDA. The results demonstrated that our
proposed UDA algorithm significantly improved the performance over existing UDA
algorithms in a cross-channel SLR task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Stage Triplet Loss Training with Curriculum Augmentation for
  Audio-Visual Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghuo Zeng, Kazushi Ikeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cross-modal retrieval model leverages the potential of triple loss
optimization to learn robust embedding spaces. However, existing methods often
train these models in a singular pass, overlooking the distinction between
semi-hard and hard triples in the optimization process. The oversight of not
distinguishing between semi-hard and hard triples leads to suboptimal model
performance. In this paper, we introduce a novel approach rooted in curriculum
learning to address this problem. We propose a two-stage training paradigm that
guides the model's learning process from semi-hard to hard triplets. In the
first stage, the model is trained with a set of semi-hard triplets, starting
from a low-loss base. Subsequently, in the second stage, we augment the
embeddings using an interpolation technique. This process identifies potential
hard negatives, alleviating issues arising from high-loss functions due to a
scarcity of hard triples. Our approach then applies hard triplet mining in the
augmented embedding space to further optimize the model. Extensive experimental
results conducted on two audio-visual datasets show a significant improvement
of approximately 9.8% in terms of average Mean Average Precision (MAP) over the
current state-of-the-art method, MSNSCA, for the Audio-Visual Cross-Modal
Retrieval (AV-CMR) task on the AVE dataset, indicating the effectiveness of our
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Definition-independent Formalization of Soundscapes: Towards a Formal
  Methodology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikel D. Jedrusiak, Thomas Harweg, Timo Haselhoff, Bryce T. Lawrence, Susanne Moebus, Frank Weichert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soundscapes have been studied by researchers from various disciplines, each
with different perspectives, goals, approaches, and terminologies. Accordingly,
depending on the field, the concept of a soundscape's components changes,
consequently changing the basic definition. This results in complicating
interdisciplinary communication and comparison of results. Especially when
soundscape-unrelated research areas are involved. For this reason, we present a
potential formalization that is independent of the underlying soundscape
definition, with the goal of being able to capture the heterogeneous structure
of the data as well as the different ideologies in one model. In an exemplary
analysis of frequency correlation matrices for land use type detection as an
alternative to features like MFCCs, we show a practical application of our
presented formalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music Augmentation and Denoising For Peak-Based Audio Fingerprinting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Akesbi, Dorian Desblancs, Benjamin Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio fingerprinting is a well-established solution for song identification
from short recording excerpts. Popular methods rely on the extraction of sparse
representations, generally spectral peaks, and have proven to be accurate,
fast, and scalable to large collections. However, real-world applications of
audio identification often happen in noisy environments, which can cause these
systems to fail. In this work, we tackle this problem by introducing and
releasing a new audio augmentation pipeline that adds noise to music snippets
in a realistic way, by stochastically mimicking real-world scenarios. We then
propose and release a deep learning model that removes noisy components from
spectrograms in order to improve peak-based fingerprinting systems' accuracy.
We show that the addition of our model improves the identification performance
of commonly used audio fingerprinting systems, even under noisy conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SALMONN: Towards Generic Hearing Abilities for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hearing is arguably an essential ability of artificial intelligence (AI)
agents in the physical world, which refers to the perception and understanding
of general auditory information consisting of at least three types of sounds:
speech, audio events, and music. In this paper, we propose SALMONN, a speech
audio language music open neural network, built by integrating a pre-trained
text-based large language model (LLM) with speech and audio encoders into a
single multimodal model. SALMONN enables the LLM to directly process and
understand general audio inputs and achieve competitive performances on a
number of speech and audio tasks used in training, such as automatic speech
recognition and translation, auditory-information-based question answering,
emotion recognition, speaker verification, and music and audio captioning
\textit{etc.} SALMONN also has a diverse set of emergent abilities unseen in
the training, which includes but is not limited to speech translation to
untrained languages, speech-based slot filling, spoken-query-based question
answering, audio-based storytelling, and speech audio co-reasoning
\textit{etc}. The presence of the cross-modal emergent abilities is studied,
and a novel few-shot activation tuning approach is proposed to activate such
abilities of SALMONN. To our knowledge, SALMONN is the first model of its type
and can be regarded as a step towards AI with generic hearing abilities. An
interactive demo of SALMONN is available at
\texttt{\url{https://github.com/bytedance/SALMONN}}, and the training code and
model checkpoints will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Language Encoder of Contrastive Cross-modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjie Zhao, Junya Ono, Zhi Zhong, Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Wei-Hsiang Liao, Takashi Shibuya, Hiromi Wakaki, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive cross-modal models such as CLIP and CLAP aid various
vision-language (VL) and audio-language (AL) tasks. However, there has been
limited investigation of and improvement in their language encoder, which is
the central component of encoding natural language descriptions of image/audio
into vector representations. We extensively evaluate how unsupervised and
supervised sentence embedding training affect language encoder quality and
cross-modal task performance. In VL pretraining, we found that sentence
embedding training language encoder quality and aids in cross-modal tasks,
improving contrastive VL models such as CyCLIP. In contrast, AL pretraining
benefits less from sentence embedding training, which may result from the
limited amount of pretraining data. We analyze the representation spaces to
understand the strengths of sentence embedding training, and find that it
improves text-space uniformity, at the cost of decreased cross-modal alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Yet Another Model for Arabic Dialect Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajinkya Kulkarni, Hanan Aldarmaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we describe a spoken Arabic dialect identification (ADI) model
for Arabic that consistently outperforms previously published results on two
benchmark datasets: ADI-5 and ADI-17. We explore two architectural variations:
ResNet and ECAPA-TDNN, coupled with two types of acoustic features: MFCCs and
features exratected from the pre-trained self-supervised model UniSpeech-SAT
Large, as well as a fusion of all four variants. We find that individually,
ECAPA-TDNN network outperforms ResNet, and models with UniSpeech-SAT features
outperform models with MFCCs by a large margin. Furthermore, a fusion of all
four variants consistently outperforms individual models. Our best models
outperform previously reported results on both datasets, with accuracies of
84.7% and 96.9% on ADI-5 and ADI-17, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCEPTED AT ArabicNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-label Open-set Audio Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sripathi Sridhar, Mark Cartwright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current audio classification models have small class vocabularies relative to
the large number of sound event classes of interest in the real world. Thus,
they provide a limited view of the world that may miss important yet unexpected
or unknown sound events. To address this issue, open-set audio classification
techniques have been developed to detect sound events from unknown classes.
Although these methods have been applied to a multi-class context in audio,
such as sound scene classification, they have yet to be investigated for
polyphonic audio in which sound events overlap, requiring the use of
multi-label models. In this study, we establish the problem of multi-label
open-set audio classification by creating a dataset with varying unknown class
distributions and evaluating baseline approaches built upon existing
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the Workshop on Detection and Classification of Acoustic
  Scenes and Events, 2023 (DCASE 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Transfer Learning Method Utilizing Acoustic and Vibration
  Signals for Rotating Machinery Fault Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongliang Chen, Zhuofei Huang, Wenxiong Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fault diagnosis of rotating machinery plays a important role for the safety
and stability of modern industrial systems. However, there is a distribution
discrepancy between training data and data of real-world operation scenarios,
which causing the decrease of performance of existing systems. This paper
proposed a transfer learning based method utilizing acoustic and vibration
signal to address this distribution discrepancy. We designed the acoustic and
vibration feature fusion MAVgram to offer richer and more reliable information
of faults, coordinating with a DNN-based classifier to obtain more effective
diagnosis representation. The backbone was pre-trained and then fine-tuned to
obtained excellent performance of the target task. Experimental results
demonstrate the effectiveness of the proposed method, and achieved improved
performance compared to STgram-MFN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-latency Speech Enhancement via Speech Token Generation <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaying Xue, Xiulian Peng, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing deep learning based speech enhancement mainly employ a data-driven
approach, which leverage large amounts of data with a variety of noise types to
achieve noise removal from noisy signal. However, the high dependence on the
data limits its generalization on the unseen complex noises in real-life
environment. In this paper, we focus on the low-latency scenario and regard
speech enhancement as a speech generation problem conditioned on the noisy
signal, where we generate clean speech instead of identifying and removing
noises. Specifically, we propose a conditional generative framework for speech
enhancement, which models clean speech by acoustic codes of a neural speech
codec and generates the speech codes conditioned on past noisy frames in an
auto-regressive way. Moreover, we propose an explicit-alignment approach to
align noisy frames with the generated speech tokens to improve the robustness
and scalability to different input lengths. Different from other methods that
leverage multiple stages to generate speech codes, we leverage a single-stage
speech generation approach based on the TF-Codec neural codec to achieve high
speech quality with low latency. Extensive results on both synthetic and
real-recorded test set show its superiority over data-driven approaches in
terms of noise robustness and temporal speech coherence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, submitted to ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effect of Spoken Language on Speech Enhancement using
  Self-Supervised Speech Representation Loss Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.14502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.14502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Close, Thomas Hain, Stefan Goetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in the field of speech enhancement (SE) has involved the use of
self-supervised speech representations (SSSRs) as feature transformations in
loss functions. However, in prior work, very little attention has been paid to
the relationship between the language of the audio used to train the
self-supervised representation and that used to train the SE system.
Enhancement models trained using a loss function which incorporates a
self-supervised representation that shares exactly the language of the noisy
data used to train the SE system show better performance than those which do
not match exactly. This may lead to enhancement systems which are language
specific and as such do not generalise well to unseen languages, unlike models
trained using traditional spectrogram or time domain loss functions. In this
work, SE models are trained and tested on a number of different languages, with
self-supervised representations which themselves are trained using different
language combinations and with differing network structures as loss function
representations. These models are then tested across unseen languages and their
performances are analysed. It is found that the training language of the
self-supervised representation appears to have a minor effect on enhancement
performance, the amount of training data of a particular language, however,
greatly affects performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WASPAA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spoken Question Answering and Speech Continuation Using
  Spectrogram-Powered LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15255v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15255v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, Michelle Tadmor Ramanovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to adapting pre-trained large language models
(LLMs) to perform question answering (QA) and speech continuation. By endowing
the LLM with a pre-trained speech encoder, our model becomes able to take
speech inputs and generate speech outputs. The entire system is trained
end-to-end and operates directly on spectrograms, simplifying our architecture.
Key to our approach is a training objective that jointly supervises speech
recognition, text continuation, and speech synthesis using only paired
speech-text pairs, enabling a `cross-modal' chain-of-thought within a single
decoding pass. Our method surpasses existing spoken language models in speaker
preservation and semantic coherence. Furthermore, the proposed model improves
upon direct initialization in retaining the knowledge of the original LLM as
demonstrated through spoken QA datasets. Audio samples can be found at
https://michelleramanovich.github.io/spectron/spectron
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural domain alignment for spoken language recognition based on optimal
  transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain shift poses a significant challenge in cross-domain spoken language
recognition (SLR) by reducing its effectiveness. Unsupervised domain adaptation
(UDA) algorithms have been explored to address domain shifts in SLR without
relying on class labels in the target domain. One successful UDA approach
focuses on learning domain-invariant representations to align feature
distributions between domains. However, disregarding the class structure during
the learning process of domain-invariant representations can result in
over-alignment, negatively impacting the classification task. To overcome this
limitation, we propose an optimal transport (OT)-based UDA algorithm for a
cross-domain SLR, leveraging the distribution geometry structure-aware property
of OT. An OT-based discrepancy measure on a joint distribution over feature and
label information is considered during domain alignment in OT-based UDA. Our
previous study discovered that completely aligning the distributions between
the source and target domains can introduce a negative transfer, where classes
or irrelevant classes from the source domain map to a different class in the
target domain during distribution alignment. This negative transfer degrades
the performance of the adaptive model. To mitigate this issue, we introduce
coupling-weighted partial optimal transport (POT) within our UDA framework for
SLR, where soft weighting on the OT coupling based on transport cost is
adaptively set during domain alignment. A cross-domain SLR task was used in the
experiments to evaluate the proposed UDA. The results demonstrated that our
proposed UDA algorithm significantly improved the performance over existing UDA
algorithms in a cross-channel SLR task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Stage Triplet Loss Training with Curriculum Augmentation for
  Audio-Visual Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghuo Zeng, Kazushi Ikeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cross-modal retrieval model leverages the potential of triple loss
optimization to learn robust embedding spaces. However, existing methods often
train these models in a singular pass, overlooking the distinction between
semi-hard and hard triples in the optimization process. The oversight of not
distinguishing between semi-hard and hard triples leads to suboptimal model
performance. In this paper, we introduce a novel approach rooted in curriculum
learning to address this problem. We propose a two-stage training paradigm that
guides the model's learning process from semi-hard to hard triplets. In the
first stage, the model is trained with a set of semi-hard triplets, starting
from a low-loss base. Subsequently, in the second stage, we augment the
embeddings using an interpolation technique. This process identifies potential
hard negatives, alleviating issues arising from high-loss functions due to a
scarcity of hard triples. Our approach then applies hard triplet mining in the
augmented embedding space to further optimize the model. Extensive experimental
results conducted on two audio-visual datasets show a significant improvement
of approximately 9.8% in terms of average Mean Average Precision (MAP) over the
current state-of-the-art method, MSNSCA, for the Audio-Visual Cross-Modal
Retrieval (AV-CMR) task on the AVE dataset, indicating the effectiveness of our
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRTF Interpolation using a Spherical Neural Process Meta-Learner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etienne Thuillier, Craig Jin, Vesa Välimäki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several individualization methods have recently been proposed to estimate a
subject's Head-Related Transfer Function (HRTF) using convenient input
modalities such as anthropometric measurements or pinnae photographs. There
exists a need for adaptively correcting the estimation error committed by such
methods using a few data point samples from the subject's HRTF, acquired using
acoustic measurements or perceptual feedback. To this end, we introduce a
Convolutional Conditional Neural Process meta-learner specialized in HRTF error
interpolation. In particular, the model includes a Spherical Convolutional
Neural Network component to accommodate the spherical geometry of HRTF data. It
also exploits potential symmetries between the HRTF's left and right channels
about the median axis. In this work, we evaluate the proposed model's
performance purely on time-aligned spectrum interpolation grounds under a
simplified setup where a generic population-mean HRTF forms the initial
estimates prior to corrections instead of individualized ones. The trained
model achieves up to 3 dB relative error reduction compared to state-of-the-art
interpolation methods despite being trained using only 85 subjects. This
improvement translates up to nearly a halving of the data point count required
to achieve comparable accuracy, in particular from 50 to 28 points to reach an
average of -20 dB relative error per interpolated feature. Moreover, we show
that the trained model provides well-calibrated uncertainty estimates.
Accordingly, such estimates can inform the sequential decision problem of
acquiring as few correcting HRTF data points as needed to meet a desired level
of HRTF individualization accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. 11 figures. Submitted for publication in IEEE/ACM
  Transactions on Audio, Speech and Language Processing (T-ASL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenDistiller: Distilling Pre-trained Language Models based on Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingying Gao, Shilei Zhang, Zihao Cui, Yanhan Xu, Chao Deng, Junlan Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-trained models such as HuBERT and WavLM leverage
unlabeled speech data for representation learning and offer significantly
improve for numerous downstream tasks. Despite the success of these methods,
their large memory and strong computational requirements hinder their
application on resource restricted devices. Therefore, this paper introduces
GenDistiller, a novel knowledge distillation framework to distill hidden
representations from teacher network based on generative language model. The
generative structure enables the proposed model to generate the target teacher
hidden layers autoregressively, considering the interactions between hidden
layers without instroducing additional inputs. A two-dimensional attention
mechanism is implemented to ensure the causality of hidden layers, while
preserving bidirectional attention in the time dimension. Experiments reveal
the advantage of the generative distiller over the baseline system that
predicts the hidden layers of teacher network directly without a generatvie
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Definition-independent Formalization of Soundscapes: Towards a Formal
  Methodology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikel D. Jedrusiak, Thomas Harweg, Timo Haselhoff, Bryce T. Lawrence, Susanne Moebus, Frank Weichert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soundscapes have been studied by researchers from various disciplines, each
with different perspectives, goals, approaches, and terminologies. Accordingly,
depending on the field, the concept of a soundscape's components changes,
consequently changing the basic definition. This results in complicating
interdisciplinary communication and comparison of results. Especially when
soundscape-unrelated research areas are involved. For this reason, we present a
potential formalization that is independent of the underlying soundscape
definition, with the goal of being able to capture the heterogeneous structure
of the data as well as the different ideologies in one model. In an exemplary
analysis of frequency correlation matrices for land use type detection as an
alternative to features like MFCCs, we show a practical application of our
presented formalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music Augmentation and Denoising For Peak-Based Audio Fingerprinting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Akesbi, Dorian Desblancs, Benjamin Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio fingerprinting is a well-established solution for song identification
from short recording excerpts. Popular methods rely on the extraction of sparse
representations, generally spectral peaks, and have proven to be accurate,
fast, and scalable to large collections. However, real-world applications of
audio identification often happen in noisy environments, which can cause these
systems to fail. In this work, we tackle this problem by introducing and
releasing a new audio augmentation pipeline that adds noise to music snippets
in a realistic way, by stochastically mimicking real-world scenarios. We then
propose and release a deep learning model that removes noisy components from
spectrograms in order to improve peak-based fingerprinting systems' accuracy.
We show that the addition of our model improves the identification performance
of commonly used audio fingerprinting systems, even under noisy conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SALMONN: Towards Generic Hearing Abilities for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hearing is arguably an essential ability of artificial intelligence (AI)
agents in the physical world, which refers to the perception and understanding
of general auditory information consisting of at least three types of sounds:
speech, audio events, and music. In this paper, we propose SALMONN, a speech
audio language music open neural network, built by integrating a pre-trained
text-based large language model (LLM) with speech and audio encoders into a
single multimodal model. SALMONN enables the LLM to directly process and
understand general audio inputs and achieve competitive performances on a
number of speech and audio tasks used in training, such as automatic speech
recognition and translation, auditory-information-based question answering,
emotion recognition, speaker verification, and music and audio captioning
\textit{etc.} SALMONN also has a diverse set of emergent abilities unseen in
the training, which includes but is not limited to speech translation to
untrained languages, speech-based slot filling, spoken-query-based question
answering, audio-based storytelling, and speech audio co-reasoning
\textit{etc}. The presence of the cross-modal emergent abilities is studied,
and a novel few-shot activation tuning approach is proposed to activate such
abilities of SALMONN. To our knowledge, SALMONN is the first model of its type
and can be regarded as a step towards AI with generic hearing abilities. An
interactive demo of SALMONN is available at
\texttt{\url{https://github.com/bytedance/SALMONN}}, and the training code and
model checkpoints will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Language Encoder of Contrastive Cross-modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjie Zhao, Junya Ono, Zhi Zhong, Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Wei-Hsiang Liao, Takashi Shibuya, Hiromi Wakaki, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive cross-modal models such as CLIP and CLAP aid various
vision-language (VL) and audio-language (AL) tasks. However, there has been
limited investigation of and improvement in their language encoder, which is
the central component of encoding natural language descriptions of image/audio
into vector representations. We extensively evaluate how unsupervised and
supervised sentence embedding training affect language encoder quality and
cross-modal task performance. In VL pretraining, we found that sentence
embedding training language encoder quality and aids in cross-modal tasks,
improving contrastive VL models such as CyCLIP. In contrast, AL pretraining
benefits less from sentence embedding training, which may result from the
limited amount of pretraining data. We analyze the representation spaces to
understand the strengths of sentence embedding training, and find that it
improves text-space uniformity, at the cost of decreased cross-modal alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Yet Another Model for Arabic Dialect Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajinkya Kulkarni, Hanan Aldarmaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we describe a spoken Arabic dialect identification (ADI) model
for Arabic that consistently outperforms previously published results on two
benchmark datasets: ADI-5 and ADI-17. We explore two architectural variations:
ResNet and ECAPA-TDNN, coupled with two types of acoustic features: MFCCs and
features exratected from the pre-trained self-supervised model UniSpeech-SAT
Large, as well as a fusion of all four variants. We find that individually,
ECAPA-TDNN network outperforms ResNet, and models with UniSpeech-SAT features
outperform models with MFCCs by a large margin. Furthermore, a fusion of all
four variants consistently outperforms individual models. Our best models
outperform previously reported results on both datasets, with accuracies of
84.7% and 96.9% on ADI-5 and ADI-17, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCEPTED AT ArabicNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-label Open-set Audio Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sripathi Sridhar, Mark Cartwright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current audio classification models have small class vocabularies relative to
the large number of sound event classes of interest in the real world. Thus,
they provide a limited view of the world that may miss important yet unexpected
or unknown sound events. To address this issue, open-set audio classification
techniques have been developed to detect sound events from unknown classes.
Although these methods have been applied to a multi-class context in audio,
such as sound scene classification, they have yet to be investigated for
polyphonic audio in which sound events overlap, requiring the use of
multi-label models. In this study, we establish the problem of multi-label
open-set audio classification by creating a dataset with varying unknown class
distributions and evaluating baseline approaches built upon existing
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the Workshop on Detection and Classification of Acoustic
  Scenes and Events, 2023 (DCASE 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Transfer Learning Method Utilizing Acoustic and Vibration
  Signals for Rotating Machinery Fault Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongliang Chen, Zhuofei Huang, Wenxiong Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fault diagnosis of rotating machinery plays a important role for the safety
and stability of modern industrial systems. However, there is a distribution
discrepancy between training data and data of real-world operation scenarios,
which causing the decrease of performance of existing systems. This paper
proposed a transfer learning based method utilizing acoustic and vibration
signal to address this distribution discrepancy. We designed the acoustic and
vibration feature fusion MAVgram to offer richer and more reliable information
of faults, coordinating with a DNN-based classifier to obtain more effective
diagnosis representation. The backbone was pre-trained and then fine-tuned to
obtained excellent performance of the target task. Experimental results
demonstrate the effectiveness of the proposed method, and achieved improved
performance compared to STgram-MFN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-latency Speech Enhancement via Speech Token Generation <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaying Xue, Xiulian Peng, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing deep learning based speech enhancement mainly employ a data-driven
approach, which leverage large amounts of data with a variety of noise types to
achieve noise removal from noisy signal. However, the high dependence on the
data limits its generalization on the unseen complex noises in real-life
environment. In this paper, we focus on the low-latency scenario and regard
speech enhancement as a speech generation problem conditioned on the noisy
signal, where we generate clean speech instead of identifying and removing
noises. Specifically, we propose a conditional generative framework for speech
enhancement, which models clean speech by acoustic codes of a neural speech
codec and generates the speech codes conditioned on past noisy frames in an
auto-regressive way. Moreover, we propose an explicit-alignment approach to
align noisy frames with the generated speech tokens to improve the robustness
and scalability to different input lengths. Different from other methods that
leverage multiple stages to generate speech codes, we leverage a single-stage
speech generation approach based on the TF-Codec neural codec to achieve high
speech quality with low latency. Extensive results on both synthetic and
real-recorded test set show its superiority over data-driven approaches in
terms of noise robustness and temporal speech coherence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, submitted to ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effect of Spoken Language on Speech Enhancement using
  Self-Supervised Speech Representation Loss Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.14502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.14502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Close, Thomas Hain, Stefan Goetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in the field of speech enhancement (SE) has involved the use of
self-supervised speech representations (SSSRs) as feature transformations in
loss functions. However, in prior work, very little attention has been paid to
the relationship between the language of the audio used to train the
self-supervised representation and that used to train the SE system.
Enhancement models trained using a loss function which incorporates a
self-supervised representation that shares exactly the language of the noisy
data used to train the SE system show better performance than those which do
not match exactly. This may lead to enhancement systems which are language
specific and as such do not generalise well to unseen languages, unlike models
trained using traditional spectrogram or time domain loss functions. In this
work, SE models are trained and tested on a number of different languages, with
self-supervised representations which themselves are trained using different
language combinations and with differing network structures as loss function
representations. These models are then tested across unseen languages and their
performances are analysed. It is found that the training language of the
self-supervised representation appears to have a minor effect on enhancement
performance, the amount of training data of a particular language, however,
greatly affects performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WASPAA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spoken Question Answering and Speech Continuation Using
  Spectrogram-Powered LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15255v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15255v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, Michelle Tadmor Ramanovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to adapting pre-trained large language models
(LLMs) to perform question answering (QA) and speech continuation. By endowing
the LLM with a pre-trained speech encoder, our model becomes able to take
speech inputs and generate speech outputs. The entire system is trained
end-to-end and operates directly on spectrograms, simplifying our architecture.
Key to our approach is a training objective that jointly supervises speech
recognition, text continuation, and speech synthesis using only paired
speech-text pairs, enabling a `cross-modal' chain-of-thought within a single
decoding pass. Our method surpasses existing spoken language models in speaker
preservation and semantic coherence. Furthermore, the proposed model improves
upon direct initialization in retaining the knowledge of the original LLM as
demonstrated through spoken QA datasets. Audio samples can be found at
https://michelleramanovich.github.io/spectron/spectron
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reproducing the Acoustic Velocity Vectors in a Spherical Listening
  Region 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07200v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07200v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Wang, Thushara Abhayapala, Jihui Aimee Zhang, Prasanga Samarasinghe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic velocity vectors are related to the human's perception of sound at
low frequencies and have been widely used in Ambisonics. This paper proposes a
spatial sound field reproduction algorithm based on matching the acoustic
velocity vectors in the spherical listening region. Using the sound field
translation formula, the spherical harmonic coefficients of the acoustic
velocity vectors in the spherical listening region are derived from the
spherical harmonic coefficients of the pressure, which can be measured by a
higher-order microphone array. Unlike previous work in which the acoustic
velocity vectors are only controlled on the boundary of the listening region or
at discrete sweet spots, this work directly manipulates the acoustic velocity
vectors in the whole spherical listening region, which allows the listener to
move beyond the sweet spots. Simulations show the proposed reproduction
algorithm can accurately reproduce the acoustic velocity vectors in the
spherical listening region, with better performance at low frequencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Signal Processing Letters</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-19T00:00:00Z">2023-10-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification of Bandgaps in Acoustic Metamaterials with
  Stochastic Geometric Defects and Material Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Rayehe Karimi Mahabadi, Cynthia Rudin, Johann Guilleminot, L. Catherine Brinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the utility of techniques within uncertainty
quantification, namely spectral projection and polynomial chaos expansion, in
reducing sampling needs for characterizing acoustic metamaterial dispersion
band responses given stochastic material properties and geometric defects. A
novel method of encoding geometric defects in an interpretable, resolution
independent is showcased in the formation of input space probability
distributions. Orders of magnitude sampling reductions down to $\sim10^0$ and
$\sim10^1$ are achieved in the 1D and 7D input space scenarios respectively
while maintaining accurate output space probability distributions through
combining Monte Carlo, quadrature rule, and sparse grid sampling with surrogate
model fitting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio Editing with Non-Rigid Text Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Paissan, Zhepei Wang, Mirco Ravanelli, Paris Smaragdis, Cem Subakan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore audio-editing with non-rigid text edits. We show
that the proposed editing pipeline is able to create audio edits that remain
faithful to the input audio. We explore text prompts that perform addition,
style transfer, and in-painting. We quantitatively and qualitatively show that
the edits are able to obtain results which outperform Audio-LDM, a recently
released text-prompted audio generation model. Qualitative inspection of the
results points out that the edits given by our approach remain more faithful to
the input audio in terms of keeping the original onsets and offsets of the
audio events.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmoDiarize: Speaker Diarization and Emotion Identification from Speech
  Signals using Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanan Hamza, Fiza Gafoor, Fathima Sithara, Gayathri Anil, V. S. Anoop
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of advanced artificial intelligence and human-computer
interaction, identifying emotions in spoken language is paramount. This
research explores the integration of deep learning techniques in speech emotion
recognition, offering a comprehensive solution to the challenges associated
with speaker diarization and emotion identification. It introduces a framework
that combines a pre-existing speaker diarization pipeline and an emotion
identification model built on a Convolutional Neural Network (CNN) to achieve
higher precision. The proposed model was trained on data from five speech
emotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out
of which the latter is a speech emotion dataset created specifically for this
research. The features extracted from each sample include Mel Frequency
Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS),
and various data augmentation algorithms like pitch, noise, stretch, and shift.
This feature extraction approach aims to enhance prediction accuracy while
reducing computational complexity. The proposed model yields an unweighted
accuracy of 63%, demonstrating remarkable efficiency in accurately identifying
emotional states within speech signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Based Models For <span class="highlight-title">Speech Synthesis</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanli Sun, Zehai Tu, Anton Ragni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there has been a lot of interest in non-autoregressive (non-AR)
models for speech synthesis, such as FastSpeech 2 and diffusion models. Unlike
AR models, these models do not have autoregressive dependencies among outputs
which makes inference efficient. This paper expands the range of available
non-AR models with another member called energy-based models (EBMs). The paper
describes how noise contrastive estimation, which relies on the comparison
between positive and negative samples, can be used to train EBMs. It proposes a
number of strategies for generating effective negative samples, including using
high-performing AR models. It also describes how sampling from EBMs can be
performed using Langevin Markov Chain Monte-Carlo (MCMC). The use of Langevin
MCMC enables to draw connections between EBMs and currently popular diffusion
models. Experiments on LJSpeech dataset show that the proposed approach offers
improvements over Tacotron 2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Zhang, Akira Maezawa, Gus Xia, Kazuhiko Yamamoto, Simon Dixon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating music is iterative, requiring varied methods at each stage. However,
existing AI music systems fall short in orchestrating multiple subsystems for
diverse needs. To address this gap, we introduce Loop Copilot, a novel system
that enables users to generate and iteratively refine music through an
interactive, multi-round dialogue interface. The system uses a large language
model to interpret user intentions and select appropriate AI models for task
execution. Each backend model is specialized for a specific task, and their
outputs are aggregated to meet the user's requirements. To ensure musical
coherence, essential attributes are maintained in a centralized table. We
evaluate the effectiveness of the proposed system through semi-structured
interviews and questionnaires, highlighting its utility not only in
facilitating music creation but also its potential for broader applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code and demo video are available at
  \url{https://sites.google.com/view/loop-copilot}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting
  Multiple Experts for Video Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammarah Hashmi, Sahibzada Adil Shahzad, Chia-Wen Lin, Yu Tsao, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forged content shared widely on social media platforms is a major social
problem that requires increased regulation and poses new challenges to the
research community. The recent proliferation of hyper-realistic deepfake videos
has drawn attention to the threat of audio and visual forgeries. Most previous
work on detecting AI-generated fake videos only utilizes visual modality or
audio modality. While there are some methods in the literature that exploit
audio and visual modalities to detect forged videos, they have not been
comprehensively evaluated on multi-modal datasets of deepfake videos involving
acoustic and visual manipulations. Moreover, these existing methods are mostly
based on CNN and suffer from low detection accuracy. Inspired by the recent
success of Transformer in various fields, to address the challenges posed by
deepfake technology, in this paper, we propose an Audio-Visual
Transformer-based Ensemble Network (AVTENet) framework that considers both
acoustic manipulation and visual manipulation to achieve effective video
forgery detection. Specifically, the proposed model integrates several purely
transformer-based variants that capture video, audio, and audio-visual salient
cues to reach a consensus in prediction. For evaluation, we use the recently
released benchmark multi-modal audio-video FakeAVCeleb dataset. For a detailed
analysis, we evaluate AVTENet, its variants, and several existing methods on
multiple test sets of the FakeAVCeleb dataset. Experimental results show that
our best model outperforms all existing methods and achieves state-of-the-art
performance on Testset-I and Testset-II of the FakeAVCeleb dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Powerset multi-class cross entropy loss for neural speaker diarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis Plaquet, Hervé Bredin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since its introduction in 2019, the whole end-to-end neural diarization
(EEND) line of work has been addressing speaker diarization as a frame-wise
multi-label classification problem with permutation-invariant training. Despite
EEND showing great promise, a few recent works took a step back and studied the
possible combination of (local) supervised EEND diarization with (global)
unsupervised clustering. Yet, these hybrid contributions did not question the
original multi-label formulation. We propose to switch from multi-label (where
any two speakers can be active at the same time) to powerset multi-class
classification (where dedicated classes are assigned to pairs of overlapping
speakers). Through extensive experiments on 9 different benchmarks, we show
that this formulation leads to significantly better performance (mostly on
overlapping speech) and robustness to domain mismatch, while eliminating the
detection threshold hyperparameter, critical for the multi-label formulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-informed Neural Network for Acoustic Resonance Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuya Yokota, Takahiko Kurahashi, Masajiro Abe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes the physics-informed neural network (PINN) framework to
solve the wave equation for acoustic resonance analysis. ResoNet, the
analytical model proposed in this study, minimizes the loss function for
periodic solutions, in addition to conventional PINN loss functions, thereby
effectively using the function approximation capability of neural networks,
while performing resonance analysis. Additionally, it can be easily applied to
inverse problems. Herein, the resonance in a one-dimensional acoustic tube was
analyzed. The effectiveness of the proposed method was validated through the
forward and inverse analyses of the wave equation with energy-loss terms. In
the forward analysis, the applicability of PINN to the resonance problem was
evaluated by comparison with the finite-difference method. The inverse
analysis, which included the identification of the energy loss term in the wave
equation and design optimization of the acoustic tube, was performed with good
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 14 figures. The following article has been submitted to the
  Journal of the Acoustical Society of America. After it is published, it will
  be found at https://pubs.aip.org/asa/jasa . v2: Corrected a typo in Eq. (22)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, <span class="highlight-author">Yossi Adi</span>, Jay Mahadeokar, <span class="highlight-author">Wei-Ning Hsu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale generative models such as GPT and DALL-E have revolutionized the
research community. These models not only generate high fidelity outputs, but
are also generalists which can solve tasks not explicitly taught. In contrast,
speech generative models are still primitive in terms of scale and task
generalization. In this paper, we present Voicebox, the most versatile
text-guided generative model for speech at scale. Voicebox is a
non-autoregressive flow-matching model trained to infill speech, given audio
context and text, trained on over 50K hours of speech that are not filtered or
enhanced. Similar to GPT, Voicebox can perform many different tasks through
in-context learning, but is more flexible as it can also condition on future
context. Voicebox can be used for mono or cross-lingual zero-shot
text-to-speech synthesis, noise removal, content editing, style conversion, and
diverse sample generation. In particular, Voicebox outperforms the
state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs
1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to
20 times faster. Audio samples can be found in
\url{https://voicebox.metademolab.com}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning and Bandits for Speech and Language Processing:
  Tutorial, <span class="highlight-title">Review</span> and Outlook <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baihan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, reinforcement learning and bandits have transformed a wide
range of real-world applications including healthcare, finance, recommendation
systems, robotics, and last but not least, the speech and natural language
processing. While most speech and language applications of reinforcement
learning algorithms are centered around improving the training of deep neural
networks with its flexible optimization properties, there are still many
grounds to explore to utilize the benefits of reinforcement learning, such as
its reward-driven adaptability, state representations, temporal structures and
generalizability. In this survey, we present an overview of recent advancements
of reinforcement learning and bandits, and discuss how they can be effectively
employed to solve speech and natural language processing problems with models
that are adaptive, interactive and scalable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Expert Systems with Applications. Accompanying
  INTERSPEECH 2022 Tutorial on the same topic. Including latest advancements in
  large language models (LLMs)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for
  <span class="highlight-title">Text-to-Speech</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02147v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02147v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Jiang, Zhe Su, <span class="highlight-author">Zhou Zhao</span>, Qian Yang, <span class="highlight-author">Yi Ren</span>, Jinglin Liu, Zhenhui Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polyphone disambiguation aims to capture accurate pronunciation knowledge
from natural text sequences for reliable Text-to-speech (TTS) systems. However,
previous approaches require substantial annotated training data and additional
efforts from language experts, making it difficult to extend high-quality
neural TTS systems to out-of-domain daily conversations and countless languages
worldwide. This paper tackles the polyphone disambiguation problem from a
concise and novel perspective: we propose Dict-TTS, a semantic-aware generative
text-to-speech model with an online website dictionary (the existing prior
information in the natural language). Specifically, we design a
semantics-to-pronunciation attention (S2PA) module to match the semantic
patterns between the input text sequence and the prior semantics in the
dictionary and obtain the corresponding pronunciations; The S2PA module can be
easily trained with the end-to-end TTS model without any annotated phoneme
labels. Experimental results in three languages show that our model outperforms
several strong baseline models in terms of pronunciation accuracy and improves
the prosody modeling of TTS systems. Further extensive analyses demonstrate
that each design in Dict-TTS is effective. The code is available at
\url{https://github.com/Zain-Jiang/Dict-TTS}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3: fix the introduction for the concurrent similar work of Neural
  Lexicon Reader (arXiv:2110.09698)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Connecting Multi-modal Contrastive Representations <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Wang, Yang Zhao, Xize Cheng, Haifeng Huang, Jiageng Liu, Li Tang, Linjun Li, Yongqi Wang, Aoxiong Yin, Ziang Zhang, <span class="highlight-author">Zhou Zhao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Contrastive Representation learning aims to encode different
modalities into a semantically aligned shared space. This paradigm shows
remarkable generalization ability on numerous downstream tasks across various
modalities. However, the reliance on massive high-quality data pairs limits its
further development on more modalities. This paper proposes a novel
training-efficient method for learning MCR without paired data called
Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given
two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project
them to a new space and use the data from the overlapping modality B to
aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A,
B) and (B, C) are already aligned within each MCR, the connection learned by
overlapping modality can also be transferred to non-overlapping modality pair
(A, C). To unleash the potential of C-MCR, we further introduce a
semantic-enhanced inter- and intra-MCR connection method. We first enhance the
semantic consistency and completion of embeddings across different modalities
for more robust alignment. Then we utilize the inter-MCR alignment to establish
the connection, and employ the intra-MCR alignment to better maintain the
connection for inputs from non-overlapping modalities. To demonstrate the
effectiveness of C-MCR, we connect CLIP and CLAP via texts to derive
audio-visual representations, and integrate CLIP and ULIP via images for
3D-language representations. Remarkably, without using any paired data, C-MCR
for audio-visual achieves state-of-the-art performance on audio-image
retrieval, audio-visual source localization, and counterfactual audio-image
recognition tasks. Furthermore, C-MCR for 3D-language also attains advanced
zero-shot 3D point cloud classification accuracy on ModelNet40.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio Contrastive based Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11895v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11895v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Wang, Qibin Liang, Chenghao Xiao, Yizhi Li, Noura Al Moubayed, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio classification plays a crucial role in speech and sound processing
tasks with a wide range of applications. There still remains a challenge of
striking the right balance between fitting the model to the training data
(avoiding overfitting) and enabling it to generalise well to a new domain.
Leveraging the transferability of contrastive learning, we introduce Audio
Contrastive-based Fine-tuning (AudioConFit), an efficient approach
characterised by robust generalisability. Empirical experiments on a variety of
audio classification tasks demonstrate the effectiveness and robustness of our
approach, which achieves state-of-the-art results in various settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LAMASSU: Streaming Language-Agnostic Multilingual Speech Recognition and
  Translation Using Neural Transducers <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02809v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02809v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peidong Wang, Eric Sun, Jian Xue, Yu Wu, Long Zhou, Yashesh Gaur, Shujie Liu, Jinyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) and speech translation (ST) can both use
neural transducers as the model structure. It is thus possible to use a single
transducer model to perform both tasks. In real-world applications, such joint
ASR and ST models may need to be streaming and do not require source language
identification (i.e. language-agnostic). In this paper, we propose LAMASSU, a
streaming language-agnostic multilingual speech recognition and translation
model using neural transducers. Based on the transducer model structure, we
propose four methods, a unified joint and prediction network for multilingual
output, a clustered multilingual encoder, target language identification for
encoder, and connectionist temporal classification regularization. Experimental
results show that LAMASSU not only drastically reduces the model size but also
reaches the performances of monolingual ASR and bilingual ST models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification of Bandgaps in Acoustic Metamaterials with
  Stochastic Geometric Defects and Material Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Rayehe Karimi Mahabadi, Cynthia Rudin, Johann Guilleminot, L. Catherine Brinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the utility of techniques within uncertainty
quantification, namely spectral projection and polynomial chaos expansion, in
reducing sampling needs for characterizing acoustic metamaterial dispersion
band responses given stochastic material properties and geometric defects. A
novel method of encoding geometric defects in an interpretable, resolution
independent is showcased in the formation of input space probability
distributions. Orders of magnitude sampling reductions down to $\sim10^0$ and
$\sim10^1$ are achieved in the 1D and 7D input space scenarios respectively
while maintaining accurate output space probability distributions through
combining Monte Carlo, quadrature rule, and sparse grid sampling with surrogate
model fitting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio Editing with Non-Rigid Text Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Paissan, Zhepei Wang, Mirco Ravanelli, Paris Smaragdis, Cem Subakan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore audio-editing with non-rigid text edits. We show
that the proposed editing pipeline is able to create audio edits that remain
faithful to the input audio. We explore text prompts that perform addition,
style transfer, and in-painting. We quantitatively and qualitatively show that
the edits are able to obtain results which outperform Audio-LDM, a recently
released text-prompted audio generation model. Qualitative inspection of the
results points out that the edits given by our approach remain more faithful to
the input audio in terms of keeping the original onsets and offsets of the
audio events.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmoDiarize: Speaker Diarization and Emotion Identification from Speech
  Signals using Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanan Hamza, Fiza Gafoor, Fathima Sithara, Gayathri Anil, V. S. Anoop
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of advanced artificial intelligence and human-computer
interaction, identifying emotions in spoken language is paramount. This
research explores the integration of deep learning techniques in speech emotion
recognition, offering a comprehensive solution to the challenges associated
with speaker diarization and emotion identification. It introduces a framework
that combines a pre-existing speaker diarization pipeline and an emotion
identification model built on a Convolutional Neural Network (CNN) to achieve
higher precision. The proposed model was trained on data from five speech
emotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out
of which the latter is a speech emotion dataset created specifically for this
research. The features extracted from each sample include Mel Frequency
Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS),
and various data augmentation algorithms like pitch, noise, stretch, and shift.
This feature extraction approach aims to enhance prediction accuracy while
reducing computational complexity. The proposed model yields an unweighted
accuracy of 63%, demonstrating remarkable efficiency in accurately identifying
emotional states within speech signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Beamforming for Speech Enhancement and Speaker Localization with an
  Array Response-Aware Loss Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsinyu Chang, Yicheng Hsu, Mingsian R. Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research advances in deep neural network (DNN)-based beamformers have
shown great promise for speech enhancement under adverse acoustic conditions.
Different network architectures and input features have been explored in
estimating beamforming weights. In this paper, we propose a deep beamformer
based on an efficient convolutional recurrent network (CNN) trained with a
novel ARray RespOnse-aWare (ARROW) loss function. The ARROW loss exploits the
array responses of the target and interferer by using the ground truth relative
transfer functions (RTFs). The DNN-based beamforming system, trained with ARROW
loss through supervised learning, is able to perform speech enhancement and
speaker localization jointly. Experimental results have shown that the proposed
deep beamformer, trained with the linearly weighted scale-invariant
source-to-noise ratio (SI-SNR) and ARROW loss functions, achieves superior
performance in speech enhancement and speaker localization compared to two
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Based Models For <span class="highlight-title">Speech Synthesis</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanli Sun, Zehai Tu, Anton Ragni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there has been a lot of interest in non-autoregressive (non-AR)
models for speech synthesis, such as FastSpeech 2 and diffusion models. Unlike
AR models, these models do not have autoregressive dependencies among outputs
which makes inference efficient. This paper expands the range of available
non-AR models with another member called energy-based models (EBMs). The paper
describes how noise contrastive estimation, which relies on the comparison
between positive and negative samples, can be used to train EBMs. It proposes a
number of strategies for generating effective negative samples, including using
high-performing AR models. It also describes how sampling from EBMs can be
performed using Langevin Markov Chain Monte-Carlo (MCMC). The use of Langevin
MCMC enables to draw connections between EBMs and currently popular diffusion
models. Experiments on LJSpeech dataset show that the proposed approach offers
improvements over Tacotron 2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Feature Importance and Interpretability of Speaker Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederik Rautenberg, Michael Kuhlmann, Jana Wiechmann, Fritz Seebauer, Petra Wagner, Reinhold Haeb-Umbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised speech disentanglement aims at separating fast varying from
slowly varying components of a speech signal. In this contribution, we take a
closer look at the embedding vector representing the slowly varying signal
components, commonly named the speaker embedding vector. We ask, which
properties of a speaker's voice are captured and investigate to which extent do
individual embedding vector components sign responsible for them, using the
concept of Shapley values. Our findings show that certain speaker-specific
acoustic-phonetic properties can be fairly well predicted from the speaker
embedding, while the investigated more abstract voice quality features cannot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the ITG conference on Speech Communication 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> An Exploration of In-Context Learning for Speech Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming-Hao Hsu, Kai-Wei Chang, Shang-Wen Li, <span class="highlight-author">Hung-yi Lee</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ever since the development of GPT-3 in the natural language processing (NLP)
field, in-context learning (ICL) has played an important role in utilizing
large language models (LLMs). By presenting the LM utterance-label
demonstrations at the input, the LM can accomplish few-shot learning without
relying on gradient descent or requiring explicit modification of its
parameters. This enables the LM to learn and adapt in a black-box manner.
Despite the success of ICL in NLP, little work is exploring the possibility of
ICL in speech processing. This study proposes the first exploration of ICL with
a speech LM without text supervision. We first show that the current speech LM
does not have the ICL capability. With the proposed warmup training, the speech
LM can, therefore, perform ICL on unseen tasks. In this work, we verify the
feasibility of ICL for speech LM on speech classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Zhang, Akira Maezawa, Gus Xia, Kazuhiko Yamamoto, Simon Dixon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating music is iterative, requiring varied methods at each stage. However,
existing AI music systems fall short in orchestrating multiple subsystems for
diverse needs. To address this gap, we introduce Loop Copilot, a novel system
that enables users to generate and iteratively refine music through an
interactive, multi-round dialogue interface. The system uses a large language
model to interpret user intentions and select appropriate AI models for task
execution. Each backend model is specialized for a specific task, and their
outputs are aggregated to meet the user's requirements. To ensure musical
coherence, essential attributes are maintained in a centralized table. We
evaluate the effectiveness of the proposed system through semi-structured
interviews and questionnaires, highlighting its utility not only in
facilitating music creation but also its potential for broader applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code and demo video are available at
  \url{https://sites.google.com/view/loop-copilot}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Time Series Similarity Measure and Its Smart Grid Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yuan, S. Ali Pourmousavi, Wen L. Soong, Andrew J. Black, Jon A. R. Liisberg, Julian Lemos-Vinasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many smart grid applications involve data mining, clustering, classification,
identification, and anomaly detection, among others. These applications
primarily depend on the measurement of similarity, which is the distance
between different time series or subsequences of a time series. The commonly
used time series distance measures, namely Euclidean Distance (ED) and Dynamic
Time Warping (DTW), do not quantify the flexible nature of electricity usage
data in terms of temporal dynamics. As a result, there is a need for a new
distance measure that can quantify both the amplitude and temporal changes of
electricity time series for smart grid applications, e.g., demand response and
load profiling. This paper introduces a novel distance measure to compare
electricity usage patterns. The method consists of two phases that quantify the
effort required to reshape one time series into another, considering both
amplitude and temporal changes. The proposed method is evaluated against ED and
DTW using real-world data in three smart grid applications. Overall, the
proposed measure outperforms ED and DTW in accurately identifying the best load
scheduling strategy, anomalous days with irregular electricity usage, and
determining electricity users' behind-the-meter (BTM) equipment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting
  Multiple Experts for Video Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammarah Hashmi, Sahibzada Adil Shahzad, Chia-Wen Lin, Yu Tsao, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forged content shared widely on social media platforms is a major social
problem that requires increased regulation and poses new challenges to the
research community. The recent proliferation of hyper-realistic deepfake videos
has drawn attention to the threat of audio and visual forgeries. Most previous
work on detecting AI-generated fake videos only utilizes visual modality or
audio modality. While there are some methods in the literature that exploit
audio and visual modalities to detect forged videos, they have not been
comprehensively evaluated on multi-modal datasets of deepfake videos involving
acoustic and visual manipulations. Moreover, these existing methods are mostly
based on CNN and suffer from low detection accuracy. Inspired by the recent
success of Transformer in various fields, to address the challenges posed by
deepfake technology, in this paper, we propose an Audio-Visual
Transformer-based Ensemble Network (AVTENet) framework that considers both
acoustic manipulation and visual manipulation to achieve effective video
forgery detection. Specifically, the proposed model integrates several purely
transformer-based variants that capture video, audio, and audio-visual salient
cues to reach a consensus in prediction. For evaluation, we use the recently
released benchmark multi-modal audio-video FakeAVCeleb dataset. For a detailed
analysis, we evaluate AVTENet, its variants, and several existing methods on
multiple test sets of the FakeAVCeleb dataset. Experimental results show that
our best model outperforms all existing methods and achieves state-of-the-art
performance on Testset-I and Testset-II of the FakeAVCeleb dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Powerset multi-class cross entropy loss for neural speaker diarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis Plaquet, Hervé Bredin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since its introduction in 2019, the whole end-to-end neural diarization
(EEND) line of work has been addressing speaker diarization as a frame-wise
multi-label classification problem with permutation-invariant training. Despite
EEND showing great promise, a few recent works took a step back and studied the
possible combination of (local) supervised EEND diarization with (global)
unsupervised clustering. Yet, these hybrid contributions did not question the
original multi-label formulation. We propose to switch from multi-label (where
any two speakers can be active at the same time) to powerset multi-class
classification (where dedicated classes are assigned to pairs of overlapping
speakers). Through extensive experiments on 9 different benchmarks, we show
that this formulation leads to significantly better performance (mostly on
overlapping speech) and robustness to domain mismatch, while eliminating the
detection threshold hyperparameter, critical for the multi-label formulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-informed Neural Network for Acoustic Resonance Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuya Yokota, Takahiko Kurahashi, Masajiro Abe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes the physics-informed neural network (PINN) framework to
solve the wave equation for acoustic resonance analysis. ResoNet, the
analytical model proposed in this study, minimizes the loss function for
periodic solutions, in addition to conventional PINN loss functions, thereby
effectively using the function approximation capability of neural networks,
while performing resonance analysis. Additionally, it can be easily applied to
inverse problems. Herein, the resonance in a one-dimensional acoustic tube was
analyzed. The effectiveness of the proposed method was validated through the
forward and inverse analyses of the wave equation with energy-loss terms. In
the forward analysis, the applicability of PINN to the resonance problem was
evaluated by comparison with the finite-difference method. The inverse
analysis, which included the identification of the energy loss term in the wave
equation and design optimization of the acoustic tube, was performed with good
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 14 figures. The following article has been submitted to the
  Journal of the Acoustical Society of America. After it is published, it will
  be found at https://pubs.aip.org/asa/jasa . v2: Corrected a typo in Eq. (22)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, <span class="highlight-author">Yossi Adi</span>, Jay Mahadeokar, <span class="highlight-author">Wei-Ning Hsu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale generative models such as GPT and DALL-E have revolutionized the
research community. These models not only generate high fidelity outputs, but
are also generalists which can solve tasks not explicitly taught. In contrast,
speech generative models are still primitive in terms of scale and task
generalization. In this paper, we present Voicebox, the most versatile
text-guided generative model for speech at scale. Voicebox is a
non-autoregressive flow-matching model trained to infill speech, given audio
context and text, trained on over 50K hours of speech that are not filtered or
enhanced. Similar to GPT, Voicebox can perform many different tasks through
in-context learning, but is more flexible as it can also condition on future
context. Voicebox can be used for mono or cross-lingual zero-shot
text-to-speech synthesis, noise removal, content editing, style conversion, and
diverse sample generation. In particular, Voicebox outperforms the
state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs
1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to
20 times faster. Audio samples can be found in
\url{https://voicebox.metademolab.com}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning and Bandits for Speech and Language Processing:
  Tutorial, <span class="highlight-title">Review</span> and Outlook <span class="chip">INTERSPEECH 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baihan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, reinforcement learning and bandits have transformed a wide
range of real-world applications including healthcare, finance, recommendation
systems, robotics, and last but not least, the speech and natural language
processing. While most speech and language applications of reinforcement
learning algorithms are centered around improving the training of deep neural
networks with its flexible optimization properties, there are still many
grounds to explore to utilize the benefits of reinforcement learning, such as
its reward-driven adaptability, state representations, temporal structures and
generalizability. In this survey, we present an overview of recent advancements
of reinforcement learning and bandits, and discuss how they can be effectively
employed to solve speech and natural language processing problems with models
that are adaptive, interactive and scalable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Expert Systems with Applications. Accompanying
  INTERSPEECH 2022 Tutorial on the same topic. Including latest advancements in
  large language models (LLMs)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for
  <span class="highlight-title">Text-to-Speech</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02147v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02147v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Jiang, Zhe Su, <span class="highlight-author">Zhou Zhao</span>, Qian Yang, <span class="highlight-author">Yi Ren</span>, Jinglin Liu, Zhenhui Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polyphone disambiguation aims to capture accurate pronunciation knowledge
from natural text sequences for reliable Text-to-speech (TTS) systems. However,
previous approaches require substantial annotated training data and additional
efforts from language experts, making it difficult to extend high-quality
neural TTS systems to out-of-domain daily conversations and countless languages
worldwide. This paper tackles the polyphone disambiguation problem from a
concise and novel perspective: we propose Dict-TTS, a semantic-aware generative
text-to-speech model with an online website dictionary (the existing prior
information in the natural language). Specifically, we design a
semantics-to-pronunciation attention (S2PA) module to match the semantic
patterns between the input text sequence and the prior semantics in the
dictionary and obtain the corresponding pronunciations; The S2PA module can be
easily trained with the end-to-end TTS model without any annotated phoneme
labels. Experimental results in three languages show that our model outperforms
several strong baseline models in terms of pronunciation accuracy and improves
the prosody modeling of TTS systems. Further extensive analyses demonstrate
that each design in Dict-TTS is effective. The code is available at
\url{https://github.com/Zain-Jiang/Dict-TTS}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3: fix the introduction for the concurrent similar work of Neural
  Lexicon Reader (arXiv:2110.09698)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Connecting Multi-modal Contrastive Representations <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Wang, Yang Zhao, Xize Cheng, Haifeng Huang, Jiageng Liu, Li Tang, Linjun Li, Yongqi Wang, Aoxiong Yin, Ziang Zhang, <span class="highlight-author">Zhou Zhao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Contrastive Representation learning aims to encode different
modalities into a semantically aligned shared space. This paradigm shows
remarkable generalization ability on numerous downstream tasks across various
modalities. However, the reliance on massive high-quality data pairs limits its
further development on more modalities. This paper proposes a novel
training-efficient method for learning MCR without paired data called
Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given
two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project
them to a new space and use the data from the overlapping modality B to
aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A,
B) and (B, C) are already aligned within each MCR, the connection learned by
overlapping modality can also be transferred to non-overlapping modality pair
(A, C). To unleash the potential of C-MCR, we further introduce a
semantic-enhanced inter- and intra-MCR connection method. We first enhance the
semantic consistency and completion of embeddings across different modalities
for more robust alignment. Then we utilize the inter-MCR alignment to establish
the connection, and employ the intra-MCR alignment to better maintain the
connection for inputs from non-overlapping modalities. To demonstrate the
effectiveness of C-MCR, we connect CLIP and CLAP via texts to derive
audio-visual representations, and integrate CLIP and ULIP via images for
3D-language representations. Remarkably, without using any paired data, C-MCR
for audio-visual achieves state-of-the-art performance on audio-image
retrieval, audio-visual source localization, and counterfactual audio-image
recognition tasks. Furthermore, C-MCR for 3D-language also attains advanced
zero-shot 3D point cloud classification accuracy on ModelNet40.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio Contrastive based Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11895v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11895v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Wang, Qibin Liang, Chenghao Xiao, Yizhi Li, Noura Al Moubayed, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio classification plays a crucial role in speech and sound processing
tasks with a wide range of applications. There still remains a challenge of
striking the right balance between fitting the model to the training data
(avoiding overfitting) and enabling it to generalise well to a new domain.
Leveraging the transferability of contrastive learning, we introduce Audio
Contrastive-based Fine-tuning (AudioConFit), an efficient approach
characterised by robust generalisability. Empirical experiments on a variety of
audio classification tasks demonstrate the effectiveness and robustness of our
approach, which achieves state-of-the-art results in various settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LAMASSU: Streaming Language-Agnostic Multilingual Speech Recognition and
  Translation Using Neural Transducers <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02809v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02809v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peidong Wang, Eric Sun, Jian Xue, Yu Wu, Long Zhou, Yashesh Gaur, Shujie Liu, Jinyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) and speech translation (ST) can both use
neural transducers as the model structure. It is thus possible to use a single
transducer model to perform both tasks. In real-world applications, such joint
ASR and ST models may need to be streaming and do not require source language
identification (i.e. language-agnostic). In this paper, we propose LAMASSU, a
streaming language-agnostic multilingual speech recognition and translation
model using neural transducers. Based on the transducer model structure, we
propose four methods, a unified joint and prediction network for multilingual
output, a clustered multilingual encoder, target language identification for
encoder, and connectionist temporal classification regularization. Experimental
results show that LAMASSU not only drastically reduces the model size but also
reaches the performances of monolingual ASR and bilingual ST models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-18T00:00:00Z">2023-10-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Take the aTrain. Introducing an Interface for the Accessible
  Transcription of Interviews 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Haberl, Jürgen Fleiß, Dominik Kowald, Stefan Thalmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  aTrain is an open-source and offline tool for transcribing audio data in
multiple languages with CPU and NVIDIA GPU support. It is specifically designed
for researchers using qualitative data generated from various forms of speech
interactions with research participants. aTrain requires no programming skills,
runs on most computers, does not require an internet connection, and was
verified not to upload data to any server. aTrain combines OpenAI's Whisper
model with speaker recognition to provide output that integrates with the
popular qualitative data analysis software tools MAXQDA and ATLAS.ti. It has an
easy-to-use graphical interface and is provided as a Windows-App through the
Microsoft Store allowing for simple installation by researchers. The source
code is freely available on GitHub. Having developed aTrain with a focus on
speed on local computers, we show that the transcription time on current mobile
CPUs is around 2 to 3 times the duration of the audio file using the
highest-accuracy transcription models. If an entry-level graphics card is
available, the transcription speed increases to 20% of the audio duration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Install via Microsoft store:
  apps.microsoft.com/store/detail/atrain/9N15Q44SZNS2. Github:
  github.com/BANDAS-Center/aTrain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BUT CHiME-7 system description 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Karafiát, Karel Veselý, Igor Szöke, Ladislav Mošner, Karel Beneš, Marcin Witkowski, Germán Barchi, Leonardo Pepino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the joint effort of Brno University of Technology (BUT),
AGH University of Krakow and University of Buenos Aires on the development of
Automatic Speech Recognition systems for the CHiME-7 Challenge. We train and
evaluate various end-to-end models with several toolkits. We heavily relied on
Guided Source Separation (GSS) to convert multi-channel audio to single
channel. The ASR is leveraging speech representations from models pre-trained
by self-supervised learning, and we do a fusion of several ASR systems. In
addition, we modified external data from the LibriSpeech corpus to become a
close domain and added it to the training. Our efforts were focused on the
far-field acoustic robustness sub-track of Task 1 - Distant Automatic Speech
Recognition (DASR), our systems use oracle segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, Chime-7 challenge 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLARA: Multilingual Contrastive Learning for Audio Representation
  Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kari A Noriy, Xiaosong Yang, Marcin Budka, Jian Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework for multilingual speech and sound
representation learning using contrastive learning. The lack of sizeable
labelled datasets hinders speech-processing research across languages. Recent
advances in contrastive learning provide self-supervised techniques to learn
from unlabelled data. Motivated by reducing data dependence and improving
generalisation across diverse languages and conditions, we develop a
multilingual contrastive framework. This framework enables models to acquire
shared representations across languages, facilitating cross-lingual transfer
with limited target language data.
  Additionally, capturing emotional cues within speech is challenging due to
subjective perceptual assessments. By learning expressive representations from
diverse, multilingual data in a self-supervised manner, our approach aims to
develop speech representations that encode emotive dimensions.
  Our method trains encoders on a large corpus of multi-lingual audio data.
Data augmentation techniques are employed to expand the dataset. The
contrastive learning approach trains the model to maximise agreement between
positive pairs and minimise agreement between negative pairs. Extensive
experiments demonstrate state-of-the-art performance of the proposed model on
emotion recognition, audio classification, and retrieval benchmarks under
zero-shot and few-shot conditions. This provides an effective approach for
acquiring shared and generalised speech representations across languages and
acoustic conditions while encoding latent emotional dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blind estimation of audio effects using an auto-encoder approach and
  differentiable signal processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Côme Peladeau, Geoffroy Peeters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind Estimation of Audio Effects (BE-AFX) aims at estimating the Audio
Effects (AFXs) applied to an original, unprocessed audio sample solely based on
the processed audio sample. To train such a system traditional approaches
optimize a loss between ground truth and estimated AFX parameters. This
involves knowing the exact implementation of the AFXs used for the process. In
this work, we propose an alternative solution that eliminates the requirement
for knowing this implementation. Instead, we introduce an auto-encoder
approach, which optimizes an audio quality metric. We explore, suggest, and
compare various implementations of commonly used mastering AFXs, using
differential signal processing or neural approximations. Our findings
demonstrate that our auto-encoder approach yields superior estimates of the
audio quality produced by a chain of AFXs, compared to the traditional
parameter-based approach, even if the latter provides a more accurate parameter
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unintended Memorization in Large ASR Models, and How to Mitigate It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lun Wang, Om Thakkar, Rajiv Mathews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well-known that neural networks can unintentionally memorize their
training examples, causing privacy concerns. However, auditing memorization in
large non-auto-regressive automatic speech recognition (ASR) models has been
challenging due to the high compute cost of existing methods such as hardness
calibration. In this work, we design a simple auditing method to measure
memorization in large ASR models without the extra compute overhead.
Concretely, we speed up randomly-generated utterances to create a mapping
between vocal and text information that is difficult to learn from typical
training examples. Hence, accurate predictions only for sped-up training
examples can serve as clear evidence for memorization, and the corresponding
accuracy can be used to measure memorization. Using the proposed method, we
showcase memorization in the state-of-the-art ASR models. To mitigate
memorization, we tried gradient clipping during training to bound the influence
of any individual example on the final model. We empirically show that clipping
each example's gradient can mitigate memorization for sped-up training examples
with up to 16 repetitions in the training set. Furthermore, we show that in
large-scale distributed training, clipping the average gradient on each compute
core maintains neutral model quality and compute cost while providing strong
privacy protection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EchoScan: Scanning Complex Indoor Geometries via Acoustic Echoes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inmo Yeon, Iljoo Jeong, Seungchul Lee, Jung-Woo Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate estimation of indoor space geometries is vital for constructing
precise digital twins, whose broad industrial applications include navigation
in unfamiliar environments and efficient evacuation planning, particularly in
low-light conditions. This study introduces EchoScan, a deep neural network
model that utilizes acoustic echoes to perform room geometry inference.
Conventional sound-based techniques rely on estimating geometry-related room
parameters such as wall position and room size, thereby limiting the diversity
of inferable room geometries. Contrarily, EchoScan overcomes this limitation by
directly inferring room floorplans and heights, thereby enabling it to handle
rooms with arbitrary shapes, including curved walls. The key innovation of
EchoScan is its ability to analyze the complex relationship between low- and
high-order reflections in room impulse responses (RIRs) using a
multi-aggregation module. The analysis of high-order reflections also enables
it to infer complex room shapes when echoes are unobservable from the position
of an audio device. Herein, EchoScan was trained and evaluated using RIRs
synthesized from complex environments, including the Manhattan and Atlanta
layouts, employing a practical audio device configuration compatible with
commercial, off-the-shelf devices. Compared with vision-based methods, EchoScan
demonstrated outstanding geometry estimation performance in rooms with various
shapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Separating Invisible Sounds Toward Universal Audiovisual Scene-Aware
  Sound Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Su, Ali Vosoughi, Shijian Deng, Yapeng Tian, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The audio-visual sound separation field assumes visible sources in videos,
but this excludes invisible sounds beyond the camera's view. Current methods
struggle with such sounds lacking visible cues. This paper introduces a novel
"Audio-Visual Scene-Aware Separation" (AVSA-Sep) framework. It includes a
semantic parser for visible and invisible sounds and a separator for
scene-informed separation. AVSA-Sep successfully separates both sound types,
with joint training and cross-modal alignment enhancing effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2023 - AV4D, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experimental Results of Underwater Sound Speed Profile Inversion by
  Few-shot Multi-task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Huang, Fan Gao, Junting Wang, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater Sound Speed Profile (SSP) distribution has great influence on the
propagation mode of acoustic signal, thus the fast and accurate estimation of
SSP is of great importance in building underwater observation systems. The
state-of-the-art SSP inversion methods include frameworks of matched field
processing (MFP), compressive sensing (CS), and feedforeward neural networks
(FNN), among which the FNN shows better real-time performance while maintain
the same level of accuracy. However, the training of FNN needs quite a lot
historical SSP samples, which is diffcult to be satisfied in many ocean areas.
This situation is called few-shot learning. To tackle this issue, we propose a
multi-task learning (MTL) model with partial parameter sharing among different
traning tasks. By MTL, common features could be extracted, thus accelerating
the learning process on given tasks, and reducing the demand for reference
samples, so as to enhance the generalization ability in few-shot learning. To
verify the feasibility and effectiveness of MTL, a deep-ocean experiment was
held in April 2023 at the South China Sea. Results shows that MTL outperforms
the state-of-the-art methods in terms of accuracy for SSP inversion, while
inherits the real-time advantage of FNN during the inversion stage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The CHiME-7 Challenge: System Description and Performance of NeMo Team's
  DASR System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tae Jin Park, He Huang, Ante Jukic, Kunal Dhawan, Krishna C. Puvvada, Nithin Koluguri, Nikolay Karpov, Aleksandr Laptev, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the NVIDIA NeMo team's multi-channel speech recognition system for
the 7th CHiME Challenge Distant Automatic Speech Recognition (DASR) Task,
focusing on the development of a multi-channel, multi-speaker speech
recognition system tailored to transcribe speech from distributed microphones
and microphone arrays. The system predominantly comprises of the following
integral modules: the Speaker Diarization Module, Multi-channel Audio Front-End
Processing Module, and the ASR Module. These components collectively establish
a cascading system, meticulously processing multi-channel and multi-speaker
audio input. Moreover, this paper highlights the comprehensive optimization
process that significantly enhanced our system's performance. Our team's
submission is largely based on NeMo toolkits and will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Property-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling
  Technique for Synthetic Data Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tae Jin Park, He Huang, Coleman Hooper, Nithin Koluguri, Kunal Dhawan, Ante Jukic, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a sophisticated multi-speaker speech data simulator,
specifically engineered to generate multi-speaker speech recordings. A notable
feature of this simulator is its capacity to modulate the distribution of
silence and overlap via the adjustment of statistical parameters. This
capability offers a tailored training environment for developing neural models
suited for speaker diarization and voice activity detection. The acquisition of
substantial datasets for speaker diarization often presents a significant
challenge, particularly in multi-speaker scenarios. Furthermore, the precise
time stamp annotation of speech data is a critical factor for training both
speaker diarization and voice activity detection. Our proposed multi-speaker
simulator tackles these problems by generating large-scale audio mixtures that
maintain statistical properties closely aligned with the input parameters. We
demonstrate that the proposed multi-speaker simulator generates audio mixtures
with statistical properties that closely align with the input parameters
derived from real-world statistics. Additionally, we present the effectiveness
of speaker diarization and voice activity detection models, which have been
trained exclusively on the generated simulated datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic
  Music Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11029v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11029v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangda Wu, Dingyao Yu, <span class="highlight-author">Xu Tan</span>, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CLaMP: Contrastive Language-Music Pre-training, which learns
cross-modal representations between natural language and symbolic music using a
music encoder and a text encoder trained jointly with a contrastive loss. To
pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs.
It employed text dropout as a data augmentation technique and bar patching to
efficiently represent music data which reduces sequence length to less than
10\%. In addition, we developed a masked music model pre-training objective to
enhance the music encoder's comprehension of musical context and structure.
CLaMP integrates textual information to enable semantic search and zero-shot
classification for symbolic music, surpassing the capabilities of previous
models. To support the evaluation of semantic search and music classification,
we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in
ABC notation, each accompanied by a title, artist, genre, and description. In
comparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP
demonstrated comparable or superior performance on score-oriented datasets. Our
models and code are available at
https://github.com/microsoft/muzic/tree/main/clamp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, 5 tables, accepted by ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Replay to Remember: Continual Layer-Specific Fine-tuning for German
  Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theresa Pekarek Rosin, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Automatic Speech Recognition (ASR) models have shown significant
advances with the introduction of unsupervised or self-supervised training
techniques, these improvements are still only limited to a subsection of
languages and speakers. Transfer learning enables the adaptation of large-scale
multilingual models to not only low-resource languages but also to more
specific speaker groups. However, fine-tuning on data from new domains is
usually accompanied by a decrease in performance on the original domain.
Therefore, in our experiments, we examine how well the performance of
large-scale ASR models can be approximated for smaller domains, with our own
dataset of German Senior Voice Commands (SVC-de), and how much of the general
speech recognition performance can be preserved by selectively freezing parts
of the model during training. To further increase the robustness of the ASR
model to vocabulary and speakers outside of the fine-tuned domain, we apply
Experience Replay for continual learning. By adding only a fraction of data
from the original domain, we are able to reach Word-Error-Rates (WERs) below
5\% on the new domain, while stabilizing performance for general speech
recognition at acceptable WERs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, accepted and presented at ICANN 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Ultrasound Tongue Image prediction from EEG during speech
  production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamás Gábor Csapó, Frigyes Viktor Arthur, Péter Nagy, Ádám Boncz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous initial research has already been carried out to propose
speech-based BCI using brain signals (e.g. non-invasive EEG and invasive sEEG /
ECoG), but there is a lack of combined methods that investigate non-invasive
brain, articulation, and speech signals together and analyze the cognitive
processes in the brain, the kinematics of the articulatory movement and the
resulting speech signal. In this paper, we describe our multimodal
(electroencephalography, ultrasound tongue imaging, and speech) analysis and
synthesis experiments, as a feasibility study. We extend the analysis of brain
signals recorded during speech production with ultrasound-based articulation
data. From the brain signal measured with EEG, we predict ultrasound images of
the tongue with a fully connected deep neural network. The results show that
there is a weak but noticeable relationship between EEG and ultrasound tongue
images, i.e. the network can differentiate articulated speech and neutral
tongue position.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Speaking Style Conversion in the Waveform Domain Using Discrete
  Self-Supervised Units <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gallil Maimon, <span class="highlight-author">Yossi Adi</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DISSC, a novel, lightweight method that converts the rhythm,
pitch contour and timbre of a recording to a target speaker in a textless
manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on
timbre, and ignore people's unique speaking style (prosody). The proposed
approach uses a pretrained, self-supervised model for encoding speech to
discrete units, which makes it simple, effective, and fast to train. All
conversion modules are only trained on reconstruction like tasks, thus suitable
for any-to-many VC with no paired data. We introduce a suite of quantitative
and qualitative evaluation metrics for this setup, and empirically demonstrate
that DISSC significantly outperforms the evaluated baselines. Code and samples
are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DASA: Difficulty-Aware Semantic Augmentation for Speaker Verification <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Wang, Yang Zhang, <span class="highlight-author">Zhiyong Wu</span>, Zhihan Yang, Tao Wei, Kun Zou, <span class="highlight-author">Helen Meng</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is vital to the generalization ability and robustness of
deep neural networks (DNNs) models. Existing augmentation methods for speaker
verification manipulate the raw signal, which are time-consuming and the
augmented samples lack diversity. In this paper, we present a novel
difficulty-aware semantic augmentation (DASA) approach for speaker
verification, which can generate diversified training samples in speaker
embedding space with negligible extra computing cost. Firstly, we augment
training samples by perturbing speaker embeddings along semantic directions,
which are obtained from speaker-wise covariance matrices. Secondly, accurate
covariance matrices are estimated from robust speaker embeddings during
training, so we introduce difficultyaware additive margin softmax
(DAAM-Softmax) to obtain optimal speaker embeddings. Finally, we assume the
number of augmented samples goes to infinity and derive a closed-form upper
bound of the expected loss with DASA, which achieves compatibility and
efficiency. Extensive experiments demonstrate the proposed approach can achieve
a remarkable performance improvement. The best result achieves a 14.6% relative
reduction in EER metric on CN-Celeb evaluation set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Spoofing Speech Detection Using Rhythm Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingze Lu, Yuxiang Zhang, Wenchao Wang, Zengqiang Shang, Pengyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoofing speech detection is a hot and in-demand research field. However,
current spoofing speech detection systems is lack of convincing evidence. In
this paper, to increase the reliability of detection systems, the flaws of
rhythm information inherent in the TTS-generated speech are analyzed. TTS
models take text as input and utilize acoustic models to predict rhythm
information, which introduces artifacts in the rhythm information. By filtering
out vocal tract response, the remaining glottal flow with rhythm information
retains detection ability for TTS-generated speech. Based on these analyses, a
rhythm perturbation module is proposed to enhance the copy-synthesis data
augmentation method. Fake utterances generated by the proposed method force the
detecting model to pay attention to the artifacts in rhythm information and
effectively improve the ability to detect TTS-generated speech of the
anti-spoofing countermeasures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, submitted to icassp2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Take the aTrain. Introducing an Interface for the Accessible
  Transcription of Interviews 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Haberl, Jürgen Fleiß, Dominik Kowald, Stefan Thalmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  aTrain is an open-source and offline tool for transcribing audio data in
multiple languages with CPU and NVIDIA GPU support. It is specifically designed
for researchers using qualitative data generated from various forms of speech
interactions with research participants. aTrain requires no programming skills,
runs on most computers, does not require an internet connection, and was
verified not to upload data to any server. aTrain combines OpenAI's Whisper
model with speaker recognition to provide output that integrates with the
popular qualitative data analysis software tools MAXQDA and ATLAS.ti. It has an
easy-to-use graphical interface and is provided as a Windows-App through the
Microsoft Store allowing for simple installation by researchers. The source
code is freely available on GitHub. Having developed aTrain with a focus on
speed on local computers, we show that the transcription time on current mobile
CPUs is around 2 to 3 times the duration of the audio file using the
highest-accuracy transcription models. If an entry-level graphics card is
available, the transcription speed increases to 20% of the audio duration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Install via Microsoft store:
  apps.microsoft.com/store/detail/atrain/9N15Q44SZNS2. Github:
  github.com/BANDAS-Center/aTrain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MusicAgent: An AI Agent for Music Understanding and Generation with
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, <span class="highlight-author">Xu Tan</span>, Wei Ye, Shikun Zhang, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-empowered music processing is a diverse field that encompasses dozens of
tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension
tasks (e.g., music classification). For developers and amateurs, it is very
difficult to grasp all of these task to satisfy their requirements in music
processing, especially considering the huge differences in the representations
of music data and the model applicability across platforms among various tasks.
Consequently, it is necessary to build a system to organize and integrate these
tasks, and thus help practitioners to automatically analyze their demand and
call suitable tools as solutions to fulfill their requirements. Inspired by the
recent success of large language models (LLMs) in task automation, we develop a
system, named MusicAgent, which integrates numerous music-related tools and an
autonomous workflow to address user requirements. More specifically, we build
1) toolset that collects tools from diverse sources, including Hugging Face,
GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g.,
ChatGPT) to organize these tools and automatically decompose user requests into
multiple sub-tasks and invoke corresponding music tools. The primary goal of
this system is to free users from the intricacies of AI-music tools, enabling
them to concentrate on the creative aspect. By granting users the freedom to
effortlessly combine tools, the system offers a seamless and enriching music
experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BUT CHiME-7 system description 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Karafiát, Karel Veselý, Igor Szöke, Ladislav Mošner, Karel Beneš, Marcin Witkowski, Germán Barchi, Leonardo Pepino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the joint effort of Brno University of Technology (BUT),
AGH University of Krakow and University of Buenos Aires on the development of
Automatic Speech Recognition systems for the CHiME-7 Challenge. We train and
evaluate various end-to-end models with several toolkits. We heavily relied on
Guided Source Separation (GSS) to convert multi-channel audio to single
channel. The ASR is leveraging speech representations from models pre-trained
by self-supervised learning, and we do a fusion of several ASR systems. In
addition, we modified external data from the LibriSpeech corpus to become a
close domain and added it to the training. Our efforts were focused on the
far-field acoustic robustness sub-track of Task 1 - Distant Automatic Speech
Recognition (DASR), our systems use oracle segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, Chime-7 challenge 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLARA: Multilingual Contrastive Learning for Audio Representation
  Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kari A Noriy, Xiaosong Yang, Marcin Budka, Jian Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework for multilingual speech and sound
representation learning using contrastive learning. The lack of sizeable
labelled datasets hinders speech-processing research across languages. Recent
advances in contrastive learning provide self-supervised techniques to learn
from unlabelled data. Motivated by reducing data dependence and improving
generalisation across diverse languages and conditions, we develop a
multilingual contrastive framework. This framework enables models to acquire
shared representations across languages, facilitating cross-lingual transfer
with limited target language data.
  Additionally, capturing emotional cues within speech is challenging due to
subjective perceptual assessments. By learning expressive representations from
diverse, multilingual data in a self-supervised manner, our approach aims to
develop speech representations that encode emotive dimensions.
  Our method trains encoders on a large corpus of multi-lingual audio data.
Data augmentation techniques are employed to expand the dataset. The
contrastive learning approach trains the model to maximise agreement between
positive pairs and minimise agreement between negative pairs. Extensive
experiments demonstrate state-of-the-art performance of the proposed model on
emotion recognition, audio classification, and retrieval benchmarks under
zero-shot and few-shot conditions. This provides an effective approach for
acquiring shared and generalised speech representations across languages and
acoustic conditions while encoding latent emotional dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blind estimation of audio effects using an auto-encoder approach and
  differentiable signal processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Côme Peladeau, Geoffroy Peeters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind Estimation of Audio Effects (BE-AFX) aims at estimating the Audio
Effects (AFXs) applied to an original, unprocessed audio sample solely based on
the processed audio sample. To train such a system traditional approaches
optimize a loss between ground truth and estimated AFX parameters. This
involves knowing the exact implementation of the AFXs used for the process. In
this work, we propose an alternative solution that eliminates the requirement
for knowing this implementation. Instead, we introduce an auto-encoder
approach, which optimizes an audio quality metric. We explore, suggest, and
compare various implementations of commonly used mastering AFXs, using
differential signal processing or neural approximations. Our findings
demonstrate that our auto-encoder approach yields superior estimates of the
audio quality produced by a chain of AFXs, compared to the traditional
parameter-based approach, even if the latter provides a more accurate parameter
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unintended Memorization in Large ASR Models, and How to Mitigate It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lun Wang, Om Thakkar, Rajiv Mathews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well-known that neural networks can unintentionally memorize their
training examples, causing privacy concerns. However, auditing memorization in
large non-auto-regressive automatic speech recognition (ASR) models has been
challenging due to the high compute cost of existing methods such as hardness
calibration. In this work, we design a simple auditing method to measure
memorization in large ASR models without the extra compute overhead.
Concretely, we speed up randomly-generated utterances to create a mapping
between vocal and text information that is difficult to learn from typical
training examples. Hence, accurate predictions only for sped-up training
examples can serve as clear evidence for memorization, and the corresponding
accuracy can be used to measure memorization. Using the proposed method, we
showcase memorization in the state-of-the-art ASR models. To mitigate
memorization, we tried gradient clipping during training to bound the influence
of any individual example on the final model. We empirically show that clipping
each example's gradient can mitigate memorization for sped-up training examples
with up to 16 repetitions in the training set. Furthermore, we show that in
large-scale distributed training, clipping the average gradient on each compute
core maintains neutral model quality and compute cost while providing strong
privacy protection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EchoScan: Scanning Complex Indoor Geometries via Acoustic Echoes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inmo Yeon, Iljoo Jeong, Seungchul Lee, Jung-Woo Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate estimation of indoor space geometries is vital for constructing
precise digital twins, whose broad industrial applications include navigation
in unfamiliar environments and efficient evacuation planning, particularly in
low-light conditions. This study introduces EchoScan, a deep neural network
model that utilizes acoustic echoes to perform room geometry inference.
Conventional sound-based techniques rely on estimating geometry-related room
parameters such as wall position and room size, thereby limiting the diversity
of inferable room geometries. Contrarily, EchoScan overcomes this limitation by
directly inferring room floorplans and heights, thereby enabling it to handle
rooms with arbitrary shapes, including curved walls. The key innovation of
EchoScan is its ability to analyze the complex relationship between low- and
high-order reflections in room impulse responses (RIRs) using a
multi-aggregation module. The analysis of high-order reflections also enables
it to infer complex room shapes when echoes are unobservable from the position
of an audio device. Herein, EchoScan was trained and evaluated using RIRs
synthesized from complex environments, including the Manhattan and Atlanta
layouts, employing a practical audio device configuration compatible with
commercial, off-the-shelf devices. Compared with vision-based methods, EchoScan
demonstrated outstanding geometry estimation performance in rooms with various
shapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Separating Invisible Sounds Toward Universal Audiovisual Scene-Aware
  Sound Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Su, Ali Vosoughi, Shijian Deng, Yapeng Tian, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The audio-visual sound separation field assumes visible sources in videos,
but this excludes invisible sounds beyond the camera's view. Current methods
struggle with such sounds lacking visible cues. This paper introduces a novel
"Audio-Visual Scene-Aware Separation" (AVSA-Sep) framework. It includes a
semantic parser for visible and invisible sounds and a separator for
scene-informed separation. AVSA-Sep successfully separates both sound types,
with joint training and cross-modal alignment enhancing effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2023 - AV4D, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experimental Results of Underwater Sound Speed Profile Inversion by
  Few-shot Multi-task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Huang, Fan Gao, Junting Wang, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater Sound Speed Profile (SSP) distribution has great influence on the
propagation mode of acoustic signal, thus the fast and accurate estimation of
SSP is of great importance in building underwater observation systems. The
state-of-the-art SSP inversion methods include frameworks of matched field
processing (MFP), compressive sensing (CS), and feedforeward neural networks
(FNN), among which the FNN shows better real-time performance while maintain
the same level of accuracy. However, the training of FNN needs quite a lot
historical SSP samples, which is diffcult to be satisfied in many ocean areas.
This situation is called few-shot learning. To tackle this issue, we propose a
multi-task learning (MTL) model with partial parameter sharing among different
traning tasks. By MTL, common features could be extracted, thus accelerating
the learning process on given tasks, and reducing the demand for reference
samples, so as to enhance the generalization ability in few-shot learning. To
verify the feasibility and effectiveness of MTL, a deep-ocean experiment was
held in April 2023 at the South China Sea. Results shows that MTL outperforms
the state-of-the-art methods in terms of accuracy for SSP inversion, while
inherits the real-time advantage of FNN during the inversion stage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The CHiME-7 Challenge: System Description and Performance of NeMo Team's
  DASR System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tae Jin Park, He Huang, Ante Jukic, Kunal Dhawan, Krishna C. Puvvada, Nithin Koluguri, Nikolay Karpov, Aleksandr Laptev, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the NVIDIA NeMo team's multi-channel speech recognition system for
the 7th CHiME Challenge Distant Automatic Speech Recognition (DASR) Task,
focusing on the development of a multi-channel, multi-speaker speech
recognition system tailored to transcribe speech from distributed microphones
and microphone arrays. The system predominantly comprises of the following
integral modules: the Speaker Diarization Module, Multi-channel Audio Front-End
Processing Module, and the ASR Module. These components collectively establish
a cascading system, meticulously processing multi-channel and multi-speaker
audio input. Moreover, this paper highlights the comprehensive optimization
process that significantly enhanced our system's performance. Our team's
submission is largely based on NeMo toolkits and will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Property-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling
  Technique for Synthetic Data Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tae Jin Park, He Huang, Coleman Hooper, Nithin Koluguri, Kunal Dhawan, Ante Jukic, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a sophisticated multi-speaker speech data simulator,
specifically engineered to generate multi-speaker speech recordings. A notable
feature of this simulator is its capacity to modulate the distribution of
silence and overlap via the adjustment of statistical parameters. This
capability offers a tailored training environment for developing neural models
suited for speaker diarization and voice activity detection. The acquisition of
substantial datasets for speaker diarization often presents a significant
challenge, particularly in multi-speaker scenarios. Furthermore, the precise
time stamp annotation of speech data is a critical factor for training both
speaker diarization and voice activity detection. Our proposed multi-speaker
simulator tackles these problems by generating large-scale audio mixtures that
maintain statistical properties closely aligned with the input parameters. We
demonstrate that the proposed multi-speaker simulator generates audio mixtures
with statistical properties that closely align with the input parameters
derived from real-world statistics. Additionally, we present the effectiveness
of speaker diarization and voice activity detection models, which have been
trained exclusively on the generated simulated datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic
  Music Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11029v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11029v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangda Wu, Dingyao Yu, <span class="highlight-author">Xu Tan</span>, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CLaMP: Contrastive Language-Music Pre-training, which learns
cross-modal representations between natural language and symbolic music using a
music encoder and a text encoder trained jointly with a contrastive loss. To
pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs.
It employed text dropout as a data augmentation technique and bar patching to
efficiently represent music data which reduces sequence length to less than
10\%. In addition, we developed a masked music model pre-training objective to
enhance the music encoder's comprehension of musical context and structure.
CLaMP integrates textual information to enable semantic search and zero-shot
classification for symbolic music, surpassing the capabilities of previous
models. To support the evaluation of semantic search and music classification,
we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in
ABC notation, each accompanied by a title, artist, genre, and description. In
comparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP
demonstrated comparable or superior performance on score-oriented datasets. Our
models and code are available at
https://github.com/microsoft/muzic/tree/main/clamp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, 5 tables, accepted by ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Replay to Remember: Continual Layer-Specific Fine-tuning for German
  Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theresa Pekarek Rosin, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Automatic Speech Recognition (ASR) models have shown significant
advances with the introduction of unsupervised or self-supervised training
techniques, these improvements are still only limited to a subsection of
languages and speakers. Transfer learning enables the adaptation of large-scale
multilingual models to not only low-resource languages but also to more
specific speaker groups. However, fine-tuning on data from new domains is
usually accompanied by a decrease in performance on the original domain.
Therefore, in our experiments, we examine how well the performance of
large-scale ASR models can be approximated for smaller domains, with our own
dataset of German Senior Voice Commands (SVC-de), and how much of the general
speech recognition performance can be preserved by selectively freezing parts
of the model during training. To further increase the robustness of the ASR
model to vocabulary and speakers outside of the fine-tuned domain, we apply
Experience Replay for continual learning. By adding only a fraction of data
from the original domain, we are able to reach Word-Error-Rates (WERs) below
5\% on the new domain, while stabilizing performance for general speech
recognition at acceptable WERs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, accepted and presented at ICANN 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Ultrasound Tongue Image prediction from EEG during speech
  production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamás Gábor Csapó, Frigyes Viktor Arthur, Péter Nagy, Ádám Boncz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous initial research has already been carried out to propose
speech-based BCI using brain signals (e.g. non-invasive EEG and invasive sEEG /
ECoG), but there is a lack of combined methods that investigate non-invasive
brain, articulation, and speech signals together and analyze the cognitive
processes in the brain, the kinematics of the articulatory movement and the
resulting speech signal. In this paper, we describe our multimodal
(electroencephalography, ultrasound tongue imaging, and speech) analysis and
synthesis experiments, as a feasibility study. We extend the analysis of brain
signals recorded during speech production with ultrasound-based articulation
data. From the brain signal measured with EEG, we predict ultrasound images of
the tongue with a fully connected deep neural network. The results show that
there is a weak but noticeable relationship between EEG and ultrasound tongue
images, i.e. the network can differentiate articulated speech and neutral
tongue position.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Speaking Style Conversion in the Waveform Domain Using Discrete
  Self-Supervised Units <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gallil Maimon, <span class="highlight-author">Yossi Adi</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DISSC, a novel, lightweight method that converts the rhythm,
pitch contour and timbre of a recording to a target speaker in a textless
manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on
timbre, and ignore people's unique speaking style (prosody). The proposed
approach uses a pretrained, self-supervised model for encoding speech to
discrete units, which makes it simple, effective, and fast to train. All
conversion modules are only trained on reconstruction like tasks, thus suitable
for any-to-many VC with no paired data. We introduce a suite of quantitative
and qualitative evaluation metrics for this setup, and empirically demonstrate
that DISSC significantly outperforms the evaluated baselines. Code and samples
are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-17T00:00:00Z">2023-10-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando López, Jordi Luque, Carlos Segura, Pablo Gómez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice-based interfaces rely on a wake-up word mechanism to initiate
communication with devices. However, achieving a robust, energy-efficient, and
fast detection remains a challenge. This paper addresses these real production
needs by enhancing data with temporal alignments and using detection based on
two phases with multi-resolution. It employs two models: a lightweight
on-device model for real-time processing of the audio stream and a verification
model on the server-side, which is an ensemble of heterogeneous architectures
that refine detection. This scheme allows the optimization of two operating
points. To protect privacy, audio features are sent to the cloud instead of raw
audio. The study investigated different parametric configurations for feature
extraction to select one for on-device detection and another for the
verification model. Furthermore, thirteen different audio classifiers were
compared in terms of performance and inference time. The proposed ensemble
outperforms our stronger classifier in every noise condition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Fidelity Noise Reduction with Differentiable Signal Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian J. Steinmetz, Thomas Walther, Joshua D. Reiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Noise reduction techniques based on deep learning have demonstrated
impressive performance in enhancing the overall quality of recorded speech.
While these approaches are highly performant, their application in audio
engineering can be limited due to a number of factors. These include operation
only on speech without support for music, lack of real-time capability, lack of
interpretable control parameters, operation at lower sample rates, and a
tendency to introduce artifacts. On the other hand, signal processing-based
noise reduction algorithms offer fine-grained control and operation on a broad
range of content, however, they often require manual operation to achieve the
best results. To address the limitations of both approaches, in this work we
introduce a method that leverages a signal processing-based denoiser that when
combined with a neural network controller, enables fully automatic and
high-fidelity noise reduction on both speech and music signals. We evaluate our
proposed method with objective metrics and a perceptual listening test. Our
evaluation reveals that speech enhancement models can be extended to music,
however training the model to remove only stationary noise is critical.
Furthermore, our proposed approach achieves performance on par with the deep
learning models, while being significantly more efficient and introducing fewer
artifacts in some cases. Listening examples are available online at
https://tape.it/research/denoiser .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 155th Convention of the Audio
  Engineering Society</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serenade: A Model for Human-in-the-loop Automatic Chord Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik Vincent Koops, Gianluca Micchi, Ilaria Manco, Elio Quinton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational harmony analysis is important for MIR tasks such as automatic
segmentation, corpus analysis and automatic chord label estimation. However,
recent research into the ambiguous nature of musical harmony, causing limited
inter-rater agreement, has made apparent that there is a glass ceiling for
common metrics such as accuracy. Commonly, these issues are addressed either in
the training data itself by creating majority-rule annotations or during the
training phase by learning soft targets. We propose a novel alternative
approach in which a human and an autoregressive model together co-create a
harmonic annotation for an audio track. After automatically generating harmony
predictions, a human sparsely annotates parts with low model confidence and the
model then adjusts its predictions following human guidance. We evaluate our
model on a dataset of popular music and we show that, with this
human-in-the-loop approach, harmonic analysis performance improves over a
model-only approach. The human contribution is amplified by the second,
constrained prediction of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MMRP23. 7 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Content-based Features from Multiple Acoustic Models for
  Singing <span class="highlight-title">Voice Conversion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyao Zhang, Yicheng Gu, Haopeng Chen, Zihao Fang, Lexiao Zou, Liumeng Xue, Zhizheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singing voice conversion (SVC) is a technique to enable an arbitrary singer
to sing an arbitrary song. To achieve that, it is important to obtain
speaker-agnostic representations from source audio, which is a challenging
task. A common solution is to extract content-based features (e.g., PPGs) from
a pretrained acoustic model. However, the choices for acoustic models are vast
and varied. It is yet to be explored what characteristics of content features
from different acoustic models are, and whether integrating multiple content
features can help each other. Motivated by that, this study investigates three
distinct content features, sourcing from WeNet, Whisper, and ContentVec,
respectively. We explore their complementary roles in intelligibility, prosody,
and conversion similarity for SVC. By integrating the multiple content features
with a diffusion-based SVC model, our SVC system achieves superior conversion
performance on both objective and subjective evaluation in comparison to a
single source of content features. Our demo page and code can be available
https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form Simultaneous Speech Translation: Thesis Proposal <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Polák
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous speech translation (SST) aims to provide real-time translation
of spoken language, even before the speaker finishes their sentence.
Traditionally, SST has been addressed primarily by cascaded systems that
decompose the task into subtasks, including speech recognition, segmentation,
and machine translation. However, the advent of deep learning has sparked
significant interest in end-to-end (E2E) systems. Nevertheless, a major
limitation of most approaches to E2E SST reported in the current literature is
that they assume that the source speech is pre-segmented into sentences, which
is a significant obstacle for practical, real-world applications. This thesis
proposal addresses end-to-end simultaneous speech translation, particularly in
the long-form setting, i.e., without pre-segmentation. We present a survey of
the latest advancements in E2E SST, assess the primary obstacles in SST and its
relevance to long-form scenarios, and suggest approaches to tackle these
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCNLP-AACL SRW 2023 - camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System <span class="chip">EMNLP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Waheed, Bashar Talafha, Peter Suvellin, Abdelrahman Elmadney, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arabic is a complex language with many varieties and dialects spoken by over
450 millions all around the world. Due to the linguistic diversity and
variations, it is challenging to build a robust and generalized ASR system for
Arabic. In this work, we address this gap by developing and demoing a system,
dubbed VoxArabica, for dialect identification (DID) as well as automatic speech
recognition (ASR) of Arabic. We train a wide range of models such as HuBERT
(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR
tasks. Our DID models are trained to identify 17 different dialects in addition
to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.
Additionally, for the remaining dialects in ASR, we provide the option to
choose various models such as Whisper and MMS in a zero-shot setting. We
integrate these models into a single web interface with diverse features such
as audio recording, file upload, model selection, and the option to raise flags
for incorrect outputs. Overall, we believe VoxArabica will be useful for a wide
range of audiences concerned with Arabic research. Our system is currently
running at https://cdce-206-12-100-168.ngrok.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ArabicNLP conference co-located with EMNLP'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lyricist-Singer Entropy Affects Lyric-Lyricist Classification
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitsuki Morita, Masato Kikuchi, Tadachika Ozono
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although lyrics represent an essential component of music, few music
information processing studies have been conducted on the characteristics of
lyricists. Because these characteristics may be valuable for musical
applications, such as recommendations, they warrant further study. We
considered a potential method that extracts features representing the
characteristics of lyricists from lyrics. Because these features must be
identified prior to extraction, we focused on lyricists with easily
identifiable features. We believe that it is desirable for singers to perform
unique songs that share certain characteristics specific to the singer.
Accordingly, we hypothesized that lyricists account for the unique
characteristics of the singers they write lyrics for. In other words,
lyric-lyricist classification performance or the ease of capturing the features
of a lyricist from the lyrics may depend on the variety of singers. In this
study, we observed a relationship between lyricist-singer entropy or the
variety of singers associated with a single lyricist and lyric-lyricist
classification performance. As an example, the lyricist-singer entropy is
minimal when the lyricist writes lyrics for only one singer. In our
experiments, we grouped lyricists among five groups in terms of lyricist-singer
entropy and assessed the lyric-lyricist classification performance within each
group. Consequently, the best F1 score was obtained for the group with the
lowest lyricist-singer entropy. Our results suggest that further analyses of
the features contributing to lyric-lyricist classification performance on the
lowest lyricist-singer entropy group may improve the feature extraction task
for lyricists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 10th International Conference on Advanced Informatics: Concepts,
  Theory and Applications (ICAICTA 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correction Focused Language Model Training for Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingyi Ma, Zhe Liu, Ozlem Kalinli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have been commonly adopted to boost the performance of
automatic speech recognition (ASR) particularly in domain adaptation tasks.
Conventional way of LM training treats all the words in corpora equally,
resulting in suboptimal improvements in ASR performance. In this work, we
introduce a novel correction focused LM training approach which aims to
prioritize ASR fallible words. The word-level ASR fallibility score,
representing the likelihood of ASR mis-recognition, is defined and shaped as a
prior word distribution to guide the LM training. To enable correction focused
training with text-only corpora, large language models (LLMs) are employed as
fallibility score predictors and text generators through multi-task
fine-tuning. Experimental results for domain adaptation tasks demonstrate the
effectiveness of our proposed method. Compared with conventional LMs,
correction focused training achieves up to relatively 5.5% word error rate
(WER) reduction in sufficient text scenarios. In insufficient text scenarios,
LM training with LLM-generated text achieves up to relatively 13% WER
reduction, while correction focused training further obtains up to relatively
6% WER reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A High Fidelity and Low Complexity Neural Audio Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhe Liu, Wei Xiao, Meng Wang, Shan Yang, Yupeng Shi, Yuyong Kang, <span class="highlight-author">Dan Su</span>, Shidong Shang, <span class="highlight-author">Dong Yu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio coding is an essential module in the real-time communication system.
Neural audio codecs can compress audio samples with a low bitrate due to the
strong modeling and generative capabilities of deep neural networks. To address
the poor high-frequency expression and high computational cost and storage
consumption, we proposed an integrated framework that utilizes a neural network
to model wide-band components and adopts traditional signal processing to
compress high-band components according to psychological hearing knowledge.
Inspired by auditory perception theory, a perception-based loss function is
designed to improve harmonic modeling. Besides, generative adversarial network
(GAN) compression is proposed for the first time for neural audio codecs. Our
method is superior to prior advanced neural codecs across subjective and
objective metrics and allows real-time inference on desktop and mobile.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial HuBERT: Self-supervised Spatial Speech Representation Learning
  for a Single Talker from Multi-channel Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoni Dimitriadis, Siqi Pan, Vidhyasaharan Sethu, Beena Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has been used to leverage unlabelled data, improving
accuracy and generalisation of speech systems through the training of
representation models. While many recent works have sought to produce effective
representations across a variety of acoustic domains, languages, modalities and
even simultaneous speakers, these studies have all been limited to
single-channel audio recordings. This paper presents Spatial HuBERT, a
self-supervised speech representation model that learns both acoustic and
spatial information pertaining to a single speaker in a potentially noisy
environment by using multi-channel audio inputs. Spatial HuBERT learns
representations that outperform state-of-the-art single-channel speech
representations on a variety of spatial downstream tasks, particularly in
reverberant and noisy environments. We also demonstrate the utility of the
representations learned by Spatial HuBERT on a speech localisation downstream
task. Along with this paper, we publicly release a new dataset of 100 000
simulated first-order ambisonics room impulse responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative error correction for code-switching speech recognition using
  large language models <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Hexin Liu, Sabato Marco Siniscalchi, Eng Siong Chng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-switching (CS) speech refers to the phenomenon of mixing two or more
languages within the same sentence. Despite the recent advances in automatic
speech recognition (ASR), CS-ASR is still a challenging task ought to the
grammatical structure complexity of the phenomenon and the data scarcity of
specific training corpus. In this work, we propose to leverage large language
models (LLMs) and lists of hypotheses generated by an ASR to address the CS
problem. Specifically, we first employ multiple well-trained ASR models for
N-best hypotheses generation, with the aim of increasing the diverse and
informative elements in the set of hypotheses. Next, we utilize the LLMs to
learn the hypotheses-to-transcription (H2T) mapping by adding a trainable
low-rank adapter. Such a generative error correction (GER) method directly
predicts the accurate transcription according to its expert linguistic
knowledge and N-best hypotheses, resulting in a paradigm shift from the
traditional language model rescoring or error correction techniques.
Experimental evidence demonstrates that GER significantly enhances CS-ASR
accuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show
remarkable data efficiency for H2T learning, providing a potential solution to
the data scarcity problem of CS-ASR in low-resource languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocSelect: Target Speaker Localization with an Auditory Selective
  Hearing Mechanism <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Chen, Xinyuan Qian, Zexu Pan, Kainan Chen, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevailing noise-resistant and reverberation-resistant localization
algorithms primarily emphasize separating and providing directional output for
each speaker in multi-speaker scenarios, without association with the identity
of speakers. In this paper, we present a target speaker localization algorithm
with a selective hearing mechanism. Given a reference speech of the target
speaker, we first produce a speaker-dependent spectrogram mask to eliminate
interfering speakers' speech. Subsequently, a Long short-term memory (LSTM)
network is employed to extract the target speaker's location from the filtered
spectrogram. Experiments validate the superiority of our proposed method over
the existing algorithms for different scale invariant signal-to-noise ratios
(SNR) conditions. Specifically, at SNR = -10 dB, our proposed network LocSelect
achieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Contextual Recognition In Automatic Speech Recognition Systems
  By Semantic Lattice Rescoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09680v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09680v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankitha Sudarshan, Vinay Samuel, Parth Patwa, Ibtihel Amara, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) has witnessed a profound research
interest. Recent breakthroughs have given ASR systems different prospects such
as faithfully transcribing spoken language, which is a pivotal advancement in
building conversational agents. However, there is still an imminent challenge
of accurately discerning context-dependent words and phrases. In this work, we
propose a novel approach for enhancing contextual recognition within ASR
systems via semantic lattice processing leveraging the power of deep learning
models in accurately delivering spot-on transcriptions across a wide variety of
vocabularies and speaking styles. Our solution consists of using Hidden Markov
Models and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks
(DNN) models integrating both language and acoustic modeling for better
accuracy. We infused our network with the use of a transformer-based model to
properly rescore the word lattice achieving remarkable capabilities with a
palpable reduction in Word Error Rate (WER). We demonstrate the effectiveness
of our proposed framework on the LibriSpeech dataset with empirical analyses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2209.01250,
  arXiv:2301.06735 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Interpreter Understands Your Meaning: End-to-end Spoken Language
  Understanding Aided by Speech Translation <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mutian He, Philip N. Garner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end spoken language understanding (SLU) remains elusive even with
current large pretrained language models on text and speech, especially in
multilingual cases. Machine translation has been established as a powerful
pretraining objective on text as it enables the model to capture high-level
semantics of the input utterance and associations between different languages,
which is desired for speech models that work on lower-level acoustic frames.
Motivated particularly by the task of cross-lingual SLU, we demonstrate that
the task of speech translation (ST) is a good means of pretraining speech
models for end-to-end SLU on both intra- and cross-lingual scenarios.
  By introducing ST, our models reach higher performance over baselines on
monolingual and multilingual intent classification as well as spoken question
answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the
effectiveness of our methods, we also create new benchmark datasets from both
synthetic and real sources, for speech summarization and low-resource/zero-shot
transfer from English to French or Spanish. We further show the value of
preserving knowledge for the ST pretraining task for better downstream
performance, possibly using Bayesian transfer regularizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures; accepted by Findings of EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wav2vec-based Detection and Severity Level Classification of Dysarthria
  from Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Javanmardi, Saska Tirronen, Manila Kodali, Sudarsana Reddy Kadiri, Paavo Alku
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic detection and severity level classification of dysarthria directly
from acoustic speech signals can be used as a tool in medical diagnosis. In
this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor
to build detection and severity level classification systems for dysarthric
speech. The experiments were carried out with the popularly used UA-speech
database. In the detection experiments, the results revealed that the best
performance was obtained using the embeddings from the first layer of the
wav2vec model that yielded an absolute improvement of 1.23% in accuracy
compared to the best performing baseline feature (spectrogram). In the studied
severity level classification task, the results revealed that the embeddings
from the final layer gave an absolute improvement of 10.62% in accuracy
compared to the best baseline features (mel-frequency cepstral coefficients).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>copyright 2023 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analysis and Detection of Pathological Voice using Glottal Source
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sudarsana Reddy Kadiri, Paavo Alku
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic detection of voice pathology enables objective assessment and
earlier intervention for the diagnosis. This study provides a systematic
analysis of glottal source features and investigates their effectiveness in
voice pathology detection. Glottal source features are extracted using glottal
flows estimated with the quasi-closed phase (QCP) glottal inverse filtering
method, using approximate glottal source signals computed with the zero
frequency filtering (ZFF) method, and using acoustic voice signals directly. In
addition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from
the glottal source waveforms computed by QCP and ZFF to effectively capture the
variations in glottal source spectra of pathological voice. Experiments were
carried out using two databases, the Hospital Universitario Principe de
Asturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database.
Analysis of features revealed that the glottal source contains information that
discriminates normal and pathological voice. Pathology detection experiments
were carried out using support vector machine (SVM). From the detection
experiments it was observed that the performance achieved with the studied
glottal source features is comparable or better than that of conventional MFCCs
and perceptual linear prediction (PLP) features. The best detection performance
was achieved when the glottal source features were combined with the
conventional MFCCs and PLP features, which indicates the complementary nature
of the features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2020 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RVAE-EM: Generative speech dereverberation based on recurrent
  variational auto-encoder and convolutive transfer function <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyu Wang, Xiaofei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In indoor scenes, reverberation is a crucial factor in degrading the
perceived quality and intelligibility of speech. In this work, we propose a
generative dereverberation method. Our approach is based on a probabilistic
model utilizing a recurrent variational auto-encoder (RVAE) network and the
convolutive transfer function (CTF) approximation. Different from most previous
approaches, the output of our RVAE serves as the prior of the clean speech. And
our target is the maximum a posteriori (MAP) estimation of clean speech, which
is achieved iteratively through the expectation maximization (EM) algorithm.
The proposed method integrates the capabilities of network-based speech prior
modelling and CTF-based observation modelling. Experiments on single-channel
speech dereverberation show that the proposed generative method noticeably
outperforms the advanced discriminative networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using
  Spatial Transformer Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19130v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19130v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        László Tóth, Amin Honarmandi Shandiz, Gábor Gosztolya, Csapó Tamás Gábor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to the latest deep learning algorithms, silent speech interfaces (SSI)
are now able to synthesize intelligible speech from articulatory movement data
under certain conditions. However, the resulting models are rather
speaker-specific, making a quick switch between users troublesome. Even for the
same speaker, these models perform poorly cross-session, i.e. after dismounting
and re-mounting the recording equipment. To aid quick speaker and session
adaptation of ultrasound tongue imaging-based SSI models, we extend our deep
networks with a spatial transformer network (STN) module, capable of performing
an affine transformation on the input images. Although the STN part takes up
only about 10% of the network, our experiments show that adapting just the STN
module might allow to reduce MSE by 88% on the average, compared to retraining
the whole network. The improvement is even larger (around 92%) when adapting
the network to different recording sessions from the same speaker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HumTrans: A Novel Open-Source <span class="highlight-title">Dataset</span> for Humming Melody Transcription
  and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shansong Liu, Xu Li, Dian Li, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the HumTrans dataset, which is publicly available and
primarily designed for humming melody transcription. The dataset can also serve
as a foundation for downstream tasks such as humming melody based music
generation. It consists of 500 musical compositions of different genres and
languages, with each composition divided into multiple segments. In total, the
dataset comprises 1000 music segments. To collect this humming dataset, we
employed 10 college students, all of whom are either music majors or proficient
in playing at least one musical instrument. Each of them hummed every segment
twice using the web recording interface provided by our designed website. The
humming recordings were sampled at a frequency of 44,100 Hz. During the humming
session, the main interface provides a musical score for students to reference,
with the melody audio playing simultaneously to aid in capturing both melody
and rhythm. The dataset encompasses approximately 56.22 hours of audio, making
it the largest known humming dataset to date. The dataset will be released on
Hugging Face, and we will provide a GitHub repository containing baseline
results and evaluation codes.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">26</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando López, Jordi Luque, Carlos Segura, Pablo Gómez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice-based interfaces rely on a wake-up word mechanism to initiate
communication with devices. However, achieving a robust, energy-efficient, and
fast detection remains a challenge. This paper addresses these real production
needs by enhancing data with temporal alignments and using detection based on
two phases with multi-resolution. It employs two models: a lightweight
on-device model for real-time processing of the audio stream and a verification
model on the server-side, which is an ensemble of heterogeneous architectures
that refine detection. This scheme allows the optimization of two operating
points. To protect privacy, audio features are sent to the cloud instead of raw
audio. The study investigated different parametric configurations for feature
extraction to select one for on-device detection and another for the
verification model. Furthermore, thirteen different audio classifiers were
compared in terms of performance and inference time. The proposed ensemble
outperforms our stronger classifier in every noise condition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Fidelity Noise Reduction with Differentiable Signal Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian J. Steinmetz, Thomas Walther, Joshua D. Reiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Noise reduction techniques based on deep learning have demonstrated
impressive performance in enhancing the overall quality of recorded speech.
While these approaches are highly performant, their application in audio
engineering can be limited due to a number of factors. These include operation
only on speech without support for music, lack of real-time capability, lack of
interpretable control parameters, operation at lower sample rates, and a
tendency to introduce artifacts. On the other hand, signal processing-based
noise reduction algorithms offer fine-grained control and operation on a broad
range of content, however, they often require manual operation to achieve the
best results. To address the limitations of both approaches, in this work we
introduce a method that leverages a signal processing-based denoiser that when
combined with a neural network controller, enables fully automatic and
high-fidelity noise reduction on both speech and music signals. We evaluate our
proposed method with objective metrics and a perceptual listening test. Our
evaluation reveals that speech enhancement models can be extended to music,
however training the model to remove only stationary noise is critical.
Furthermore, our proposed approach achieves performance on par with the deep
learning models, while being significantly more efficient and introducing fewer
artifacts in some cases. Listening examples are available online at
https://tape.it/research/denoiser .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 155th Convention of the Audio
  Engineering Society</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zipformer: A faster and better encoder for automatic speech recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, Yifan Yang, Zengrui Jin, Long Lin, Daniel Povey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Conformer has become the most popular encoder model for automatic speech
recognition (ASR). It adds convolution modules to a transformer to learn both
local and global dependencies. In this work we describe a faster, more
memory-efficient, and better-performing transformer, called Zipformer. Modeling
changes include: 1) a U-Net-like encoder structure where middle stacks operate
at lower frame rates; 2) reorganized block structure with more modules, within
which we re-use attention weights for efficiency; 3) a modified form of
LayerNorm called BiasNorm allows us to retain some length information; 4) new
activation functions SwooshR and SwooshL work better than Swish. We also
propose a new optimizer, called ScaledAdam, which scales the update by each
tensor's current scale to keep the relative change about the same, and also
explictly learns the parameter scale. It achieves faster convergence and better
performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and
WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer
over other state-of-the-art ASR models. Our code is publicly available at
https://github.com/k2-fsa/icefall.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serenade: A Model for Human-in-the-loop Automatic Chord Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik Vincent Koops, Gianluca Micchi, Ilaria Manco, Elio Quinton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational harmony analysis is important for MIR tasks such as automatic
segmentation, corpus analysis and automatic chord label estimation. However,
recent research into the ambiguous nature of musical harmony, causing limited
inter-rater agreement, has made apparent that there is a glass ceiling for
common metrics such as accuracy. Commonly, these issues are addressed either in
the training data itself by creating majority-rule annotations or during the
training phase by learning soft targets. We propose a novel alternative
approach in which a human and an autoregressive model together co-create a
harmonic annotation for an audio track. After automatically generating harmony
predictions, a human sparsely annotates parts with low model confidence and the
model then adjusts its predictions following human guidance. We evaluate our
model on a dataset of popular music and we show that, with this
human-in-the-loop approach, harmonic analysis performance improves over a
model-only approach. The human contribution is amplified by the second,
constrained prediction of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MMRP23. 7 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Content-based Features from Multiple Acoustic Models for
  Singing <span class="highlight-title">Voice Conversion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyao Zhang, Yicheng Gu, Haopeng Chen, Zihao Fang, Lexiao Zou, Liumeng Xue, Zhizheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singing voice conversion (SVC) is a technique to enable an arbitrary singer
to sing an arbitrary song. To achieve that, it is important to obtain
speaker-agnostic representations from source audio, which is a challenging
task. A common solution is to extract content-based features (e.g., PPGs) from
a pretrained acoustic model. However, the choices for acoustic models are vast
and varied. It is yet to be explored what characteristics of content features
from different acoustic models are, and whether integrating multiple content
features can help each other. Motivated by that, this study investigates three
distinct content features, sourcing from WeNet, Whisper, and ContentVec,
respectively. We explore their complementary roles in intelligibility, prosody,
and conversion similarity for SVC. By integrating the multiple content features
with a diffusion-based SVC model, our SVC system achieves superior conversion
performance on both objective and subjective evaluation in comparison to a
single source of content features. Our demo page and code can be available
https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form Simultaneous Speech Translation: Thesis Proposal <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Polák
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous speech translation (SST) aims to provide real-time translation
of spoken language, even before the speaker finishes their sentence.
Traditionally, SST has been addressed primarily by cascaded systems that
decompose the task into subtasks, including speech recognition, segmentation,
and machine translation. However, the advent of deep learning has sparked
significant interest in end-to-end (E2E) systems. Nevertheless, a major
limitation of most approaches to E2E SST reported in the current literature is
that they assume that the source speech is pre-segmented into sentences, which
is a significant obstacle for practical, real-world applications. This thesis
proposal addresses end-to-end simultaneous speech translation, particularly in
the long-form setting, i.e., without pre-segmentation. We present a survey of
the latest advancements in E2E SST, assess the primary obstacles in SST and its
relevance to long-form scenarios, and suggest approaches to tackle these
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCNLP-AACL SRW 2023 - camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System <span class="chip">EMNLP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Waheed, Bashar Talafha, Peter Suvellin, Abdelrahman Elmadney, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arabic is a complex language with many varieties and dialects spoken by over
450 millions all around the world. Due to the linguistic diversity and
variations, it is challenging to build a robust and generalized ASR system for
Arabic. In this work, we address this gap by developing and demoing a system,
dubbed VoxArabica, for dialect identification (DID) as well as automatic speech
recognition (ASR) of Arabic. We train a wide range of models such as HuBERT
(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR
tasks. Our DID models are trained to identify 17 different dialects in addition
to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.
Additionally, for the remaining dialects in ASR, we provide the option to
choose various models such as Whisper and MMS in a zero-shot setting. We
integrate these models into a single web interface with diverse features such
as audio recording, file upload, model selection, and the option to raise flags
for incorrect outputs. Overall, we believe VoxArabica will be useful for a wide
range of audiences concerned with Arabic research. Our system is currently
running at https://cdce-206-12-100-168.ngrok.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ArabicNLP conference co-located with EMNLP'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lyricist-Singer Entropy Affects Lyric-Lyricist Classification
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitsuki Morita, Masato Kikuchi, Tadachika Ozono
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although lyrics represent an essential component of music, few music
information processing studies have been conducted on the characteristics of
lyricists. Because these characteristics may be valuable for musical
applications, such as recommendations, they warrant further study. We
considered a potential method that extracts features representing the
characteristics of lyricists from lyrics. Because these features must be
identified prior to extraction, we focused on lyricists with easily
identifiable features. We believe that it is desirable for singers to perform
unique songs that share certain characteristics specific to the singer.
Accordingly, we hypothesized that lyricists account for the unique
characteristics of the singers they write lyrics for. In other words,
lyric-lyricist classification performance or the ease of capturing the features
of a lyricist from the lyrics may depend on the variety of singers. In this
study, we observed a relationship between lyricist-singer entropy or the
variety of singers associated with a single lyricist and lyric-lyricist
classification performance. As an example, the lyricist-singer entropy is
minimal when the lyricist writes lyrics for only one singer. In our
experiments, we grouped lyricists among five groups in terms of lyricist-singer
entropy and assessed the lyric-lyricist classification performance within each
group. Consequently, the best F1 score was obtained for the group with the
lowest lyricist-singer entropy. Our results suggest that further analyses of
the features contributing to lyric-lyricist classification performance on the
lowest lyricist-singer entropy group may improve the feature extraction task
for lyricists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 10th International Conference on Advanced Informatics: Concepts,
  Theory and Applications (ICAICTA 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Shallow Fusion of Backward Language Model for End-to-End
  Speech Recognition <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsunori Ogawa, Takafumi Moriya, Naoyuki Kamo, Naohiro Tawara, Marc Delcroix
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new shallow fusion (SF) method to exploit an external backward
language model (BLM) for end-to-end automatic speech recognition (ASR). The BLM
has complementary characteristics with a forward language model (FLM), and the
effectiveness of their combination has been confirmed by rescoring ASR
hypotheses as post-processing. In the proposed SF, we iteratively apply the BLM
to partial ASR hypotheses in the backward direction (i.e., from the possible
next token to the start symbol) during decoding, substituting the newly
calculated BLM scores for the scores calculated at the last iteration. To
enhance the effectiveness of this iterative SF (ISF), we train a partial
sentence-aware BLM (PBLM) using reversed text data including partial sentences,
considering the framework of ISF. In experiments using an attention-based
encoder-decoder ASR system, we confirmed that ISF using the PBLM shows
comparable performance with SF using the FLM. By performing ISF, early pruning
of prospective hypotheses can be prevented during decoding, and we can obtain a
performance improvement compared to applying the PBLM as post-processing.
Finally, we confirmed that, by combining SF and ISF, further performance
improvement can be obtained thanks to the complementarity of the FLM and PBLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced accent/dialect identification and accentedness assessment with
  multi-embedding models and automatic speech recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahram Ghorbani, John H. L. Hansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately classifying accents and assessing accentedness in non-native
speakers are both challenging tasks due to the complexity and diversity of
accent and dialect variations. In this study, embeddings from advanced
pre-trained language identification (LID) and speaker identification (SID)
models are leveraged to improve the accuracy of accent classification and
non-native accentedness assessment. Findings demonstrate that employing
pre-trained LID and SID models effectively encodes accent/dialect information
in speech. Furthermore, the LID and SID encoded accent information complement
an end-to-end accent identification (AID) model trained from scratch. By
incorporating all three embeddings, the proposed multi-embedding AID system
achieves superior accuracy in accent identification. Next, we investigate
leveraging automatic speech recognition (ASR) and accent identification models
to explore accentedness estimation. The ASR model is an end-to-end
connectionist temporal classification (CTC) model trained exclusively with
en-US utterances. The ASR error rate and en-US output of the AID model are
leveraged as objective accentedness scores. Evaluation results demonstrate a
strong correlation between the scores estimated by the two models.
Additionally, a robust correlation between the objective accentedness scores
and subjective scores based on human perception is demonstrated, providing
evidence for the reliability and validity of utilizing AID-based and ASR-based
systems for accentedness assessment in non-native speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to The Journal of the Acoustical Society of America</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correction Focused Language Model Training for Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingyi Ma, Zhe Liu, Ozlem Kalinli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have been commonly adopted to boost the performance of
automatic speech recognition (ASR) particularly in domain adaptation tasks.
Conventional way of LM training treats all the words in corpora equally,
resulting in suboptimal improvements in ASR performance. In this work, we
introduce a novel correction focused LM training approach which aims to
prioritize ASR fallible words. The word-level ASR fallibility score,
representing the likelihood of ASR mis-recognition, is defined and shaped as a
prior word distribution to guide the LM training. To enable correction focused
training with text-only corpora, large language models (LLMs) are employed as
fallibility score predictors and text generators through multi-task
fine-tuning. Experimental results for domain adaptation tasks demonstrate the
effectiveness of our proposed method. Compared with conventional LMs,
correction focused training achieves up to relatively 5.5% word error rate
(WER) reduction in sufficient text scenarios. In insufficient text scenarios,
LM training with LLM-generated text achieves up to relatively 13% WER
reduction, while correction focused training further obtains up to relatively
6% WER reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A High Fidelity and Low Complexity Neural Audio Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhe Liu, Wei Xiao, Meng Wang, Shan Yang, Yupeng Shi, Yuyong Kang, <span class="highlight-author">Dan Su</span>, Shidong Shang, <span class="highlight-author">Dong Yu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio coding is an essential module in the real-time communication system.
Neural audio codecs can compress audio samples with a low bitrate due to the
strong modeling and generative capabilities of deep neural networks. To address
the poor high-frequency expression and high computational cost and storage
consumption, we proposed an integrated framework that utilizes a neural network
to model wide-band components and adopts traditional signal processing to
compress high-band components according to psychological hearing knowledge.
Inspired by auditory perception theory, a perception-based loss function is
designed to improve harmonic modeling. Besides, generative adversarial network
(GAN) compression is proposed for the first time for neural audio codecs. Our
method is superior to prior advanced neural codecs across subjective and
objective metrics and allows real-time inference on desktop and mobile.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial HuBERT: Self-supervised Spatial Speech Representation Learning
  for a Single Talker from Multi-channel Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoni Dimitriadis, Siqi Pan, Vidhyasaharan Sethu, Beena Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has been used to leverage unlabelled data, improving
accuracy and generalisation of speech systems through the training of
representation models. While many recent works have sought to produce effective
representations across a variety of acoustic domains, languages, modalities and
even simultaneous speakers, these studies have all been limited to
single-channel audio recordings. This paper presents Spatial HuBERT, a
self-supervised speech representation model that learns both acoustic and
spatial information pertaining to a single speaker in a potentially noisy
environment by using multi-channel audio inputs. Spatial HuBERT learns
representations that outperform state-of-the-art single-channel speech
representations on a variety of spatial downstream tasks, particularly in
reverberant and noisy environments. We also demonstrate the utility of the
representations learned by Spatial HuBERT on a speech localisation downstream
task. Along with this paper, we publicly release a new dataset of 100 000
simulated first-order ambisonics room impulse responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and
  Phonetic Domains for Speech Representation Learning <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noé Tits
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a methodology for linguistic feature extraction,
focusing particularly on automatically syllabifying words in multiple
languages, with a design to be compatible with a forced-alignment tool, the
Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our
method focuses on the extraction of phonetic transcriptions from text, stress
marks, and a unified automatic syllabification (in text and phonetic domains).
The system was built with open-source components and resources. Through an
ablation study, we demonstrate the efficacy of our approach in automatically
syllabifying words from several languages (English, French and Spanish).
Additionally, we apply the technique to the transcriptions of the CMU ARCTIC
dataset, generating valuable annotations available
online\footnote{\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for
speech representation learning, speech unit discovery, and disentanglement of
speech factors in several speech-related fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-stage Large Language Model Correction for Speech Recognition <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Pu, Thai-Son Nguyen, Sebastian Stüker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the usage of large language models (LLMs) to
improve the performance of competitive speech recognition systems. Different
from traditional language models that focus on one single data domain, the rise
of LLMs brings us the opportunity to push the limit of state-of-the-art ASR
performance, and at the same time to achieve higher robustness and generalize
effectively across multiple domains. Motivated by this, we propose a novel
multi-stage approach to combine traditional language model re-scoring and LLM
prompting. Specifically, the proposed method has two stages: the first stage
uses a language model to re-score an N-best list of ASR hypotheses and run a
confidence check; The second stage uses prompts to a LLM to perform ASR error
correction on less confident results from the first stage. Our experimental
results demonstrate the effectiveness of the proposed method by showing a 10% ~
20% relative improvement in WER over a competitive ASR system -- across
multiple test domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End real time tracking of children's reading with pointer network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishal Sunder, Beulah Karrolla, Eric Fosler-Lussier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we explore how a real time reading tracker can be built
efficiently for children's voices. While previously proposed reading trackers
focused on ASR-based cascaded approaches, we propose a fully end-to-end model
making it less prone to lags in voice tracking. We employ a pointer network
that directly learns to predict positions in the ground truth text conditioned
on the streaming speech. To train this pointer network, we generate ground
truth training signals by using forced alignment between the read speech and
the text being read on the training set. Exploring different forced alignment
models, we find a neural attention based model is at least as close in
alignment accuracy to the Montreal Forced Aligner, but surprisingly is a better
training signal for the pointer network. Our results are reported on one adult
speech data (TIMIT) and two children's speech datasets (CMU Kids and Reading
Races). Our best model can accurately track adult speech with 87.8% accuracy
and the much harder and disfluent children's speech with 77.1% accuracy on CMU
Kids data and a 65.3% accuracy on the Reading Races dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-AdapterFusion: A Task-ID-free Approach for Efficient and
  Non-Destructive Multi-task Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hillary Ngai, Rohan Agrawal, Neeraj Gaur, Ronny Huang, Parisa Haghani, Pedro Moreno Mengibar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapters are an efficient, composable alternative to full fine-tuning of
pre-trained models and help scale the deployment of large ASR models to many
tasks. In practice, a task ID is commonly prepended to the input during
inference to route to single-task adapters for the specified task. However, one
major limitation of this approach is that the task ID may not be known during
inference, rendering it unsuitable for most multi-task settings. To address
this, we propose three novel task-ID-free methods to combine single-task
adapters in multi-task ASR and investigate two learning algorithms for
training. We evaluate our methods on 10 test sets from 4 diverse ASR tasks and
show that our methods are non-destructive and parameter-efficient. While only
updating 17% of the model parameters, our methods can achieve an 8% mean WER
improvement relative to full fine-tuning and are on-par with task-ID adapter
routing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2023 IEEE Automatic Speech Recognition and Understanding Workshop
  (ASRU) Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative error correction for code-switching speech recognition using
  large language models <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Hexin Liu, Sabato Marco Siniscalchi, Eng Siong Chng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-switching (CS) speech refers to the phenomenon of mixing two or more
languages within the same sentence. Despite the recent advances in automatic
speech recognition (ASR), CS-ASR is still a challenging task ought to the
grammatical structure complexity of the phenomenon and the data scarcity of
specific training corpus. In this work, we propose to leverage large language
models (LLMs) and lists of hypotheses generated by an ASR to address the CS
problem. Specifically, we first employ multiple well-trained ASR models for
N-best hypotheses generation, with the aim of increasing the diverse and
informative elements in the set of hypotheses. Next, we utilize the LLMs to
learn the hypotheses-to-transcription (H2T) mapping by adding a trainable
low-rank adapter. Such a generative error correction (GER) method directly
predicts the accurate transcription according to its expert linguistic
knowledge and N-best hypotheses, resulting in a paradigm shift from the
traditional language model rescoring or error correction techniques.
Experimental evidence demonstrates that GER significantly enhances CS-ASR
accuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show
remarkable data efficiency for H2T learning, providing a potential solution to
the data scarcity problem of CS-ASR in low-resource languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocSelect: Target Speaker Localization with an Auditory Selective
  Hearing Mechanism <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Chen, Xinyuan Qian, Zexu Pan, Kainan Chen, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevailing noise-resistant and reverberation-resistant localization
algorithms primarily emphasize separating and providing directional output for
each speaker in multi-speaker scenarios, without association with the identity
of speakers. In this paper, we present a target speaker localization algorithm
with a selective hearing mechanism. Given a reference speech of the target
speaker, we first produce a speaker-dependent spectrogram mask to eliminate
interfering speakers' speech. Subsequently, a Long short-term memory (LSTM)
network is employed to extract the target speaker's location from the filtered
spectrogram. Experiments validate the superiority of our proposed method over
the existing algorithms for different scale invariant signal-to-noise ratios
(SNR) conditions. Specifically, at SNR = -10 dB, our proposed network LocSelect
achieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Contextual Recognition In Automatic Speech Recognition Systems
  By Semantic Lattice Rescoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09680v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09680v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankitha Sudarshan, Vinay Samuel, Parth Patwa, Ibtihel Amara, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) has witnessed a profound research
interest. Recent breakthroughs have given ASR systems different prospects such
as faithfully transcribing spoken language, which is a pivotal advancement in
building conversational agents. However, there is still an imminent challenge
of accurately discerning context-dependent words and phrases. In this work, we
propose a novel approach for enhancing contextual recognition within ASR
systems via semantic lattice processing leveraging the power of deep learning
models in accurately delivering spot-on transcriptions across a wide variety of
vocabularies and speaking styles. Our solution consists of using Hidden Markov
Models and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks
(DNN) models integrating both language and acoustic modeling for better
accuracy. We infused our network with the use of a transformer-based model to
properly rescore the word lattice achieving remarkable capabilities with a
palpable reduction in Word Error Rate (WER). We demonstrate the effectiveness
of our proposed framework on the LibriSpeech dataset with empirical analyses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2209.01250,
  arXiv:2301.06735 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Interpreter Understands Your Meaning: End-to-end Spoken Language
  Understanding Aided by Speech Translation <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mutian He, Philip N. Garner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end spoken language understanding (SLU) remains elusive even with
current large pretrained language models on text and speech, especially in
multilingual cases. Machine translation has been established as a powerful
pretraining objective on text as it enables the model to capture high-level
semantics of the input utterance and associations between different languages,
which is desired for speech models that work on lower-level acoustic frames.
Motivated particularly by the task of cross-lingual SLU, we demonstrate that
the task of speech translation (ST) is a good means of pretraining speech
models for end-to-end SLU on both intra- and cross-lingual scenarios.
  By introducing ST, our models reach higher performance over baselines on
monolingual and multilingual intent classification as well as spoken question
answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the
effectiveness of our methods, we also create new benchmark datasets from both
synthetic and real sources, for speech summarization and low-resource/zero-shot
transfer from English to French or Spanish. We further show the value of
preserving knowledge for the ST pretraining task for better downstream
performance, possibly using Bayesian transfer regularizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures; accepted by Findings of EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wav2vec-based Detection and Severity Level Classification of Dysarthria
  from Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Javanmardi, Saska Tirronen, Manila Kodali, Sudarsana Reddy Kadiri, Paavo Alku
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic detection and severity level classification of dysarthria directly
from acoustic speech signals can be used as a tool in medical diagnosis. In
this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor
to build detection and severity level classification systems for dysarthric
speech. The experiments were carried out with the popularly used UA-speech
database. In the detection experiments, the results revealed that the best
performance was obtained using the embeddings from the first layer of the
wav2vec model that yielded an absolute improvement of 1.23% in accuracy
compared to the best performing baseline feature (spectrogram). In the studied
severity level classification task, the results revealed that the embeddings
from the final layer gave an absolute improvement of 10.62% in accuracy
compared to the best baseline features (mel-frequency cepstral coefficients).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>copyright 2023 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analysis and Detection of Pathological Voice using Glottal Source
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sudarsana Reddy Kadiri, Paavo Alku
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic detection of voice pathology enables objective assessment and
earlier intervention for the diagnosis. This study provides a systematic
analysis of glottal source features and investigates their effectiveness in
voice pathology detection. Glottal source features are extracted using glottal
flows estimated with the quasi-closed phase (QCP) glottal inverse filtering
method, using approximate glottal source signals computed with the zero
frequency filtering (ZFF) method, and using acoustic voice signals directly. In
addition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from
the glottal source waveforms computed by QCP and ZFF to effectively capture the
variations in glottal source spectra of pathological voice. Experiments were
carried out using two databases, the Hospital Universitario Principe de
Asturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database.
Analysis of features revealed that the glottal source contains information that
discriminates normal and pathological voice. Pathology detection experiments
were carried out using support vector machine (SVM). From the detection
experiments it was observed that the performance achieved with the studied
glottal source features is comparable or better than that of conventional MFCCs
and perceptual linear prediction (PLP) features. The best detection performance
was achieved when the glottal source features were combined with the
conventional MFCCs and PLP features, which indicates the complementary nature
of the features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2020 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RVAE-EM: Generative speech dereverberation based on recurrent
  variational auto-encoder and convolutive transfer function <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyu Wang, Xiaofei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In indoor scenes, reverberation is a crucial factor in degrading the
perceived quality and intelligibility of speech. In this work, we propose a
generative dereverberation method. Our approach is based on a probabilistic
model utilizing a recurrent variational auto-encoder (RVAE) network and the
convolutive transfer function (CTF) approximation. Different from most previous
approaches, the output of our RVAE serves as the prior of the clean speech. And
our target is the maximum a posteriori (MAP) estimation of clean speech, which
is achieved iteratively through the expectation maximization (EM) algorithm.
The proposed method integrates the capabilities of network-based speech prior
modelling and CTF-based observation modelling. Experiments on single-channel
speech dereverberation show that the proposed generative method noticeably
outperforms the advanced discriminative networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using
  Spatial Transformer Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19130v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19130v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        László Tóth, Amin Honarmandi Shandiz, Gábor Gosztolya, Csapó Tamás Gábor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to the latest deep learning algorithms, silent speech interfaces (SSI)
are now able to synthesize intelligible speech from articulatory movement data
under certain conditions. However, the resulting models are rather
speaker-specific, making a quick switch between users troublesome. Even for the
same speaker, these models perform poorly cross-session, i.e. after dismounting
and re-mounting the recording equipment. To aid quick speaker and session
adaptation of ultrasound tongue imaging-based SSI models, we extend our deep
networks with a spatial transformer network (STN) module, capable of performing
an affine transformation on the input images. Although the STN part takes up
only about 10% of the network, our experiments show that adapting just the STN
module might allow to reduce MSE by 88% on the average, compared to retraining
the whole network. The improvement is even larger (around 92%) when adapting
the network to different recording sessions from the same speaker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HumTrans: A Novel Open-Source <span class="highlight-title">Dataset</span> for Humming Melody Transcription
  and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shansong Liu, Xu Li, Dian Li, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the HumTrans dataset, which is publicly available and
primarily designed for humming melody transcription. The dataset can also serve
as a foundation for downstream tasks such as humming melody based music
generation. It consists of 500 musical compositions of different genres and
languages, with each composition divided into multiple segments. In total, the
dataset comprises 1000 music segments. To collect this humming dataset, we
employed 10 college students, all of whom are either music majors or proficient
in playing at least one musical instrument. Each of them hummed every segment
twice using the web recording interface provided by our designed website. The
humming recordings were sampled at a frequency of 44,100 Hz. During the humming
session, the main interface provides a musical score for students to reference,
with the melody audio playing simultaneously to aid in capturing both melody
and rhythm. The dataset encompasses approximately 56.22 hours of audio, making
it the largest known humming dataset to date. The dataset will be released on
Hugging Face, and we will provide a GitHub repository containing baseline
results and evaluation codes.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-16T00:00:00Z">2023-10-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generation or Replication: Auscultating Audio Latent Diffusion Models <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Bralios, Gordon Wichern, François G. Germain, Zexu Pan, Sameer Khurana, Chiori Hori, Jonathan Le Roux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of audio latent diffusion models possessing the ability to
generate realistic sound clips on demand from a text description has the
potential to revolutionize how we work with audio. In this work, we make an
initial attempt at understanding the inner workings of audio latent diffusion
models by investigating how their audio outputs compare with the training data,
similar to how a doctor auscultates a patient by listening to the sounds of
their organs. Using text-to-audio latent diffusion models trained on the
AudioCaps dataset, we systematically analyze memorization behavior as a
function of training set size. We also evaluate different retrieval metrics for
evidence of training data memorization, finding the similarity between mel
spectrograms to be more robust in detecting matches than learned embedding
vectors. In the process of analyzing memorization in audio latent diffusion
models, we also discover a large amount of duplicated audio clips within the
AudioCaps database.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework
  for Music-Dance Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixing Yang, Xukun Zhou, Xulong Tang, Ran Diao, Hongyan Liu, Jun He, Zhaoxin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dance and music are closely related forms of expression, with mutual
retrieval between dance videos and music being a fundamental task in various
fields like education, art, and sports. However, existing methods often suffer
from unnatural generation effects or fail to fully explore the correlation
between music and dance. To overcome these challenges, we propose BeatDance, a
novel beat-based model-agnostic contrastive learning framework. BeatDance
incorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat
Blender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval
performance by utilizing the alignment between music beats and dance movements.
We also introduce the Music-Dance (MD) dataset, a large-scale collection of
over 10,000 music-dance video pairs for training and testing. Experimental
results on the MD dataset demonstrate the superiority of our method over
existing baselines, achieving state-of-the-art performance. The code and
dataset will be made public available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Audio Emotion and Intent Recognition with Large Pre-Trained
  Models and Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dejan Porjazovski, Yaroslav Getman, Tamás Grósz, Mikko Kurimo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pre-trained models are essential in paralinguistic systems,
demonstrating effectiveness in tasks like emotion recognition and stuttering
detection. In this paper, we employ large pre-trained models for the ACM
Multimedia Computational Paralinguistics Challenge, addressing the Requests and
Emotion Share tasks. We explore audio-only and hybrid solutions leveraging
audio and text modalities. Our empirical results consistently show the
superiority of the hybrid approaches over the audio-only models. Moreover, we
introduce a Bayesian layer as an alternative to the standard linear output
layer. The multimodal fusion approach achieves an 85.4% UAR on HC-Requests and
60.2% on HC-Complaints. The ensemble model for the Emotion Share task yields
the best rho value of .614. The Bayesian wav2vec2 approach, explored in this
study, allows us to easily build ensembles, at the cost of fine-tuning only one
model. Moreover, we can have usable confidence values instead of the usual
overconfident posterior probabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACMM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Music and Language Attention Models for Zero-shot Music Tagging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Du, Zhesong Yu, Jiaju Lin, Bilei Zhu, Qiuqiang Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music tagging is a task to predict the tags of music recordings. However,
previous music tagging research primarily focuses on close-set music tagging
tasks which can not be generalized to new tags. In this work, we propose a
zero-shot music tagging system modeled by a joint music and language attention
(JMLA) model to address the open-set music tagging problem. The JMLA model
consists of an audio encoder modeled by a pretrained masked autoencoder and a
decoder modeled by a Falcon7B. We introduce preceiver resampler to convert
arbitrary length audio into fixed length embeddings. We introduce dense
attention connections between encoder and decoder layers to improve the
information flow between the encoder and decoder layers. We collect a
large-scale music and description dataset from the internet. We propose to use
ChatGPT to convert the raw descriptions into formalized and diverse
descriptions to train the JMLA models. Our proposed JMLA system achieves a
zero-shot audio tagging accuracy of $ 64.82\% $ on the GTZAN dataset,
outperforming previous zero-shot systems and achieves comparable results to
previous systems on the FMA and the MagnaTagATune datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\begin{keywords} Music tagging, joint music and language attention
  models, Music Foundation Model. \end{keywords}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder
  and Input Feature Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Cui, Imran Ahamad Sheikh, Mostafa Sadeghi, Emmanuel Vincent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an end-to-end multichannel speaker-attributed automatic speech
recognition (MC-SA-ASR) system that combines a Conformer-based encoder with
multi-frame crosschannel attention and a speaker-attributed Transformer-based
decoder. To the best of our knowledge, this is the first model that efficiently
integrates ASR and speaker identification modules in a multichannel setting. On
simulated mixtures of LibriSpeech data, our system reduces the word error rate
(WER) by up to 12% and 16% relative compared to previously proposed
single-channel and multichannel approaches, respectively. Furthermore, we
investigate the impact of different input features, including multichannel
magnitude and phase information, on the ASR performance. Finally, our
experiments on the AMI corpus confirm the effectiveness of our system for
real-world multichannel meeting transcription.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2023 IEEE Automatic Speech Recognition and Understanding Workshop
  (ASRU 2023), Dec 2023, Taipei, Taiwan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Speech Enhancement and Separation with a Unified Deep Neural
  Network for Single/Dual Talker Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Patel, Anton Kovalyov, Issa Panahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a practical approach for leveraging a real-time deep
learning model to alternate between speech enhancement and joint speech
enhancement and separation depending on whether the input mixture contains one
or two active speakers. Scale-invariant signal-to-distortion ratio (SI-SDR) has
shown to be a highly effective training measure in time-domain speech
separation. However, the SI-SDR metric is ill-defined for zero-energy target
signals, which is a problem when training a speech separation model using
utterances with varying numbers of talkers. Unlike existing solutions that
focus on modifying the loss function to accommodate zero-energy target signals,
the proposed approach circumvents this problem by training the model to extract
speech on both its output channels regardless if the input is a single or
dual-talker mixture. A lightweight speaker overlap detection (SOD) module is
also introduced to differentiate between single and dual-talker segments in
real-time. The proposed module takes advantage of the new formulation by
operating directly on the separated masks, given by the separation model,
instead of the original mixture, thus effectively simplifying the detection
task. Experimental results show that the proposed training approach outperforms
existing solutions, and the SOD module exhibits high accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages, Accepted at IEEE Asilomar</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Lead Sheet Generation via Semantic Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Novack, Nikita Srivatsan, Taylor Berg-Kirkpatrick, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lead sheets have become commonplace in generative music research, being used
as an initial compressed representation for downstream tasks like multitrack
music generation and automatic arrangement. Despite this, researchers have
often fallen back on deterministic reduction methods (such as the skyline
algorithm) to generate lead sheets when seeking paired lead sheets and full
scores, with little attention being paid toward the quality of the lead sheets
themselves and how they accurately reflect their orchestrated counterparts. To
address these issues, we propose the problem of conditional lead sheet
generation (i.e. generating a lead sheet given its full score version), and
show that this task can be formulated as an unsupervised music compression
task, where the lead sheet represents a compressed latent version of the score.
We introduce a novel model, called Lead-AE, that models the lead sheets as a
discrete subselection of the original sequence, using a differentiable top-k
operator to allow for controllable local sparsity constraints. Across both
automatic proxy tasks and direct human evaluations, we find that our method
improves upon the established deterministic baseline and produces coherent
reductions of large multitrack scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimized Tokenization for Transcribed Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Wullach, Shlomo E. Chazan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenges facing speech recognition systems, such as variations in
pronunciations, adverse audio conditions, and the scarcity of labeled data,
emphasize the necessity for a post-processing step that corrects recurring
errors. Previous research has shown the advantages of employing dedicated error
correction models, yet training such models requires large amounts of labeled
data which is not easily obtained. To overcome this limitation, synthetic
transcribed-like data is often utilized, however, bridging the distribution gap
between transcribed errors and synthetic noise is not trivial. In this paper,
we demonstrate that the performance of correction models can be significantly
increased by training solely using synthetic data. Specifically, we empirically
show that: (1) synthetic data generated using the error distribution derived
from a set of transcribed data outperforms the common approach of applying
random perturbations; (2) applying language-specific adjustments to the
vocabulary of a BPE tokenizer strike a balance between adapting to unseen
distributions and retaining knowledge of transcribed errors. We showcase the
benefits of these key observations, and evaluate our approach using multiple
languages, speech recognition systems and prominent speech recognition
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving End-to-End Speech Processing by Efficient Text Data
  Utilization with Latent Synthesis <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqiao Lu, Wenyong Huang, Nianzu Zheng, Xingshan Zeng, Yu Ting Yeung, Xiao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a high performance end-to-end speech (E2E) processing model requires
an enormous amount of labeled speech data, especially in the era of
data-centric artificial intelligence. However, labeled speech data are usually
scarcer and more expensive for collection, compared to textual data. We propose
Latent Synthesis (LaSyn), an efficient textual data utilization framework for
E2E speech processing models. We train a latent synthesizer to convert textual
data into an intermediate latent representation of a pre-trained speech model.
These pseudo acoustic representations of textual data augment acoustic data for
model training. We evaluate LaSyn on low-resource automatic speech recognition
(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an
E2E baseline trained on LibriSpeech train-clean-100, with relative word error
rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our
E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for
slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)
and EM-Tree accuracies on STOP respectively. With fewer parameters, the results
of LaSyn are competitive to published state-of-the-art works. The results
demonstrate the quality of the augmented training data. The source code will be
available to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 8 tables, Accepted to EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene
  Synthesis <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02088v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02088v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can machines recording an audio-visual scene produce realistic, matching
audio-visual experiences at novel positions and novel view directions? We
answer it by studying a new task -- real-world audio-visual scene synthesis --
and a first-of-its-kind NeRF-based approach for multimodal learning.
Concretely, given a video recording of an audio-visual scene, the task is to
synthesize new videos with spatial audios along arbitrary novel camera
trajectories in that scene. We propose an acoustic-aware audio generation
module that integrates prior knowledge of audio propagation into NeRF, in which
we implicitly associate audio generation with the 3D geometry and material
properties of a visual environment. Furthermore, we present a coordinate
transformation module that expresses a view direction relative to the sound
source, enabling the model to learn sound source-centric acoustic fields. To
facilitate the study of this new task, we collect a high-quality Real-World
Audio-Visual Scene (RWAVS) dataset. We demonstrate the advantages of our method
on this real-world dataset and the simulation-based SoundSpaces dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyPoradise: An Open Baseline for Generative Speech Recognition with
  Large Language Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Sabato Macro Siniscalchi, Pin-Yu Chen, Eng Siong Chng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in deep neural networks have allowed automatic speech
recognition (ASR) systems to attain human parity on several publicly available
clean speech datasets. However, even state-of-the-art ASR systems experience
performance degradation when confronted with adverse conditions, as a
well-trained acoustic model is sensitive to variations in the speech domain,
e.g., background noise. Intuitively, humans address this issue by relying on
their linguistic knowledge: the meaning of ambiguous spoken terms is usually
inferred from contextual cues thereby reducing the dependency on the auditory
system. Inspired by this observation, we introduce the first open-source
benchmark to utilize external large language models (LLMs) for ASR error
correction, where N-best decoding hypotheses provide informative elements for
true transcription prediction. This approach is a paradigm shift from the
traditional language model rescoring strategy that can only select one
candidate hypothesis as the output transcription. The proposed benchmark
contains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairs
of N-best hypotheses and corresponding accurate transcriptions across prevalent
speech domains. Given this dataset, we examine three types of error correction
techniques based on LLMs with varying amounts of labeled
hypotheses-transcription pairs, which gains a significant word error rate (WER)
reduction. Experimental evidence demonstrates the proposed technique achieves a
breakthrough by surpassing the upper bound of traditional re-ranking based
methods. More surprisingly, LLM with reasonable prompt and its generative
capability can even correct those tokens that are missing in N-best list. We
make our results publicly accessible for reproducible pipelines with released
pre-trained models, thus providing a new evaluation paradigm for ASR error
correction with LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023, 24 pages. Datasets and Benchmarks Track.
  Added the first Mandarin and code-switching (zh-cn and en-us) results from
  the LLM-based generative ASR error correction to Table 8 on Page 21</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Iterative Refinement for Modular Source Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Bralios, Efthymios Tzinis, Gordon Wichern, Paris Smaragdis, Jonathan Le Roux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional source separation approaches train deep neural network models
end-to-end with all the data available at once by minimizing the empirical risk
on the whole training set. On the inference side, after training the model, the
user fetches a static computation graph and runs the full model on some
specified observed mixture signal to get the estimated source signals.
Additionally, many of those models consist of several basic processing blocks
which are applied sequentially. We argue that we can significantly increase
resource efficiency during both training and inference stages by reformulating
a model's training and inference procedures as iterative mappings of latent
signal representations. First, we can apply the same processing block more than
once on its output to refine the input signal and consequently improve
parameter efficiency. During training, we can follow a block-wise procedure
which enables a reduction on memory requirements. Thus, one can train a very
complicated network structure using significantly less computation compared to
end-to-end training. During inference, we can dynamically adjust how many
processing blocks and iterations of a specific block an input signal needs
using a gating module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Whispering LLaMA: A Cross-Modal Generative Error Correction Framework
  for Speech Recognition <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srijith Radhakrishnan, Chao-Han Huck Yang, Sumeer Ahmad Khan, Rohit Kumar, Narsis A. Kiani, David Gomez-Cabrero, Jesper N. Tegner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new cross-modal fusion technique designed for generative error
correction in automatic speech recognition (ASR). Our methodology leverages
both acoustic information and external linguistic representations to generate
accurate speech transcription contexts. This marks a step towards a fresh
paradigm in generative error correction within the realm of n-best hypotheses.
Unlike the existing ranking-based rescoring methods, our approach adeptly uses
distinct initialization techniques and parameter-efficient algorithms to boost
ASR performance derived from pre-trained speech and text models. Through
evaluation across diverse ASR datasets, we evaluate the stability and
reproducibility of our fusion technique, demonstrating its improved word error
rate relative (WERR) performance in comparison to n-best hypotheses by
relatively 37.66%. To encourage future research, we have made our code and
pre-trained models open source at
https://github.com/Srijith-rkr/Whispering-LLaMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023 as main paper. 10 pages. Revised math
  notations. GitHub: https://github.com/Srijith-rkr/Whispering-LLaMA</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generation or Replication: Auscultating Audio Latent Diffusion Models <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Bralios, Gordon Wichern, François G. Germain, Zexu Pan, Sameer Khurana, Chiori Hori, Jonathan Le Roux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of audio latent diffusion models possessing the ability to
generate realistic sound clips on demand from a text description has the
potential to revolutionize how we work with audio. In this work, we make an
initial attempt at understanding the inner workings of audio latent diffusion
models by investigating how their audio outputs compare with the training data,
similar to how a doctor auscultates a patient by listening to the sounds of
their organs. Using text-to-audio latent diffusion models trained on the
AudioCaps dataset, we systematically analyze memorization behavior as a
function of training set size. We also evaluate different retrieval metrics for
evidence of training data memorization, finding the similarity between mel
spectrograms to be more robust in detecting matches than learned embedding
vectors. In the process of analyzing memorization in audio latent diffusion
models, we also discover a large amount of duplicated audio clips within the
AudioCaps database.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework
  for Music-Dance Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixing Yang, Xukun Zhou, Xulong Tang, Ran Diao, Hongyan Liu, Jun He, Zhaoxin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dance and music are closely related forms of expression, with mutual
retrieval between dance videos and music being a fundamental task in various
fields like education, art, and sports. However, existing methods often suffer
from unnatural generation effects or fail to fully explore the correlation
between music and dance. To overcome these challenges, we propose BeatDance, a
novel beat-based model-agnostic contrastive learning framework. BeatDance
incorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat
Blender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval
performance by utilizing the alignment between music beats and dance movements.
We also introduce the Music-Dance (MD) dataset, a large-scale collection of
over 10,000 music-dance video pairs for training and testing. Experimental
results on the MD dataset demonstrate the superiority of our method over
existing baselines, achieving state-of-the-art performance. The code and
dataset will be made public available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Audio Emotion and Intent Recognition with Large Pre-Trained
  Models and Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dejan Porjazovski, Yaroslav Getman, Tamás Grósz, Mikko Kurimo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pre-trained models are essential in paralinguistic systems,
demonstrating effectiveness in tasks like emotion recognition and stuttering
detection. In this paper, we employ large pre-trained models for the ACM
Multimedia Computational Paralinguistics Challenge, addressing the Requests and
Emotion Share tasks. We explore audio-only and hybrid solutions leveraging
audio and text modalities. Our empirical results consistently show the
superiority of the hybrid approaches over the audio-only models. Moreover, we
introduce a Bayesian layer as an alternative to the standard linear output
layer. The multimodal fusion approach achieves an 85.4% UAR on HC-Requests and
60.2% on HC-Complaints. The ensemble model for the Emotion Share task yields
the best rho value of .614. The Bayesian wav2vec2 approach, explored in this
study, allows us to easily build ensembles, at the cost of fine-tuning only one
model. Moreover, we can have usable confidence values instead of the usual
overconfident posterior probabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACMM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Music and Language Attention Models for Zero-shot Music Tagging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Du, Zhesong Yu, Jiaju Lin, Bilei Zhu, Qiuqiang Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music tagging is a task to predict the tags of music recordings. However,
previous music tagging research primarily focuses on close-set music tagging
tasks which can not be generalized to new tags. In this work, we propose a
zero-shot music tagging system modeled by a joint music and language attention
(JMLA) model to address the open-set music tagging problem. The JMLA model
consists of an audio encoder modeled by a pretrained masked autoencoder and a
decoder modeled by a Falcon7B. We introduce preceiver resampler to convert
arbitrary length audio into fixed length embeddings. We introduce dense
attention connections between encoder and decoder layers to improve the
information flow between the encoder and decoder layers. We collect a
large-scale music and description dataset from the internet. We propose to use
ChatGPT to convert the raw descriptions into formalized and diverse
descriptions to train the JMLA models. Our proposed JMLA system achieves a
zero-shot audio tagging accuracy of $ 64.82\% $ on the GTZAN dataset,
outperforming previous zero-shot systems and achieves comparable results to
previous systems on the FMA and the MagnaTagATune datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\begin{keywords} Music tagging, joint music and language attention
  models, Music Foundation Model. \end{keywords}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder
  and Input Feature Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Cui, Imran Ahamad Sheikh, Mostafa Sadeghi, Emmanuel Vincent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an end-to-end multichannel speaker-attributed automatic speech
recognition (MC-SA-ASR) system that combines a Conformer-based encoder with
multi-frame crosschannel attention and a speaker-attributed Transformer-based
decoder. To the best of our knowledge, this is the first model that efficiently
integrates ASR and speaker identification modules in a multichannel setting. On
simulated mixtures of LibriSpeech data, our system reduces the word error rate
(WER) by up to 12% and 16% relative compared to previously proposed
single-channel and multichannel approaches, respectively. Furthermore, we
investigate the impact of different input features, including multichannel
magnitude and phase information, on the ASR performance. Finally, our
experiments on the AMI corpus confirm the effectiveness of our system for
real-world multichannel meeting transcription.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2023 IEEE Automatic Speech Recognition and Understanding Workshop
  (ASRU 2023), Dec 2023, Taipei, Taiwan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Speech Enhancement and Separation with a Unified Deep Neural
  Network for Single/Dual Talker Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Patel, Anton Kovalyov, Issa Panahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a practical approach for leveraging a real-time deep
learning model to alternate between speech enhancement and joint speech
enhancement and separation depending on whether the input mixture contains one
or two active speakers. Scale-invariant signal-to-distortion ratio (SI-SDR) has
shown to be a highly effective training measure in time-domain speech
separation. However, the SI-SDR metric is ill-defined for zero-energy target
signals, which is a problem when training a speech separation model using
utterances with varying numbers of talkers. Unlike existing solutions that
focus on modifying the loss function to accommodate zero-energy target signals,
the proposed approach circumvents this problem by training the model to extract
speech on both its output channels regardless if the input is a single or
dual-talker mixture. A lightweight speaker overlap detection (SOD) module is
also introduced to differentiate between single and dual-talker segments in
real-time. The proposed module takes advantage of the new formulation by
operating directly on the separated masks, given by the separation model,
instead of the original mixture, thus effectively simplifying the detection
task. Experimental results show that the proposed training approach outperforms
existing solutions, and the SOD module exhibits high accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages, Accepted at IEEE Asilomar</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheol Jun Cho, Abdelrahman Mohamed, Shang-Wen Li, Alan W Black, Gopala K. Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven unit discovery in self-supervised learning (SSL) of speech has
embarked on a new era of spoken language processing. Yet, the discovered units
often remain in phonetic space, limiting the utility of SSL representations.
Here, we demonstrate that a syllabic organization emerges in learning
sentence-level representation of speech. In particular, we adopt
"self-distillation" objective to fine-tune the pretrained HuBERT with an
aggregator token that summarizes the entire sentence. Without any supervision,
the resulting model draws definite boundaries in speech, and the
representations across frames show salient syllabic structures. We demonstrate
that this emergent structure largely corresponds to the ground truth syllables.
Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating
sentence-level representation of speech. When compared to previous models, our
model outperforms in both unsupervised syllable discovery and learning
sentence-level representation. Together, we demonstrate that the
self-distillation of HuBERT gives rise to syllabic organization without relying
on external labels or modalities, and potentially provides novel data-driven
units for spoken language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Models of Speech Infer Universal Articulatory Kinematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheol Jun Cho, Abdelrahman Mohamed, Alan W Black, Gopala K. Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Supervised Learning (SSL) based models of speech have shown remarkable
performance on a range of downstream tasks. These state-of-the-art models have
remained blackboxes, but many recent studies have begun "probing" models like
HuBERT, to correlate their internal representations to different aspects of
speech. In this paper, we show "inference of articulatory kinematics" as
fundamental property of SSL models, i.e., the ability of these models to
transform acoustics into the causal articulatory dynamics underlying the speech
signal. We also show that this abstraction is largely overlapping across the
language of the data used to train the model, with preference to the language
with similar phonological system. Furthermore, we show that with simple affine
transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable
across speakers, even across genders, languages, and dialects, showing the
generalizability of this property. Together, these results shed new light on
the internals of SSL models that are critical to their superior performance,
and open up new avenues into language-agnostic universal models for speech
engineering, that are interpretable and grounded in speech science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Lead Sheet Generation via Semantic Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Novack, Nikita Srivatsan, Taylor Berg-Kirkpatrick, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lead sheets have become commonplace in generative music research, being used
as an initial compressed representation for downstream tasks like multitrack
music generation and automatic arrangement. Despite this, researchers have
often fallen back on deterministic reduction methods (such as the skyline
algorithm) to generate lead sheets when seeking paired lead sheets and full
scores, with little attention being paid toward the quality of the lead sheets
themselves and how they accurately reflect their orchestrated counterparts. To
address these issues, we propose the problem of conditional lead sheet
generation (i.e. generating a lead sheet given its full score version), and
show that this task can be formulated as an unsupervised music compression
task, where the lead sheet represents a compressed latent version of the score.
We introduce a novel model, called Lead-AE, that models the lead sheets as a
discrete subselection of the original sequence, using a differentiable top-k
operator to allow for controllable local sparsity constraints. Across both
automatic proxy tasks and direct human evaluations, we find that our method
improves upon the established deterministic baseline and produces coherent
reductions of large multitrack scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimized Tokenization for Transcribed Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Wullach, Shlomo E. Chazan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenges facing speech recognition systems, such as variations in
pronunciations, adverse audio conditions, and the scarcity of labeled data,
emphasize the necessity for a post-processing step that corrects recurring
errors. Previous research has shown the advantages of employing dedicated error
correction models, yet training such models requires large amounts of labeled
data which is not easily obtained. To overcome this limitation, synthetic
transcribed-like data is often utilized, however, bridging the distribution gap
between transcribed errors and synthetic noise is not trivial. In this paper,
we demonstrate that the performance of correction models can be significantly
increased by training solely using synthetic data. Specifically, we empirically
show that: (1) synthetic data generated using the error distribution derived
from a set of transcribed data outperforms the common approach of applying
random perturbations; (2) applying language-specific adjustments to the
vocabulary of a BPE tokenizer strike a balance between adapting to unseen
distributions and retaining knowledge of transcribed errors. We showcase the
benefits of these key observations, and evaluate our approach using multiple
languages, speech recognition systems and prominent speech recognition
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Speech Abnormalities with a Perceiver-based Sequence
  Classifier that Leverages a Universal Speech Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hagen Soltau, Izhak Shafran, Alex Ottenwess, Joseph R. JR Duffy, Rene L. Utianski, Leland R. Barnard, John L. Stricker, Daniela Wiepert, David T. Jones, Hugo Botha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a Perceiver-based sequence classifier to detect abnormalities in
speech reflective of several neurological disorders. We combine this classifier
with a Universal Speech Model (USM) that is trained (unsupervised) on 12
million hours of diverse audio recordings. Our model compresses long sequences
into a small set of class-specific latent representations and a factorized
projection is used to predict different attributes of the disordered input
speech. The benefit of our approach is that it allows us to model different
regions of the input for different classes and is at the same time data
efficient. We evaluated the proposed model extensively on a curated corpus from
the Mayo Clinic. Our model outperforms standard transformer (80.9%) and
perceiver (81.8%) models and achieves an average accuracy of 83.1%. With
limited task-specific data, we find that pretraining is important and
surprisingly pretraining with the unrelated automatic speech recognition (ASR)
task is also beneficial. Encodings from the middle layers provide a mix of both
acoustic and phonetic information and achieve best prediction results compared
to just using the final layer encodings (83.1% vs. 79.6%). The results are
promising and with further refinements may help clinicians detect speech
abnormalities without needing access to highly specialized speech-language
pathologists.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving End-to-End Speech Processing by Efficient Text Data
  Utilization with Latent Synthesis <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqiao Lu, Wenyong Huang, Nianzu Zheng, Xingshan Zeng, Yu Ting Yeung, Xiao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a high performance end-to-end speech (E2E) processing model requires
an enormous amount of labeled speech data, especially in the era of
data-centric artificial intelligence. However, labeled speech data are usually
scarcer and more expensive for collection, compared to textual data. We propose
Latent Synthesis (LaSyn), an efficient textual data utilization framework for
E2E speech processing models. We train a latent synthesizer to convert textual
data into an intermediate latent representation of a pre-trained speech model.
These pseudo acoustic representations of textual data augment acoustic data for
model training. We evaluate LaSyn on low-resource automatic speech recognition
(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an
E2E baseline trained on LibriSpeech train-clean-100, with relative word error
rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our
E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for
slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)
and EM-Tree accuracies on STOP respectively. With fewer parameters, the results
of LaSyn are competitive to published state-of-the-art works. The results
demonstrate the quality of the augmented training data. The source code will be
available to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 8 tables, Accepted to EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene
  Synthesis <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02088v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02088v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can machines recording an audio-visual scene produce realistic, matching
audio-visual experiences at novel positions and novel view directions? We
answer it by studying a new task -- real-world audio-visual scene synthesis --
and a first-of-its-kind NeRF-based approach for multimodal learning.
Concretely, given a video recording of an audio-visual scene, the task is to
synthesize new videos with spatial audios along arbitrary novel camera
trajectories in that scene. We propose an acoustic-aware audio generation
module that integrates prior knowledge of audio propagation into NeRF, in which
we implicitly associate audio generation with the 3D geometry and material
properties of a visual environment. Furthermore, we present a coordinate
transformation module that expresses a view direction relative to the sound
source, enabling the model to learn sound source-centric acoustic fields. To
facilitate the study of this new task, we collect a high-quality Real-World
Audio-Visual Scene (RWAVS) dataset. We demonstrate the advantages of our method
on this real-world dataset and the simulation-based SoundSpaces dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyPoradise: An Open Baseline for Generative Speech Recognition with
  Large Language Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Sabato Macro Siniscalchi, Pin-Yu Chen, Eng Siong Chng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in deep neural networks have allowed automatic speech
recognition (ASR) systems to attain human parity on several publicly available
clean speech datasets. However, even state-of-the-art ASR systems experience
performance degradation when confronted with adverse conditions, as a
well-trained acoustic model is sensitive to variations in the speech domain,
e.g., background noise. Intuitively, humans address this issue by relying on
their linguistic knowledge: the meaning of ambiguous spoken terms is usually
inferred from contextual cues thereby reducing the dependency on the auditory
system. Inspired by this observation, we introduce the first open-source
benchmark to utilize external large language models (LLMs) for ASR error
correction, where N-best decoding hypotheses provide informative elements for
true transcription prediction. This approach is a paradigm shift from the
traditional language model rescoring strategy that can only select one
candidate hypothesis as the output transcription. The proposed benchmark
contains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairs
of N-best hypotheses and corresponding accurate transcriptions across prevalent
speech domains. Given this dataset, we examine three types of error correction
techniques based on LLMs with varying amounts of labeled
hypotheses-transcription pairs, which gains a significant word error rate (WER)
reduction. Experimental evidence demonstrates the proposed technique achieves a
breakthrough by surpassing the upper bound of traditional re-ranking based
methods. More surprisingly, LLM with reasonable prompt and its generative
capability can even correct those tokens that are missing in N-best list. We
make our results publicly accessible for reproducible pipelines with released
pre-trained models, thus providing a new evaluation paradigm for ASR error
correction with LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023, 24 pages. Datasets and Benchmarks Track.
  Added the first Mandarin and code-switching (zh-cn and en-us) results from
  the LLM-based generative ASR error correction to Table 8 on Page 21</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Gap in Visual Speech Recognition Between Normal and Silent
  Speech Based on Metric Learning <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Kashiwagi, Keitaro Tanaka, Qi Feng, Shigeo Morishima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel metric learning approach to address the
performance gap between normal and silent speech in visual speech recognition
(VSR). The difference in lip movements between the two poses a challenge for
existing VSR models, which exhibit degraded accuracy when applied to silent
speech. To solve this issue and tackle the scarcity of training data for silent
speech, we propose to leverage the shared literal content between normal and
silent speech and present a metric learning approach based on visemes.
Specifically, we aim to map the input of two speech types close to each other
in a latent space if they have similar viseme representations. By minimizing
the Kullback-Leibler divergence of the predicted viseme probability
distributions between and within the two speech types, our model effectively
learns and predicts viseme identities. Our evaluation demonstrates that our
method improves the accuracy of silent VSR, even when limited training data is
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Iterative Refinement for Modular Source Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Bralios, Efthymios Tzinis, Gordon Wichern, Paris Smaragdis, Jonathan Le Roux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional source separation approaches train deep neural network models
end-to-end with all the data available at once by minimizing the empirical risk
on the whole training set. On the inference side, after training the model, the
user fetches a static computation graph and runs the full model on some
specified observed mixture signal to get the estimated source signals.
Additionally, many of those models consist of several basic processing blocks
which are applied sequentially. We argue that we can significantly increase
resource efficiency during both training and inference stages by reformulating
a model's training and inference procedures as iterative mappings of latent
signal representations. First, we can apply the same processing block more than
once on its output to refine the input signal and consequently improve
parameter efficiency. During training, we can follow a block-wise procedure
which enables a reduction on memory requirements. Thus, one can train a very
complicated network structure using significantly less computation compared to
end-to-end training. During inference, we can dynamically adjust how many
processing blocks and iterations of a specific block an input signal needs
using a gating module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Whispering LLaMA: A Cross-Modal Generative Error Correction Framework
  for Speech Recognition <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srijith Radhakrishnan, Chao-Han Huck Yang, Sumeer Ahmad Khan, Rohit Kumar, Narsis A. Kiani, David Gomez-Cabrero, Jesper N. Tegner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new cross-modal fusion technique designed for generative error
correction in automatic speech recognition (ASR). Our methodology leverages
both acoustic information and external linguistic representations to generate
accurate speech transcription contexts. This marks a step towards a fresh
paradigm in generative error correction within the realm of n-best hypotheses.
Unlike the existing ranking-based rescoring methods, our approach adeptly uses
distinct initialization techniques and parameter-efficient algorithms to boost
ASR performance derived from pre-trained speech and text models. Through
evaluation across diverse ASR datasets, we evaluate the stability and
reproducibility of our fusion technique, demonstrating its improved word error
rate relative (WERR) performance in comparison to n-best hypotheses by
relatively 37.66%. To encourage future research, we have made our code and
pre-trained models open source at
https://github.com/Srijith-rkr/Whispering-LLaMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023 as main paper. 10 pages. Revised math
  notations. GitHub: https://github.com/Srijith-rkr/Whispering-LLaMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manifold-Aware Deep Clustering: Maximizing Angles between Embedding
  Vectors Based on Regular Simplex 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.02331v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.02331v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keitaro Tanaka, Ryosuke Sawata, Shusuke Takahashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new deep clustering (DC) method called manifold-aware
DC (M-DC) that can enhance hyperspace utilization more effectively than the
original DC. The original DC has a limitation in that a pair of two speakers
has to be embedded having an orthogonal relationship due to its use of the
one-hot vector-based loss function, while our method derives a unique loss
function aimed at maximizing the target angle in the hyperspace based on the
nature of a regular simplex. Our proposed loss imposes a higher penalty than
the original DC when the speaker is assigned incorrectly. The change from DC to
M-DC can be easily achieved by rewriting just one term in the loss function of
DC, without any other modifications to the network architecture or model
parameters. As such, our method has high practicability because it does not
affect the original inference part. The experimental results show that the
proposed method improves the performances of the original DC and its expansion
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2021</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-15T00:00:00Z">2023-10-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MERTech: Instrument Playing Technique Detection Using Self-Supervised
  Pretrained Model With Multi-Task Finetuning <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dichucheng Li, Yinghao Ma, Weixing Wei, Qiuqiang Kong, Yulun Wu, Mingjin Che, Fan Xia, Emmanouil Benetos, Wei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instrument playing techniques (IPTs) constitute a pivotal component of
musical expression. However, the development of automatic IPT detection methods
suffers from limited labeled data and inherent class imbalance issues. In this
paper, we propose to apply a self-supervised learning model pre-trained on
large-scale unlabeled music data and finetune it on IPT detection tasks. This
approach addresses data scarcity and class imbalance challenges. Recognizing
the significance of pitch in capturing the nuances of IPTs and the importance
of onset in locating IPT events, we investigate multi-task finetuning with
pitch and onset detection as auxiliary tasks. Additionally, we apply a
post-processing approach for event-level prediction, where an IPT activation
initiates an event only if the onset output confirms an onset in that frame.
Our method outperforms prior approaches in both frame-level and event-level
metrics across multiple IPT benchmark datasets. Further experiments demonstrate
the efficacy of multi-task finetuning on each IPT class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoCoFormer: A controllable feature-rich polyphonic music generation
  method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuyang Zhou, Tengfei Niu, Hong Zhu, Xingping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the modeling method of polyphonic music sequence. Due to
the great potential of Transformer models in music generation, controllable
music generation is receiving more attention. In the task of polyphonic music,
current controllable generation research focuses on controlling the generation
of chords, but lacks precise adjustment for the controllable generation of
choral music textures. This paper proposed Condition Choir Transformer
(CoCoFormer) which controls the output of the model by controlling the chord
and rhythm inputs at a fine-grained level. In this paper, the self-supervised
method improves the loss function and performs joint training through
conditional control input and unconditional input training. In order to
alleviate the lack of diversity on generated samples caused by the teacher
forcing training, this paper added an adversarial training method. CoCoFormer
enhances model performance with explicit and implicit inputs to chords and
rhythms. In this paper, the experiments proves that CoCoFormer has reached the
current better level than current models. On the premise of specifying the
polyphonic music texture, the same melody can also be generated in a variety of
ways.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Typing to Listen at the Cocktail Party: Text-Guided Target Speaker
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07284v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07284v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hao, Jibin Wu, Jianwei Yu, Chenglin Xu, Kay Chen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess an extraordinary ability to selectively focus on the sound
source of interest amidst complex acoustic environments, commonly referred to
as cocktail party scenarios. In an attempt to replicate this remarkable
auditory attention capability in machines, target speaker extraction (TSE)
models have been developed. These models leverage the pre-registered cues of
the target speaker to extract the sound source of interest. However, the
effectiveness of these models is hindered in real-world scenarios due to the
unreliable or even absence of pre-registered cues. To address this limitation,
this study investigates the integration of natural language description to
enhance the feasibility, controllability, and performance of existing TSE
models. Specifically, we propose a model named LLM-TSE, wherein a large
language model (LLM) extracts useful semantic cues from the user's typed text
input. These cues can serve as independent extraction cues, task selectors to
control the TSE process or complement the pre-registered cues. Our experimental
results demonstrate competitive performance when only text-based cues are
presented, the effectiveness of using input text as a task selector, and a new
state-of-the-art when combining text-based cues with pre-registered cues. To
our knowledge, this is the first study to successfully incorporate LLMs to
guide target speaker extraction, which can be a cornerstone for cocktail party
problem research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, https://github.com/haoxiangsnr/llm-tse</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MERTech: Instrument Playing Technique Detection Using Self-Supervised
  Pretrained Model With Multi-Task Finetuning <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dichucheng Li, Yinghao Ma, Weixing Wei, Qiuqiang Kong, Yulun Wu, Mingjin Che, Fan Xia, Emmanouil Benetos, Wei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instrument playing techniques (IPTs) constitute a pivotal component of
musical expression. However, the development of automatic IPT detection methods
suffers from limited labeled data and inherent class imbalance issues. In this
paper, we propose to apply a self-supervised learning model pre-trained on
large-scale unlabeled music data and finetune it on IPT detection tasks. This
approach addresses data scarcity and class imbalance challenges. Recognizing
the significance of pitch in capturing the nuances of IPTs and the importance
of onset in locating IPT events, we investigate multi-task finetuning with
pitch and onset detection as auxiliary tasks. Additionally, we apply a
post-processing approach for event-level prediction, where an IPT activation
initiates an event only if the onset output confirms an onset in that frame.
Our method outperforms prior approaches in both frame-level and event-level
metrics across multiple IPT benchmark datasets. Further experiments demonstrate
the efficacy of multi-task finetuning on each IPT class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoCoFormer: A controllable feature-rich polyphonic music generation
  method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuyang Zhou, Tengfei Niu, Hong Zhu, Xingping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the modeling method of polyphonic music sequence. Due to
the great potential of Transformer models in music generation, controllable
music generation is receiving more attention. In the task of polyphonic music,
current controllable generation research focuses on controlling the generation
of chords, but lacks precise adjustment for the controllable generation of
choral music textures. This paper proposed Condition Choir Transformer
(CoCoFormer) which controls the output of the model by controlling the chord
and rhythm inputs at a fine-grained level. In this paper, the self-supervised
method improves the loss function and performs joint training through
conditional control input and unconditional input training. In order to
alleviate the lack of diversity on generated samples caused by the teacher
forcing training, this paper added an adversarial training method. CoCoFormer
enhances model performance with explicit and implicit inputs to chords and
rhythms. In this paper, the experiments proves that CoCoFormer has reached the
current better level than current models. On the premise of specifying the
polyphonic music texture, the same melody can also be generated in a variety of
ways.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-14T00:00:00Z">2023-10-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfVC: <span class="highlight-title">Voice Conversion</span> With Iterative Refinement using Self
  Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paarth Neekhara, Shehzeen Hussain, Rafael Valle, Boris Ginsburg, Rishabh Ranjan, Shlomo Dubnov, Farinaz Koushanfar, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SelfVC, a training strategy to iteratively improve a voice
conversion model with self-synthesized examples. Previous efforts on voice
conversion focus on explicitly disentangling speech representations to
separately encode speaker characteristics and linguistic content. However,
disentangling speech representations to capture such attributes using
task-specific loss terms can lead to information loss by discarding finer
nuances of the original signal. In this work, instead of explicitly
disentangling attributes with loss terms, we present a framework to train a
controllable voice conversion model on entangled speech representations derived
from self-supervised learning and speaker verification models. First, we
develop techniques to derive prosodic information from the audio signal and SSL
representations to train predictive submodules in the synthesis model. Next, we
propose a training strategy to iteratively improve the synthesis model for
voice conversion, by creating a challenging training objective using
self-synthesized examples. In this training approach, the current state of the
synthesis model is used to generate voice-converted variations of an utterance,
which serve as inputs for the reconstruction task, ensuring a continuous and
purposeful refinement of the model. We demonstrate that incorporating such
self-synthesized examples during training improves the speaker similarity of
generated speech as compared to a baseline voice conversion model trained
solely on heuristically perturbed inputs. SelfVC is trained without any text
and is applicable to a range of tasks such as zero-shot voice conversion,
cross-lingual voice conversion, and controllable speech synthesis with pitch
and pace modifications. SelfVC achieves state-of-the-art results in zero-shot
voice conversion on metrics evaluating naturalness, speaker similarity, and
intelligibility of synthesized audio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Prediction of Full-Ocean Depth SSP by Hierarchical LSTM: An
  Experimental Result 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Lu, Wei Huang, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SSP distribution is an important parameter for underwater positioning,
navigation and timing (PNT) because it affects the propagation mode of
underwater acoustic signals. To accurate predict future sound speed
distribution, we propose a hierarchical long short--term memory (H--LSTM)
neural network for future sound speed prediction, which explore the
distribution pattern of sound velocity in the time dimension. To verify the
feasibility and effectiveness, we conducted both simulations and real
experiments. The ocean experiment was held in the South China Sea in April,
2023. Results show that the accuracy of the proposed method outperforms the
state--of--the--art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Test-Time Adaptation for Acoustic Foundation Models in
  Open-World Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongfu Liu, Hengguan Huang, Ye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-Time Adaptation (TTA) is a critical paradigm for tackling distribution
shifts during inference, especially in visual recognition tasks. However, while
acoustic models face similar challenges due to distribution shifts in test-time
speech, TTA techniques specifically designed for acoustic modeling in the
context of open-world data shifts remain scarce. This gap is further
exacerbated when considering the unique characteristics of acoustic foundation
models: 1) they are primarily built on transformer architectures with layer
normalization and 2) they deal with test-time speech data of varying lengths in
a non-stationary manner. These aspects make the direct application of
vision-focused TTA methods, which are mostly reliant on batch normalization and
assume independent samples, infeasible. In this paper, we delve into TTA for
pre-trained acoustic models facing open-world data shifts. We find that noisy,
high-entropy speech frames, often non-silent, carry key semantic content.
Traditional TTA methods might inadvertently filter out this information using
potentially flawed heuristics. In response, we introduce a heuristic-free,
learning-based adaptation enriched by confidence enhancement. Noting that
speech signals' short-term consistency, we also apply consistency
regularization during test-time optimization. Our experiments on synthetic and
real-world datasets affirm our method's superiority over existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A small vocabulary database of ultrasound image sequences of vocal tract
  dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margareth Castillo, Felipe Rubio, Dagoberto Porras, Sonia H. Contreras-Ortiz, Alexander Sepúlveda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new database consisting of concurrent articulatory and
acoustic speech data. The articulatory data correspond to ultrasound videos of
the vocal tract dynamics, which allow the visualization of the tongue upper
contour during the speech production process. Acoustic data is composed of 30
short sentences that were acquired by a directional cardioid microphone. This
database includes data from 17 young subjects (8 male and 9 female) from the
Santander region in Colombia, who reported not having any speech pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text
  Translation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Le, Yao Qian, Long Zhou, Shujie Liu, Yanmin Qian, Michael Zeng, Xuedong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint speech-language training is challenging due to the large demand for
training data and GPU consumption, as well as the modality gap between speech
and language. We present ComSL, a speech-language model built atop a composite
architecture of public pretrained speech-only and language-only models and
optimized data-efficiently for spoken language tasks. Particularly, we propose
to incorporate cross-modality learning into transfer learning and conduct them
simultaneously for downstream tasks in a multi-task learning manner. Our
approach has demonstrated effectiveness in end-to-end speech-to-text
translation tasks, achieving a new state-of-the-art average BLEU score of 31.5
on the multilingual speech to English text translation task for 21 languages,
as measured on the public CoVoST2 evaluation set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023, Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Voice-preserving Zero-shot Multiple Accent Conversion <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mumin Jin, Prashant Serai, Jilong Wu, Andros Tjandra, Vimal Manohar, Qing He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most people who have tried to learn a foreign language would have experienced
difficulties understanding or speaking with a native speaker's accent. For
native speakers, understanding or speaking a new accent is likewise a difficult
task. An accent conversion system that changes a speaker's accent but preserves
that speaker's voice identity, such as timbre and pitch, has the potential for
a range of applications, such as communication, language learning, and
entertainment. Existing accent conversion models tend to change the speaker
identity and accent at the same time. Here, we use adversarial learning to
disentangle accent dependent features while retaining other acoustic
characteristics. What sets our work apart from existing accent conversion
models is the capability to convert an unseen speaker's utterance to multiple
accents while preserving its original voice identity. Subjective evaluations
show that our model generates audio that sound closer to the target accent and
like the original speaker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfVC: <span class="highlight-title">Voice Conversion</span> With Iterative Refinement using Self
  Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paarth Neekhara, Shehzeen Hussain, Rafael Valle, Boris Ginsburg, Rishabh Ranjan, Shlomo Dubnov, Farinaz Koushanfar, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SelfVC, a training strategy to iteratively improve a voice
conversion model with self-synthesized examples. Previous efforts on voice
conversion focus on explicitly disentangling speech representations to
separately encode speaker characteristics and linguistic content. However,
disentangling speech representations to capture such attributes using
task-specific loss terms can lead to information loss by discarding finer
nuances of the original signal. In this work, instead of explicitly
disentangling attributes with loss terms, we present a framework to train a
controllable voice conversion model on entangled speech representations derived
from self-supervised learning and speaker verification models. First, we
develop techniques to derive prosodic information from the audio signal and SSL
representations to train predictive submodules in the synthesis model. Next, we
propose a training strategy to iteratively improve the synthesis model for
voice conversion, by creating a challenging training objective using
self-synthesized examples. In this training approach, the current state of the
synthesis model is used to generate voice-converted variations of an utterance,
which serve as inputs for the reconstruction task, ensuring a continuous and
purposeful refinement of the model. We demonstrate that incorporating such
self-synthesized examples during training improves the speaker similarity of
generated speech as compared to a baseline voice conversion model trained
solely on heuristically perturbed inputs. SelfVC is trained without any text
and is applicable to a range of tasks such as zero-shot voice conversion,
cross-lingual voice conversion, and controllable speech synthesis with pitch
and pace modifications. SelfVC achieves state-of-the-art results in zero-shot
voice conversion on metrics evaluating naturalness, speaker similarity, and
intelligibility of synthesized audio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Prediction of Full-Ocean Depth SSP by Hierarchical LSTM: An
  Experimental Result 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Lu, Wei Huang, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SSP distribution is an important parameter for underwater positioning,
navigation and timing (PNT) because it affects the propagation mode of
underwater acoustic signals. To accurate predict future sound speed
distribution, we propose a hierarchical long short--term memory (H--LSTM)
neural network for future sound speed prediction, which explore the
distribution pattern of sound velocity in the time dimension. To verify the
feasibility and effectiveness, we conducted both simulations and real
experiments. The ocean experiment was held in the South China Sea in April,
2023. Results show that the accuracy of the proposed method outperforms the
state--of--the--art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Test-Time Adaptation for Acoustic Foundation Models in
  Open-World Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongfu Liu, Hengguan Huang, Ye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-Time Adaptation (TTA) is a critical paradigm for tackling distribution
shifts during inference, especially in visual recognition tasks. However, while
acoustic models face similar challenges due to distribution shifts in test-time
speech, TTA techniques specifically designed for acoustic modeling in the
context of open-world data shifts remain scarce. This gap is further
exacerbated when considering the unique characteristics of acoustic foundation
models: 1) they are primarily built on transformer architectures with layer
normalization and 2) they deal with test-time speech data of varying lengths in
a non-stationary manner. These aspects make the direct application of
vision-focused TTA methods, which are mostly reliant on batch normalization and
assume independent samples, infeasible. In this paper, we delve into TTA for
pre-trained acoustic models facing open-world data shifts. We find that noisy,
high-entropy speech frames, often non-silent, carry key semantic content.
Traditional TTA methods might inadvertently filter out this information using
potentially flawed heuristics. In response, we introduce a heuristic-free,
learning-based adaptation enriched by confidence enhancement. Noting that
speech signals' short-term consistency, we also apply consistency
regularization during test-time optimization. Our experiments on synthetic and
real-world datasets affirm our method's superiority over existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A small vocabulary database of ultrasound image sequences of vocal tract
  dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margareth Castillo, Felipe Rubio, Dagoberto Porras, Sonia H. Contreras-Ortiz, Alexander Sepúlveda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new database consisting of concurrent articulatory and
acoustic speech data. The articulatory data correspond to ultrasound videos of
the vocal tract dynamics, which allow the visualization of the tongue upper
contour during the speech production process. Acoustic data is composed of 30
short sentences that were acquired by a directional cardioid microphone. This
database includes data from 17 young subjects (8 male and 9 female) from the
Santander region in Colombia, who reported not having any speech pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text
  Translation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Le, Yao Qian, Long Zhou, Shujie Liu, Yanmin Qian, Michael Zeng, Xuedong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint speech-language training is challenging due to the large demand for
training data and GPU consumption, as well as the modality gap between speech
and language. We present ComSL, a speech-language model built atop a composite
architecture of public pretrained speech-only and language-only models and
optimized data-efficiently for spoken language tasks. Particularly, we propose
to incorporate cross-modality learning into transfer learning and conduct them
simultaneously for downstream tasks in a multi-task learning manner. Our
approach has demonstrated effectiveness in end-to-end speech-to-text
translation tasks, achieving a new state-of-the-art average BLEU score of 31.5
on the multilingual speech to English text translation task for 21 languages,
as measured on the public CoVoST2 evaluation set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023, Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Voice-preserving Zero-shot Multiple Accent Conversion <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mumin Jin, Prashant Serai, Jilong Wu, Andros Tjandra, Vimal Manohar, Qing He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most people who have tried to learn a foreign language would have experienced
difficulties understanding or speaking with a native speaker's accent. For
native speakers, understanding or speaking a new accent is likewise a difficult
task. An accent conversion system that changes a speaker's accent but preserves
that speaker's voice identity, such as timbre and pitch, has the potential for
a range of applications, such as communication, language learning, and
entertainment. Existing accent conversion models tend to change the speaker
identity and accent at the same time. Here, we use adversarial learning to
disentangle accent dependent features while retaining other acoustic
characteristics. What sets our work apart from existing accent conversion
models is the capability to convert an unseen speaker's utterance to multiple
accents while preserving its original voice identity. Subjective evaluations
show that our model generates audio that sound closer to the target accent and
like the original speaker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-13T00:00:00Z">2023-10-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-latency Speech Enhancement via Speech Token Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaying Xue, Xiulian Peng, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing deep learning based speech enhancement mainly employ a data-driven
approach, which leverage large amounts of data with a variety of noise types to
achieve noise removal from noisy signal. However, the high dependence on the
data limits its generalization on the unseen complex noises in real-life
environment. In this paper, we focus on the low-latency scenario and regard
speech enhancement as a speech generation problem conditioned on the noisy
signal, where we generate clean speech instead of identifying and removing
noises. Specifically, we propose a conditional generative framework for speech
enhancement, which models clean speech by acoustic codes of a neural speech
codec and generates the speech codes conditioned on past noisy frames in an
auto-regressive way. Moreover, we propose an explicit-alignment approach to
align noisy frames with the generated speech tokens to improve the robustness
and scalability to different input lengths. Different from other methods that
leverage multiple stages to generate speech codes, we leverage a single-stage
speech generation approach based on the TF-Codec neural codec to achieve high
speech quality with low latency. Extensive results on both synthetic and
real-recorded test set show its superiority over data-driven approaches in
terms of noise robustness and temporal speech coherence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformer-based Autoencoder with ID Constraint for Unsupervised
  Anomalous Sound Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Guan, Youde Liu, Qiuqiang Kong, Feiyang Xiao, Qiaoxi Zhu, Jiantong Tian, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised anomalous sound detection (ASD) aims to detect unknown anomalous
sounds of devices when only normal sound data is available. The autoencoder
(AE) and self-supervised learning based methods are two mainstream methods.
However, the AE-based methods could be limited as the feature learned from
normal sounds can also fit with anomalous sounds, reducing the ability of the
model in detecting anomalies from sound. The self-supervised methods are not
always stable and perform differently, even for machines of the same type. In
addition, the anomalous sound may be short-lived, making it even harder to
distinguish from normal sound. This paper proposes an ID constrained
Transformer-based autoencoder (IDC-TransAE) architecture with weighted anomaly
score computation for unsupervised ASD. Machine ID is employed to constrain the
latent space of the Transformer-based autoencoder (TransAE) by introducing a
simple ID classifier to learn the difference in the distribution for the same
machine type and enhance the ability of the model in distinguishing anomalous
sound. Moreover, weighted anomaly score computation is introduced to highlight
the anomaly scores of anomalous events that only appear for a short time.
Experiments performed on DCASE 2020 Challenge Task2 development dataset
demonstrate the effectiveness and superiority of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EURASIP Journal on Audio, Speech, and Music Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential Evolution Algorithm based Hyper-Parameters Selection of
  Convolutional Neural Network for Speech Command Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandipan Dhar, Anuvab Sen, Aritra Bandyopadhyay, Nanda Dulal Jana, Arjun Ghosh, Zahra Sarayloo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech Command Recognition (SCR), which deals with identification of short
uttered speech commands, is crucial for various applications, including IoT
devices and assistive technology. Despite the promise shown by Convolutional
Neural Networks (CNNs) in SCR tasks, their efficacy relies heavily on
hyper-parameter selection, which is typically laborious and time-consuming when
done manually. This paper introduces a hyper-parameter selection method for
CNNs based on the Differential Evolution (DE) algorithm, aiming to enhance
performance in SCR tasks. Training and testing with the Google Speech Command
(GSC) dataset, the proposed approach showed effectiveness in classifying speech
commands. Moreover, a comparative analysis with Genetic Algorithm based
selections and other deep CNN (DCNN) models highlighted the efficiency of the
proposed DE algorithm in hyper-parameter selection for CNNs in SCR tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 7 Figures, 4 Tables, Accepted by the 15th International
  Joint Conference on Computational Intelligence (IJCCI 2023), November 13-15,
  2023, Rome, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Behave Like Clean Speech: Dual-Branch Knowledge Distillation
  for Noise-Robust Fake Audio Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cunhang Fan, Mingming Ding, Jianhua Tao, Ruibo Fu, Jiangyan Yi, Zhengqi Wen, Zhao Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most research in fake audio detection (FAD) focuses on improving performance
on standard noise-free datasets. However, in actual situations, there is
usually noise interference, which will cause significant performance
degradation in FAD systems. To improve the noise robustness, we propose a
dual-branch knowledge distillation fake audio detection (DKDFAD) method.
Specifically, a parallel data flow of the clean teacher branch and the noisy
student branch is designed, and interactive fusion and response-based
teacher-student paradigms are proposed to guide the training of noisy data from
the data distribution and decision-making perspectives. In the noise branch,
speech enhancement is first introduced for denoising, which reduces the
interference of strong noise. The proposed interactive fusion combines
denoising features and noise features to reduce the impact of speech distortion
and seek consistency with the data distribution of clean branch. The
teacher-student paradigm maps the student's decision space to the teacher's
decision space, making noisy speech behave as clean. In addition, a joint
training method is used to optimize the two branches to achieve global
optimality. Experimental results based on multiple datasets show that the
proposed method performs well in noisy environments and maintains performance
in cross-dataset experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SALM: Speech-augmented Language Model with In-context Learning for
  Speech Recognition and Translation <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehuai Chen, He Huang, Andrei Andrusenko, Oleksii Hrinchuk, Krishna C. Puvvada, Jason Li, Subhankar Ghosh, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel Speech Augmented Language Model (SALM) with {\em
multitask} and {\em in-context} learning capabilities. SALM comprises a frozen
text LLM, a audio encoder, a modality adapter module, and LoRA layers to
accommodate speech input and associated task instructions. The unified SALM not
only achieves performance on par with task-specific Conformer baselines for
Automatic Speech Recognition (ASR) and Speech Translation (AST), but also
exhibits zero-shot in-context learning capabilities, demonstrated through
keyword-boosting task for ASR and AST. Moreover, {\em speech supervised
in-context training} is proposed to bridge the gap between LLM training and
downstream speech tasks, which further boosts the in-context learning ability
of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submit to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORN: Co-Trained Full-Reference And No-Reference Audio Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranay Manocha, Donald Williamson, Adam Finkelstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual evaluation constitutes a crucial aspect of various
audio-processing tasks. Full reference (FR) or similarity-based metrics rely on
high-quality reference recordings, to which lower-quality or corrupted versions
of the recording may be compared for evaluation. In contrast, no-reference (NR)
metrics evaluate a recording without relying on a reference. Both the FR and NR
approaches exhibit advantages and drawbacks relative to each other. In this
paper, we present a novel framework called CORN that amalgamates these dual
approaches, concurrently training both FR and NR models together. After
training, the models can be applied independently. We evaluate CORN by
predicting several common objective metrics and across two different
architectures. The NR model trained using CORN has access to a reference
recording during training, and thus, as one would expect, it consistently
outperforms baseline NR models trained independently. Perhaps even more
remarkable is that the CORN FR model also outperforms its baseline counterpart,
even though it relies on the same training data and the same model
architecture. Thus, a single training regime produces two independently useful
models, each outperforming independently trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soundify: Matching Sound Effects to Video <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09726v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09726v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chuan-En Lin, Anastasis Germanidis, Cristóbal Valenzuela, Yining Shi, Nikolas Martelaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the art of video editing, sound helps add character to an object and
immerse the viewer within a space. Through formative interviews with
professional editors (N=10), we found that the task of adding sounds to video
can be challenging. This paper presents Soundify, a system that assists editors
in matching sounds to video. Given a video, Soundify identifies matching
sounds, synchronizes the sounds to the video, and dynamically adjusts panning
and volume to create spatial audio. In a human evaluation study (N=889), we
show that Soundify is capable of matching sounds to video out-of-the-box for a
diverse range of audio categories. In a within-subjects expert study (N=12), we
demonstrate the usefulness of Soundify in helping video editors match sounds to
video with lighter workload, reduced task completion time, and improved
usability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;
  Online demo: http://soundify.cc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A cry for help: Early detection of brain injury in newborns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles C. Onu, Samantha Latremouille, Arsenii Gorin, Junhao Wang, Uchenna Ekwochi, Peter O. Ubuane, Omolara A. Kehinde, Muhammad A. Salisu, Datonye Briggs, <span class="highlight-author">Yoshua Bengio</span>, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the 1960s, neonatal clinicians have known that newborns suffering from
certain neurological conditions exhibit altered crying patterns such as the
high-pitched cry in birth asphyxia. Despite an annual burden of over 1.5
million infant deaths and disabilities, early detection of neonatal brain
injuries due to asphyxia remains a challenge, particularly in developing
countries where the majority of births are not attended by a trained physician.
Here we report on the first inter-continental clinical study to demonstrate
that neonatal brain injury can be reliably determined from recorded infant
cries using an AI algorithm we call Roseline. Previous and recent work has been
limited by the lack of a large, high-quality clinical database of cry
recordings, constraining the application of state-of-the-art machine learning.
We develop a new training methodology for audio-based pathology detection
models and evaluate this system on a large database of newborn cry sounds
acquired from geographically diverse settings -- 5 hospitals across 3
continents. Our system extracts interpretable acoustic biomarkers that support
clinical decisions and is able to accurately detect neurological injury from
newborns' cries with an AUC of 92.5% (88.7% sensitivity at 80% specificity).
Cry-based neurological monitoring opens the door for low-cost, easy-to-use,
non-invasive and contact-free screening of at-risk babies, especially when
integrated into simple devices like smartphones or neonatal ICU monitors. This
would provide a reliable tool where there are no alternatives, but also curtail
the need to regularly exert newborns to physically-exhausting or
radiation-exposing assessments such as brain CT scans. This work sets the stage
for embracing the infant cry as a vital sign and indicates the potential of
AI-driven sound monitoring for the future of affordable healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-latency Speech Enhancement via Speech Token Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaying Xue, Xiulian Peng, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing deep learning based speech enhancement mainly employ a data-driven
approach, which leverage large amounts of data with a variety of noise types to
achieve noise removal from noisy signal. However, the high dependence on the
data limits its generalization on the unseen complex noises in real-life
environment. In this paper, we focus on the low-latency scenario and regard
speech enhancement as a speech generation problem conditioned on the noisy
signal, where we generate clean speech instead of identifying and removing
noises. Specifically, we propose a conditional generative framework for speech
enhancement, which models clean speech by acoustic codes of a neural speech
codec and generates the speech codes conditioned on past noisy frames in an
auto-regressive way. Moreover, we propose an explicit-alignment approach to
align noisy frames with the generated speech tokens to improve the robustness
and scalability to different input lengths. Different from other methods that
leverage multiple stages to generate speech codes, we leverage a single-stage
speech generation approach based on the TF-Codec neural codec to achieve high
speech quality with low latency. Extensive results on both synthetic and
real-recorded test set show its superiority over data-driven approaches in
terms of noise robustness and temporal speech coherence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformer-based Autoencoder with ID Constraint for Unsupervised
  Anomalous Sound Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Guan, Youde Liu, Qiuqiang Kong, Feiyang Xiao, Qiaoxi Zhu, Jiantong Tian, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised anomalous sound detection (ASD) aims to detect unknown anomalous
sounds of devices when only normal sound data is available. The autoencoder
(AE) and self-supervised learning based methods are two mainstream methods.
However, the AE-based methods could be limited as the feature learned from
normal sounds can also fit with anomalous sounds, reducing the ability of the
model in detecting anomalies from sound. The self-supervised methods are not
always stable and perform differently, even for machines of the same type. In
addition, the anomalous sound may be short-lived, making it even harder to
distinguish from normal sound. This paper proposes an ID constrained
Transformer-based autoencoder (IDC-TransAE) architecture with weighted anomaly
score computation for unsupervised ASD. Machine ID is employed to constrain the
latent space of the Transformer-based autoencoder (TransAE) by introducing a
simple ID classifier to learn the difference in the distribution for the same
machine type and enhance the ability of the model in distinguishing anomalous
sound. Moreover, weighted anomaly score computation is introduced to highlight
the anomaly scores of anomalous events that only appear for a short time.
Experiments performed on DCASE 2020 Challenge Task2 development dataset
demonstrate the effectiveness and superiority of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EURASIP Journal on Audio, Speech, and Music Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential Evolution Algorithm based Hyper-Parameters Selection of
  Convolutional Neural Network for Speech Command Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandipan Dhar, Anuvab Sen, Aritra Bandyopadhyay, Nanda Dulal Jana, Arjun Ghosh, Zahra Sarayloo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech Command Recognition (SCR), which deals with identification of short
uttered speech commands, is crucial for various applications, including IoT
devices and assistive technology. Despite the promise shown by Convolutional
Neural Networks (CNNs) in SCR tasks, their efficacy relies heavily on
hyper-parameter selection, which is typically laborious and time-consuming when
done manually. This paper introduces a hyper-parameter selection method for
CNNs based on the Differential Evolution (DE) algorithm, aiming to enhance
performance in SCR tasks. Training and testing with the Google Speech Command
(GSC) dataset, the proposed approach showed effectiveness in classifying speech
commands. Moreover, a comparative analysis with Genetic Algorithm based
selections and other deep CNN (DCNN) models highlighted the efficiency of the
proposed DE algorithm in hyper-parameter selection for CNNs in SCR tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 7 Figures, 4 Tables, Accepted by the 15th International
  Joint Conference on Computational Intelligence (IJCCI 2023), November 13-15,
  2023, Rome, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Behave Like Clean Speech: Dual-Branch Knowledge Distillation
  for Noise-Robust Fake Audio Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cunhang Fan, Mingming Ding, Jianhua Tao, Ruibo Fu, Jiangyan Yi, Zhengqi Wen, Zhao Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most research in fake audio detection (FAD) focuses on improving performance
on standard noise-free datasets. However, in actual situations, there is
usually noise interference, which will cause significant performance
degradation in FAD systems. To improve the noise robustness, we propose a
dual-branch knowledge distillation fake audio detection (DKDFAD) method.
Specifically, a parallel data flow of the clean teacher branch and the noisy
student branch is designed, and interactive fusion and response-based
teacher-student paradigms are proposed to guide the training of noisy data from
the data distribution and decision-making perspectives. In the noise branch,
speech enhancement is first introduced for denoising, which reduces the
interference of strong noise. The proposed interactive fusion combines
denoising features and noise features to reduce the impact of speech distortion
and seek consistency with the data distribution of clean branch. The
teacher-student paradigm maps the student's decision space to the teacher's
decision space, making noisy speech behave as clean. In addition, a joint
training method is used to optimize the two branches to achieve global
optimality. Experimental results based on multiple datasets show that the
proposed method performs well in noisy environments and maintains performance
in cross-dataset experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speaking rate attention-based duration prediction for speed control TTS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesuraj Bandekar, Sathvik Udupa, Abhayjeet Singh, Anjali Jayakumar, Deekshitha G, Sandhya Badiger, Saurabh Kumar, Pooja VH, Prasanta Kumar Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of high-quality speech synthesis, there is a lot of interest
in controlling various prosodic attributes of speech. Speaking rate is an
essential attribute towards modelling the expressivity of speech. In this work,
we propose a novel approach to control the speaking rate for non-autoregressive
TTS. We achieve this by conditioning the speaking rate inside the duration
predictor, allowing implicit speaking rate control. We show the benefits of
this approach by synthesising audio at various speaking rate factors and
measuring the quality of speaking rate-controlled synthesised speech. Further,
we study the effect of the speaking rate distribution of the training data
towards effective rate control. Finally, we fine-tune a baseline pretrained TTS
model to obtain speaking rate control TTS. We provide various analyses to
showcase the benefits of using this proposed approach, along with objective as
well as subjective metrics. We find that the proposed methods have higher
subjective scores and lower speaker rate errors across many speaking rate
factors over the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 20XX IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SALM: Speech-augmented Language Model with In-context Learning for
  Speech Recognition and Translation <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehuai Chen, He Huang, Andrei Andrusenko, Oleksii Hrinchuk, Krishna C. Puvvada, Jason Li, Subhankar Ghosh, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel Speech Augmented Language Model (SALM) with {\em
multitask} and {\em in-context} learning capabilities. SALM comprises a frozen
text LLM, a audio encoder, a modality adapter module, and LoRA layers to
accommodate speech input and associated task instructions. The unified SALM not
only achieves performance on par with task-specific Conformer baselines for
Automatic Speech Recognition (ASR) and Speech Translation (AST), but also
exhibits zero-shot in-context learning capabilities, demonstrated through
keyword-boosting task for ASR and AST. Moreover, {\em speech supervised
in-context training} is proposed to bridge the gap between LLM training and
downstream speech tasks, which further boosts the in-context learning ability
of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submit to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Protecting Voice-Controlled Devices against LASER Injection Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hashim Ali, Dhimant Khuttan, Rafi Ud Daula Refat, Hafiz Malik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice-Controllable Devices (VCDs) have seen an increasing trend towards their
adoption due to the small form factor of the MEMS microphones and their easy
integration into modern gadgets. Recent studies have revealed that MEMS
microphones are vulnerable to audio-modulated laser injection attacks. This
paper aims to develop countermeasures to detect and prevent laser injection
attacks on MEMS microphones. A time-frequency decomposition based on discrete
wavelet transform (DWT) is employed to decompose microphone output audio signal
into n + 1 frequency subbands to capture photo-acoustic related artifacts.
Higher-order statistical features consisting of the first four moments of
subband audio signals, e.g., variance, skew, and kurtosis are used to
distinguish between acoustic and photo-acoustic responses. An SVM classifier is
used to learn the underlying model that differentiates between an acoustic- and
laser-induced (photo-acoustic) response in the MEMS microphone. The proposed
framework is evaluated on a data set of 190 audios, consisting of 19 speakers.
The experimental results indicate that the proposed framework is able to
correctly classify $98\%$ of the acoustic- and laser-induced audio in a random
data partition setting and $100\%$ of the audio in speaker-independent and
text-independent data partition settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORN: Co-Trained Full-Reference And No-Reference Audio Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranay Manocha, Donald Williamson, Adam Finkelstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual evaluation constitutes a crucial aspect of various
audio-processing tasks. Full reference (FR) or similarity-based metrics rely on
high-quality reference recordings, to which lower-quality or corrupted versions
of the recording may be compared for evaluation. In contrast, no-reference (NR)
metrics evaluate a recording without relying on a reference. Both the FR and NR
approaches exhibit advantages and drawbacks relative to each other. In this
paper, we present a novel framework called CORN that amalgamates these dual
approaches, concurrently training both FR and NR models together. After
training, the models can be applied independently. We evaluate CORN by
predicting several common objective metrics and across two different
architectures. The NR model trained using CORN has access to a reference
recording during training, and thus, as one would expect, it consistently
outperforms baseline NR models trained independently. Perhaps even more
remarkable is that the CORN FR model also outperforms its baseline counterpart,
even though it relies on the same training data and the same model
architecture. Thus, a single training regime produces two independently useful
models, each outperforming independently trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soundify: Matching Sound Effects to Video <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09726v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09726v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chuan-En Lin, Anastasis Germanidis, Cristóbal Valenzuela, Yining Shi, Nikolas Martelaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the art of video editing, sound helps add character to an object and
immerse the viewer within a space. Through formative interviews with
professional editors (N=10), we found that the task of adding sounds to video
can be challenging. This paper presents Soundify, a system that assists editors
in matching sounds to video. Given a video, Soundify identifies matching
sounds, synchronizes the sounds to the video, and dynamically adjusts panning
and volume to create spatial audio. In a human evaluation study (N=889), we
show that Soundify is capable of matching sounds to video out-of-the-box for a
diverse range of audio categories. In a within-subjects expert study (N=12), we
demonstrate the usefulness of Soundify in helping video editors match sounds to
video with lighter workload, reduced task completion time, and improved
usability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;
  Online demo: http://soundify.cc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A cry for help: Early detection of brain injury in newborns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles C. Onu, Samantha Latremouille, Arsenii Gorin, Junhao Wang, Uchenna Ekwochi, Peter O. Ubuane, Omolara A. Kehinde, Muhammad A. Salisu, Datonye Briggs, <span class="highlight-author">Yoshua Bengio</span>, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the 1960s, neonatal clinicians have known that newborns suffering from
certain neurological conditions exhibit altered crying patterns such as the
high-pitched cry in birth asphyxia. Despite an annual burden of over 1.5
million infant deaths and disabilities, early detection of neonatal brain
injuries due to asphyxia remains a challenge, particularly in developing
countries where the majority of births are not attended by a trained physician.
Here we report on the first inter-continental clinical study to demonstrate
that neonatal brain injury can be reliably determined from recorded infant
cries using an AI algorithm we call Roseline. Previous and recent work has been
limited by the lack of a large, high-quality clinical database of cry
recordings, constraining the application of state-of-the-art machine learning.
We develop a new training methodology for audio-based pathology detection
models and evaluate this system on a large database of newborn cry sounds
acquired from geographically diverse settings -- 5 hospitals across 3
continents. Our system extracts interpretable acoustic biomarkers that support
clinical decisions and is able to accurately detect neurological injury from
newborns' cries with an AUC of 92.5% (88.7% sensitivity at 80% specificity).
Cry-based neurological monitoring opens the door for low-cost, easy-to-use,
non-invasive and contact-free screening of at-risk babies, especially when
integrated into simple devices like smartphones or neonatal ICU monitors. This
would provide a reliable tool where there are no alternatives, but also curtail
the need to regularly exert newborns to physically-exhausting or
radiation-exposing assessments such as brain CT scans. This work sets the stage
for embracing the infant cry as a vital sign and indicates the potential of
AI-driven sound monitoring for the future of affordable healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ICASSP 2023 Speech Signal Improvement Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06566v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06566v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Cutler, Ando Saabas, Babak Naderi, Nicolae-Cătălin Ristea, Sebastian Braun, Solomiya Branets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ICASSP 2023 Speech Signal Improvement Challenge is intended to stimulate
research in the area of improving the speech signal quality in communication
systems. The speech signal quality can be measured with SIG in ITU-T P.835 and
is still a top issue in audio communication and conferencing systems. For
example, in the ICASSP 2022 Deep Noise Suppression challenge, the improvement
in the background and overall quality is impressive, but the improvement in the
speech signal is not statistically significant. To improve the speech signal
the following speech impairment areas must be addressed: coloration,
discontinuity, loudness, reverberation, and noise. A training and test set was
provided for the challenge, and the winners were determined using an extended
crowdsourced implementation of ITU-T P.804's listening phase. The results show
significant improvement was made across all measured dimensions of speech
quality.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-12T00:00:00Z">2023-10-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Sound <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of time and note duration tokenizations on deep learning symbolic
  music modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Fradet, Nicolas Gutowski, Fabien Chhel, Jean-Pierre Briot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic music is widely used in various deep learning tasks, including
generation, transcription, synthesis, and Music Information Retrieval (MIR). It
is mostly employed with discrete models like Transformers, which require music
to be tokenized, i.e., formatted into sequences of distinct elements called
tokens. Tokenization can be performed in different ways. As Transformer can
struggle at reasoning, but capture more easily explicit information, it is
important to study how the way the information is represented for such model
impact their performances. In this work, we analyze the common tokenization
methods and experiment with time and note duration representations. We compare
the performances of these two impactful criteria on several tasks, including
composer and emotion classification, music generation, and sequence
representation learning. We demonstrate that explicit information leads to
better results depending on the task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowdsourced and Automatic Speech Prominence Estimation <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Morrison, Pranav Pawar, Nathan Pruyne, Jennifer Cole, Bryan Pardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prominence of a spoken word is the degree to which an average native
listener perceives the word as salient or emphasized relative to its context.
Speech prominence estimation is the process of assigning a numeric value to the
prominence of each word in an utterance. These prominence labels are useful for
linguistic analysis, as well as training automated systems to perform
emphasis-controlled text-to-speech or emotion recognition. Manually annotating
prominence is time-consuming and expensive, which motivates the development of
automated methods for speech prominence estimation. However, developing such an
automated system using machine-learning methods requires human-annotated
training data. Using our system for acquiring such human annotations, we
collect and open-source crowdsourced annotations of a portion of the LibriTTS
dataset. We use these annotations as ground truth to train a neural speech
prominence estimator that generalizes to unseen speakers, datasets, and
speaking styles. We investigate design decisions for neural prominence
estimation as well as how neural prominence estimation improves as a function
of two key factors of annotation cost: dataset size and the number of
annotations per utterance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A cry for help: Early detection of brain injury in newborns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles C. Onu, Samantha Latremouille, Arsenii Gorin, Junhao Wang, Uchenna Ekwochi, Peter O. Ubuane, Omolara A. Kehinde, Muhammad A. Salisu, Datonye Briggs, <span class="highlight-author">Yoshua Bengio</span>, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the 1960s, neonatal clinicians have known that newborns suffering from
certain neurological conditions exhibit altered crying patterns such as the
high-pitched cry in birth asphyxia. Despite an annual burden of over 1.5
million infant deaths and disabilities, early detection of neonatal brain
injuries due to asphyxia remains a challenge, particularly in developing
countries where the majority of births are not attended by a trained physician.
Here we report on the first inter-continental clinical study to demonstrate
that neonatal brain injury can be reliably determined from recorded infant
cries using an AI algorithm we call Roseline. Previous and recent work has been
limited by the lack of a large, high-quality clinical database of cry
recordings, constraining the application of state-of-the-art machine learning.
We develop a new training methodology for audio-based pathology detection
models and evaluate this system on a large database of newborn cry sounds
acquired from geographically diverse settings -- 5 hospitals across 3
continents. Our system extracts interpretable acoustic biomarkers that support
clinical decisions and is able to accurately detect neurological injury from
newborns' cries with an AUC of 92.5% (88.7% sensitivity at 80% specificity).
Cry-based neurological monitoring opens the door for low-cost, easy-to-use,
non-invasive and contact-free screening of at-risk babies, especially when
integrated into simple devices like smartphones or neonatal ICU monitors. This
would provide a reliable tool where there are no alternatives, but also curtail
the need to regularly exert newborns to physically-exhausting or
radiation-exposing assessments such as brain CT scans. This work sets the stage
for embracing the infant cry as a vital sign and indicates the potential of
AI-driven sound monitoring for the future of affordable healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Variational Auto-encoder based Audio-Visual Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Mao, Jing Zhang, Mochu Xiang, Yiran Zhong, Yuchao Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an Explicit Conditional Multimodal Variational Auto-Encoder
(ECMVAE) for audio-visual segmentation (AVS), aiming to segment sound sources
in the video sequence. Existing AVS methods focus on implicit feature fusion
strategies, where models are trained to fit the discrete samples in the
dataset. With a limited and less diverse dataset, the resulting performance is
usually unsatisfactory. In contrast, we address this problem from an effective
representation learning perspective, aiming to model the contribution of each
modality explicitly. Specifically, we find that audio contains critical
category information of the sound producers, and visual data provides candidate
sound producer(s). Their shared information corresponds to the target sound
producer(s) shown in the visual data. In this case, cross-modal shared
representation learning is especially important for AVS. To achieve this, our
ECMVAE factorizes the representations of each modality with a modality-shared
representation and a modality-specific representation. An orthogonality
constraint is applied between the shared and specific representations to
maintain the exclusive attribute of the factorized latent code. Further, a
mutual information maximization regularizer is introduced to achieve extensive
exploration of each modality. Quantitative and qualitative evaluations on the
AVSBench demonstrate the effectiveness of our approach, leading to a new
state-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging
MS3 subset for multiple sound source segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2023,Project
  page(https://npucvr.github.io/MMVAE-AVS),Code(https://github.com/OpenNLPLab/MMVAE-AVS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A Single Speech Enhancement Model Unifying Dereverberation, Denoising,
  Speaker Counting, Separation, and Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kohei Saijo, Wangyou Zhang, Zhong-Qiu Wang, <span class="highlight-author">Shinji Watanabe</span>, Tetsunori Kobayashi, Tetsuji Ogawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a multi-task universal speech enhancement (MUSE) model that can
perform five speech enhancement (SE) tasks: dereverberation, denoising, speech
separation (SS), target speaker extraction (TSE), and speaker counting. This is
achieved by integrating two modules into an SE model: 1) an internal separation
module that does both speaker counting and separation; and 2) a TSE module that
extracts the target speech from the internal separation outputs using target
speaker cues. The model is trained to perform TSE if the target speaker cue is
given and SS otherwise. By training the model to remove noise and
reverberation, we allow the model to tackle the five tasks mentioned above with
a single model, which has not been accomplished yet. Evaluation results
demonstrate that the proposed MUSE model can successfully handle multiple tasks
with a single model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 2 tables, accepted by ASRU2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Word Error Rate Estimation Using Self-Supervised Representations
  For Speech And Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanho Park, Chengsong Lu, Mingjie Chen, Thomas Hain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of automatic speech recognition (ASR) is typically measured by
word error rate (WER). WER estimation is a task aiming to predict the WER of an
ASR system, given a speech utterance and a transcription. This task has gained
increasing attention while advanced ASR systems are trained on large amounts of
data. In this case, WER estimation becomes necessary in many scenarios, for
example, selecting training data with unknown transcription quality or
estimating the testing performance of an ASR system without ground truth
transcriptions. Facing large amounts of data, the computation efficiency of a
WER estimator becomes essential in practical applications. However, previous
works usually did not consider it as a priority. In this paper, a Fast WER
estimator (Fe-WER) using self-supervised learning representation (SSLR) is
introduced. The estimator is built upon SSLR aggregated by average pooling. The
results show that Fe-WER outperformed the e-WER3 baseline relatively by 19.69%
and 7.16% on Ted-Lium3 in both evaluation metrics of root mean square error and
Pearson correlation coefficient, respectively. Moreover, the estimation
weighted by duration was 10.43% when the target was 10.88%. Lastly, the
inference speed was about 4x in terms of a real-time factor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Relevance of Phoneme Duration Variability of Synthesized Training
  Data for Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Rossenbach, Benedikt Hilmes, Ralf Schlüter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generated by text-to-speech (TTS) systems can be used to
improve automatic speech recognition (ASR) systems in low-resource or domain
mismatch tasks. It has been shown that TTS-generated outputs still do not have
the same qualities as real data. In this work we focus on the temporal
structure of synthetic data and its relation to ASR training. By using a novel
oracle setup we show how much the degradation of synthetic data quality is
influenced by duration modeling in non-autoregressive (NAR) TTS. To get
reference phoneme durations we use two common alignment methods, a hidden
Markov Gaussian-mixture model (HMM-GMM) aligner and a neural connectionist
temporal classification (CTC) aligner. Using a simple algorithm based on random
walks we shift phoneme duration distributions of the TTS system closer to real
durations, resulting in an improvement of an ASR system using synthetic data in
a semi-supervised setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ASRU 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> <span class="highlight-title">Voice Conversion</span> for Stuttered Speech, Instruments, Unseen Languages and
  Textually Described Voices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Baas, <span class="highlight-author">Herman Kamper</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice conversion aims to convert source speech into a target voice using
recordings of the target speaker as a reference. Newer models are producing
increasingly realistic output. But what happens when models are fed with
non-standard data, such as speech from a user with a speech impairment? We
investigate how a recent voice conversion model performs on non-standard
downstream voice conversion tasks. We use a simple but robust approach called
k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard
applications: stuttered voice conversion, cross-lingual voice conversion,
musical instrument conversion, and text-to-voice conversion. The latter
involves converting to a target voice specified through a text description,
e.g. "a young man with a high-pitched voice". Compared to an established
baseline, we find that kNN-VC retains high performance in stuttered and
cross-lingual voice conversion. Results are more mixed for the musical
instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some
instruments like drums but not on others. Nevertheless, this shows that voice
conversion models - and kNN-VC in particular - are increasingly applicable in a
range of non-standard downstream tasks. But there are still limitations when
samples are very far from the training distribution. Code, samples, trained
models: https://rf5.github.io/sacair2023-knnvc-demo/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure, 5 tables. Accepted at SACAIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CompA: Addressing the Gap in Compositional Reasoning in Audio-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreyan Ghosh, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, Chandra Kiran Evuru, S. Ramaneswaran, S. Sakshi, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental characteristic of audio is its compositional nature.
Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP)
that learns a shared representation between audio and language modalities have
improved performance in many downstream applications, including zero-shot audio
classification, audio retrieval, etc. However, the ability of these models to
effectively perform compositional reasoning remains largely unexplored and
necessitates additional research. In this paper, we propose CompA, a collection
of two expert-annotated benchmarks with a majority of real-world audio samples,
to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates
how well an ALM understands the order or occurrence of acoustic events in
audio, and CompA-attribute evaluates attribute binding of acoustic events. An
instance from either benchmark consists of two audio-caption pairs, where both
audios have the same acoustic events but with different compositions. An ALM is
evaluated on how well it matches the right audio to the right caption. Using
this benchmark, we first show that current ALMs perform only marginally better
than random chance, thereby struggling with compositional reasoning. Next, we
propose CompA-CLAP, where we fine-tune CLAP using a novel learning method to
improve its compositional reasoning abilities. To train CompA-CLAP, we first
propose improvements to contrastive training with composition-aware hard
negatives, allowing for more focused training. Next, we propose a novel modular
contrastive loss that helps the model learn fine-grained compositional
understanding and overcomes the acute scarcity of openly available
compositional audios. CompA-CLAP significantly improves over all our baseline
models on the CompA benchmark, indicating its superior compositional reasoning
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Toward Joint Language Modeling for Speech Units and Text <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju-Chieh Chou, Chung-Ming Chien, <span class="highlight-author">Wei-Ning Hsu</span>, Karen Livescu, Arun Babu, Alexis Conneau, Alexei Baevski, Michael Auli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech and text are two major forms of human language. The research community
has been focusing on mapping speech to text or vice versa for many years.
However, in the field of language modeling, very little effort has been made to
model them jointly. In light of this, we explore joint language modeling for
speech units and text. Specifically, we compare different speech tokenizers to
transform continuous speech signals into discrete units and use different
methods to construct mixed speech-text data. We introduce automatic metrics to
evaluate how well the joint LM mixes speech and text. We also fine-tune the LM
on downstream spoken language understanding (SLU) tasks with different
modalities (speech or text) and test its performance to assess the model's
learning of shared representations. Our results show that by mixing speech
units and text with our proposed mixing techniques, the joint LM improves over
a speech-only baseline on SLU tasks and shows zero-shot cross-modal
transferability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end Online Speaker Diarization with Target Speaker Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqing Wang, Ming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an online target speaker voice activity detection system
for speaker diarization tasks, which does not require a priori knowledge from
the clustering-based diarization system to obtain the target speaker
embeddings. By adapting the conventional target speaker voice activity
detection for real-time operation, this framework can identify speaker
activities using self-generated embeddings, resulting in consistent performance
without permutation inconsistencies in the inference phase. During the
inference process, we employ a front-end model to extract the frame-level
speaker embeddings for each coming block of a signal. Next, we predict the
detection state of each speaker based on these frame-level speaker embeddings
and the previously estimated target speaker embedding. Then, the target speaker
embeddings are updated by aggregating these frame-level speaker embeddings
according to the predictions in the current block. Our model predicts the
results for each block and updates the target speakers' embeddings until
reaching the end of the signal. Experimental results show that the proposed
method outperforms the offline clustering-based diarization system on the
DIHARD III and AliMeeting datasets. The proposed method is further extended to
multi-channel data, which achieves similar performance with the
state-of-the-art offline diarization systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/ACM Transactions on Audio, Speech, and Language
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Vec-Tok Speech: speech vectorization and tokenization for neural speech
  generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfa Zhu, Yuanjun Lv, Yi Lei, Tao Li, Wendi He, Hongbin Zhou, Heng Lu, <span class="highlight-author">Lei Xie</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have recently flourished in natural language processing
and computer vision, generating high-fidelity texts or images in various tasks.
In contrast, the current speech generative models are still struggling
regarding speech quality and task generalization. This paper presents Vec-Tok
Speech, an extensible framework that resembles multiple speech generation
tasks, generating expressive and high-fidelity speech. Specifically, we propose
a novel speech codec based on speech vectors and semantic tokens. Speech
vectors contain acoustic details contributing to high-fidelity speech
reconstruction, while semantic tokens focus on the linguistic content of
speech, facilitating language modeling. Based on the proposed speech codec,
Vec-Tok Speech leverages an LM to undertake the core of speech generation.
Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and
bit rate for lower exposure bias and longer context coverage, improving the
performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual
zero-shot voice conversion (VC), zero-shot speaking style transfer
text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising,
and speaker de-identification and anonymization. Experiments show that Vec-Tok
Speech, built on 50k hours of speech, performs better than other SOTA models.
Code will be available at https://github.com/BakerBunker/VecTok .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Initial Investigation of Neural Replay Simulator for Over-the-Air
  Adversarial Perturbations to Automatic Speaker Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05354v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05354v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Li, Li Wang, Liumeng Xue, Lei Wang, Zhizheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning has advanced Automatic Speaker Verification (ASV) in the past
few years. Although it is known that deep learning-based ASV systems are
vulnerable to adversarial examples in digital access, there are few studies on
adversarial attacks in the context of physical access, where a replay process
(i.e., over the air) is involved. An over-the-air attack involves a
loudspeaker, a microphone, and a replaying environment that impacts the
movement of the sound wave. Our initial experiment confirms that the replay
process impacts the effectiveness of the over-the-air attack performance. This
study performs an initial investigation towards utilizing a neural replay
simulator to improve over-the-air adversarial attack robustness. This is
achieved by using a neural waveform synthesizer to simulate the replay process
when estimating the adversarial perturbations. Experiments conducted on the
ASVspoof2019 dataset confirm that the neural replay simulator can considerably
increase the success rates of over-the-air adversarial attacks. This raises the
concern for adversarial attacks on speaker verification in physical access
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soundify: Matching Sound Effects to Video <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chuan-En Lin, Anastasis Germanidis, Cristóbal Valenzuela, Yining Shi, Nikolas Martelaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the art of video editing, sound helps add character to an object and
immerse the viewer within a space. Through formative interviews with
professional editors (N=10), we found that the task of adding sounds to video
can be challenging. This paper presents Soundify, a system that assists editors
in matching sounds to video. Given a video, Soundify identifies matching
sounds, synchronizes the sounds to the video, and dynamically adjusts panning
and volume to create spatial audio. In a human evaluation study (N=889), we
show that Soundify is capable of matching sounds to video out-of-the-box for a
diverse range of audio categories. In a within-subjects expert study (N=12), we
demonstrate the usefulness of Soundify in helping video editors match sounds to
video with lighter workload, reduced task completion time, and improved
usability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;
  Online demo: https://soundify.cc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04946v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04946v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking-head synthesis is a popular research topic for virtual
human-related applications. However, the inflexibility and inefficiency of
existing methods, which necessitate expensive end-to-end training to transfer
emotions from guidance videos to talking-head predictions, are significant
limitations. In this work, we propose the Emotional Adaptation for Audio-driven
Talking-head (EAT) method, which transforms emotion-agnostic talking-head
models into emotion-controllable ones in a cost-effective and efficient manner
through parameter-efficient adaptations. Our approach utilizes a pretrained
emotion-agnostic talking-head transformer and introduces three lightweight
adaptations (the Deep Emotional Prompts, Emotional Deformation Network, and
Emotional Adaptation Module) from different perspectives to enable precise and
realistic emotion controls. Our experiments demonstrate that our approach
achieves state-of-the-art performance on widely-used benchmarks, including LRW
and MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable
generalization ability, even in scenarios where emotional training videos are
scarce or nonexistent. Project website: https://yuangan.github.io/eat/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2023. Project page: https://yuangan.github.io/eat/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasiia Iashchenko, Pavel Andreev, Ivan Shchekotov, Nicholas Babaev, Dmitry Vetrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces UnDiff, a diffusion probabilistic model capable of
solving various speech inverse tasks. Being once trained for speech waveform
generation in an unconditional manner, it can be adapted to different tasks
including degradation inversion, neural vocoding, and source separation. In
this paper, we, first, tackle the challenging problem of unconditional waveform
generation by comparing different neural architectures and preconditioning
domains. After that, we demonstrate how the trained unconditional diffusion
could be adapted to different tasks of speech processing by the means of recent
developments in post-training conditioning of diffusion models. Finally, we
demonstrate the performance of the proposed technique on the tasks of bandwidth
extension, declipping, vocoding, and speech source separation and compare it to
the baselines. The codes are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> PromptTTS 2: Describing and Generating Voices with Text Prompt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02285v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02285v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichong Leng, Zhifang Guo, Kai Shen, <span class="highlight-author">Xu Tan</span>, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech conveys more information than text, as the same word can be uttered in
various voices to convey diverse information. Compared to traditional
text-to-speech (TTS) methods relying on speech prompts (reference speech) for
voice variability, using text prompts (descriptions) is more user-friendly
since speech prompts can be hard to find or may not exist at all. TTS
approaches based on the text prompt face two main challenges: 1) the
one-to-many problem, where not all details about voice variability can be
described in the text prompt, and 2) the limited availability of text prompt
datasets, where vendors and large cost of data labeling are required to write
text prompts for speech. In this work, we introduce PromptTTS 2 to address
these challenges with a variation network to provide variability information of
voice not captured by text prompts, and a prompt generation pipeline to utilize
the large language models (LLM) to compose high quality text prompts.
Specifically, the variation network predicts the representation extracted from
the reference speech (which contains full information about voice variability)
based on the text prompt representation. For the prompt generation pipeline, it
generates text prompts for speech with a speech language understanding model to
recognize voice attributes (e.g., gender, speed) from speech and a large
language model to formulate text prompts based on the recognition results.
Experiments on a large-scale (44K hours) speech dataset demonstrate that
compared to the previous works, PromptTTS 2 generates voices more consistent
with text prompts and supports the sampling of diverse voice variability,
thereby offering users more choices on voice generation. Additionally, the
prompt generation pipeline produces high-quality text prompts, eliminating the
large labeling cost. The demo page of PromptTTS 2 is available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo page: https://speechresearch.github.io/prompttts2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MusiLingo: Bridging Music and Text with Pre-trained Language Models for
  Music Captioning and Query Response 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Deng, Yinghao Ma, Yudong Liu, Rongchen Guo, Ge Zhang, Wenhu Chen, Wenhao Huang, Emmanouil Benetos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown immense potential in multimodal
applications, yet the convergence of textual and musical domains remains
relatively unexplored. To address this gap, we present MusiLingo, a novel
system for music caption generation and music-related query responses.
MusiLingo employs a single projection layer to align music representations from
the pre-trained frozen music audio model MERT with the frozen Vicuna-7B
language model (an adaption of LLaMA), bridging the gap between music audio and
textual contexts. We train it on an extensive music caption dataset and
fine-tune it with instructional data. Due to the scarcity of high-quality music
Q\&A datasets, we created the Music Instruct (MI) dataset from captions in the
MusicCaps datasets, tailored for open-ended music inquiries. Empirical
evaluations demonstrate its competitive performance in generating music
captions and composing music-related Q&A pairs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Audio and Speech Processing <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of time and note duration tokenizations on deep learning symbolic
  music modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Fradet, Nicolas Gutowski, Fabien Chhel, Jean-Pierre Briot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic music is widely used in various deep learning tasks, including
generation, transcription, synthesis, and Music Information Retrieval (MIR). It
is mostly employed with discrete models like Transformers, which require music
to be tokenized, i.e., formatted into sequences of distinct elements called
tokens. Tokenization can be performed in different ways. As Transformer can
struggle at reasoning, but capture more easily explicit information, it is
important to study how the way the information is represented for such model
impact their performances. In this work, we analyze the common tokenization
methods and experiment with time and note duration representations. We compare
the performances of these two impactful criteria on several tasks, including
composer and emotion classification, music generation, and sequence
representation learning. We demonstrate that explicit information leads to
better results depending on the task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowdsourced and Automatic Speech Prominence Estimation <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Morrison, Pranav Pawar, Nathan Pruyne, Jennifer Cole, Bryan Pardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prominence of a spoken word is the degree to which an average native
listener perceives the word as salient or emphasized relative to its context.
Speech prominence estimation is the process of assigning a numeric value to the
prominence of each word in an utterance. These prominence labels are useful for
linguistic analysis, as well as training automated systems to perform
emphasis-controlled text-to-speech or emotion recognition. Manually annotating
prominence is time-consuming and expensive, which motivates the development of
automated methods for speech prominence estimation. However, developing such an
automated system using machine-learning methods requires human-annotated
training data. Using our system for acquiring such human annotations, we
collect and open-source crowdsourced annotations of a portion of the LibriTTS
dataset. We use these annotations as ground truth to train a neural speech
prominence estimator that generalizes to unseen speakers, datasets, and
speaking styles. We investigate design decisions for neural prominence
estimation as well as how neural prominence estimation improves as a function
of two key factors of annotation cost: dataset size and the number of
annotations per utterance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A cry for help: Early detection of brain injury in newborns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles C. Onu, Samantha Latremouille, Arsenii Gorin, Junhao Wang, Uchenna Ekwochi, Peter O. Ubuane, Omolara A. Kehinde, Muhammad A. Salisu, Datonye Briggs, <span class="highlight-author">Yoshua Bengio</span>, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the 1960s, neonatal clinicians have known that newborns suffering from
certain neurological conditions exhibit altered crying patterns such as the
high-pitched cry in birth asphyxia. Despite an annual burden of over 1.5
million infant deaths and disabilities, early detection of neonatal brain
injuries due to asphyxia remains a challenge, particularly in developing
countries where the majority of births are not attended by a trained physician.
Here we report on the first inter-continental clinical study to demonstrate
that neonatal brain injury can be reliably determined from recorded infant
cries using an AI algorithm we call Roseline. Previous and recent work has been
limited by the lack of a large, high-quality clinical database of cry
recordings, constraining the application of state-of-the-art machine learning.
We develop a new training methodology for audio-based pathology detection
models and evaluate this system on a large database of newborn cry sounds
acquired from geographically diverse settings -- 5 hospitals across 3
continents. Our system extracts interpretable acoustic biomarkers that support
clinical decisions and is able to accurately detect neurological injury from
newborns' cries with an AUC of 92.5% (88.7% sensitivity at 80% specificity).
Cry-based neurological monitoring opens the door for low-cost, easy-to-use,
non-invasive and contact-free screening of at-risk babies, especially when
integrated into simple devices like smartphones or neonatal ICU monitors. This
would provide a reliable tool where there are no alternatives, but also curtail
the need to regularly exert newborns to physically-exhausting or
radiation-exposing assessments such as brain CT scans. This work sets the stage
for embracing the infant cry as a vital sign and indicates the potential of
AI-driven sound monitoring for the future of affordable healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Variational Auto-encoder based Audio-Visual Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Mao, Jing Zhang, Mochu Xiang, Yiran Zhong, Yuchao Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an Explicit Conditional Multimodal Variational Auto-Encoder
(ECMVAE) for audio-visual segmentation (AVS), aiming to segment sound sources
in the video sequence. Existing AVS methods focus on implicit feature fusion
strategies, where models are trained to fit the discrete samples in the
dataset. With a limited and less diverse dataset, the resulting performance is
usually unsatisfactory. In contrast, we address this problem from an effective
representation learning perspective, aiming to model the contribution of each
modality explicitly. Specifically, we find that audio contains critical
category information of the sound producers, and visual data provides candidate
sound producer(s). Their shared information corresponds to the target sound
producer(s) shown in the visual data. In this case, cross-modal shared
representation learning is especially important for AVS. To achieve this, our
ECMVAE factorizes the representations of each modality with a modality-shared
representation and a modality-specific representation. An orthogonality
constraint is applied between the shared and specific representations to
maintain the exclusive attribute of the factorized latent code. Further, a
mutual information maximization regularizer is introduced to achieve extensive
exploration of each modality. Quantitative and qualitative evaluations on the
AVSBench demonstrate the effectiveness of our approach, leading to a new
state-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging
MS3 subset for multiple sound source segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2023,Project
  page(https://npucvr.github.io/MMVAE-AVS),Code(https://github.com/OpenNLPLab/MMVAE-AVS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A Single Speech Enhancement Model Unifying Dereverberation, Denoising,
  Speaker Counting, Separation, and Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kohei Saijo, Wangyou Zhang, Zhong-Qiu Wang, <span class="highlight-author">Shinji Watanabe</span>, Tetsunori Kobayashi, Tetsuji Ogawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a multi-task universal speech enhancement (MUSE) model that can
perform five speech enhancement (SE) tasks: dereverberation, denoising, speech
separation (SS), target speaker extraction (TSE), and speaker counting. This is
achieved by integrating two modules into an SE model: 1) an internal separation
module that does both speaker counting and separation; and 2) a TSE module that
extracts the target speech from the internal separation outputs using target
speaker cues. The model is trained to perform TSE if the target speaker cue is
given and SS otherwise. By training the model to remove noise and
reverberation, we allow the model to tackle the five tasks mentioned above with
a single model, which has not been accomplished yet. Evaluation results
demonstrate that the proposed MUSE model can successfully handle multiple tasks
with a single model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 2 tables, accepted by ASRU2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Word Error Rate Estimation Using Self-Supervised Representations
  For Speech And Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanho Park, Chengsong Lu, Mingjie Chen, Thomas Hain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of automatic speech recognition (ASR) is typically measured by
word error rate (WER). WER estimation is a task aiming to predict the WER of an
ASR system, given a speech utterance and a transcription. This task has gained
increasing attention while advanced ASR systems are trained on large amounts of
data. In this case, WER estimation becomes necessary in many scenarios, for
example, selecting training data with unknown transcription quality or
estimating the testing performance of an ASR system without ground truth
transcriptions. Facing large amounts of data, the computation efficiency of a
WER estimator becomes essential in practical applications. However, previous
works usually did not consider it as a priority. In this paper, a Fast WER
estimator (Fe-WER) using self-supervised learning representation (SSLR) is
introduced. The estimator is built upon SSLR aggregated by average pooling. The
results show that Fe-WER outperformed the e-WER3 baseline relatively by 19.69%
and 7.16% on Ted-Lium3 in both evaluation metrics of root mean square error and
Pearson correlation coefficient, respectively. Moreover, the estimation
weighted by duration was 10.43% when the target was 10.88%. Lastly, the
inference speed was about 4x in terms of a real-time factor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Relevance of Phoneme Duration Variability of Synthesized Training
  Data for Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Rossenbach, Benedikt Hilmes, Ralf Schlüter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generated by text-to-speech (TTS) systems can be used to
improve automatic speech recognition (ASR) systems in low-resource or domain
mismatch tasks. It has been shown that TTS-generated outputs still do not have
the same qualities as real data. In this work we focus on the temporal
structure of synthetic data and its relation to ASR training. By using a novel
oracle setup we show how much the degradation of synthetic data quality is
influenced by duration modeling in non-autoregressive (NAR) TTS. To get
reference phoneme durations we use two common alignment methods, a hidden
Markov Gaussian-mixture model (HMM-GMM) aligner and a neural connectionist
temporal classification (CTC) aligner. Using a simple algorithm based on random
walks we shift phoneme duration distributions of the TTS system closer to real
durations, resulting in an improvement of an ASR system using synthetic data in
a semi-supervised setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ASRU 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> <span class="highlight-title">Voice Conversion</span> for Stuttered Speech, Instruments, Unseen Languages and
  Textually Described Voices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Baas, <span class="highlight-author">Herman Kamper</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice conversion aims to convert source speech into a target voice using
recordings of the target speaker as a reference. Newer models are producing
increasingly realistic output. But what happens when models are fed with
non-standard data, such as speech from a user with a speech impairment? We
investigate how a recent voice conversion model performs on non-standard
downstream voice conversion tasks. We use a simple but robust approach called
k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard
applications: stuttered voice conversion, cross-lingual voice conversion,
musical instrument conversion, and text-to-voice conversion. The latter
involves converting to a target voice specified through a text description,
e.g. "a young man with a high-pitched voice". Compared to an established
baseline, we find that kNN-VC retains high performance in stuttered and
cross-lingual voice conversion. Results are more mixed for the musical
instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some
instruments like drums but not on others. Nevertheless, this shows that voice
conversion models - and kNN-VC in particular - are increasingly applicable in a
range of non-standard downstream tasks. But there are still limitations when
samples are very far from the training distribution. Code, samples, trained
models: https://rf5.github.io/sacair2023-knnvc-demo/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure, 5 tables. Accepted at SACAIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CompA: Addressing the Gap in Compositional Reasoning in Audio-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreyan Ghosh, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, Chandra Kiran Evuru, S. Ramaneswaran, S. Sakshi, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental characteristic of audio is its compositional nature.
Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP)
that learns a shared representation between audio and language modalities have
improved performance in many downstream applications, including zero-shot audio
classification, audio retrieval, etc. However, the ability of these models to
effectively perform compositional reasoning remains largely unexplored and
necessitates additional research. In this paper, we propose CompA, a collection
of two expert-annotated benchmarks with a majority of real-world audio samples,
to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates
how well an ALM understands the order or occurrence of acoustic events in
audio, and CompA-attribute evaluates attribute binding of acoustic events. An
instance from either benchmark consists of two audio-caption pairs, where both
audios have the same acoustic events but with different compositions. An ALM is
evaluated on how well it matches the right audio to the right caption. Using
this benchmark, we first show that current ALMs perform only marginally better
than random chance, thereby struggling with compositional reasoning. Next, we
propose CompA-CLAP, where we fine-tune CLAP using a novel learning method to
improve its compositional reasoning abilities. To train CompA-CLAP, we first
propose improvements to contrastive training with composition-aware hard
negatives, allowing for more focused training. Next, we propose a novel modular
contrastive loss that helps the model learn fine-grained compositional
understanding and overcomes the acute scarcity of openly available
compositional audios. CompA-CLAP significantly improves over all our baseline
models on the CompA benchmark, indicating its superior compositional reasoning
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Toward Joint Language Modeling for Speech Units and Text <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju-Chieh Chou, Chung-Ming Chien, <span class="highlight-author">Wei-Ning Hsu</span>, Karen Livescu, Arun Babu, Alexis Conneau, Alexei Baevski, Michael Auli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech and text are two major forms of human language. The research community
has been focusing on mapping speech to text or vice versa for many years.
However, in the field of language modeling, very little effort has been made to
model them jointly. In light of this, we explore joint language modeling for
speech units and text. Specifically, we compare different speech tokenizers to
transform continuous speech signals into discrete units and use different
methods to construct mixed speech-text data. We introduce automatic metrics to
evaluate how well the joint LM mixes speech and text. We also fine-tune the LM
on downstream spoken language understanding (SLU) tasks with different
modalities (speech or text) and test its performance to assess the model's
learning of shared representations. Our results show that by mixing speech
units and text with our proposed mixing techniques, the joint LM improves over
a speech-only baseline on SLU tasks and shows zero-shot cross-modal
transferability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end Online Speaker Diarization with Target Speaker Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqing Wang, Ming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an online target speaker voice activity detection system
for speaker diarization tasks, which does not require a priori knowledge from
the clustering-based diarization system to obtain the target speaker
embeddings. By adapting the conventional target speaker voice activity
detection for real-time operation, this framework can identify speaker
activities using self-generated embeddings, resulting in consistent performance
without permutation inconsistencies in the inference phase. During the
inference process, we employ a front-end model to extract the frame-level
speaker embeddings for each coming block of a signal. Next, we predict the
detection state of each speaker based on these frame-level speaker embeddings
and the previously estimated target speaker embedding. Then, the target speaker
embeddings are updated by aggregating these frame-level speaker embeddings
according to the predictions in the current block. Our model predicts the
results for each block and updates the target speakers' embeddings until
reaching the end of the signal. Experimental results show that the proposed
method outperforms the offline clustering-based diarization system on the
DIHARD III and AliMeeting datasets. The proposed method is further extended to
multi-channel data, which achieves similar performance with the
state-of-the-art offline diarization systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/ACM Transactions on Audio, Speech, and Language
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Typing to Listen at the Cocktail Party: Text-Guided Target Speaker
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hao, Jibin Wu, Jianwei Yu, Chenglin Xu, Kay Chen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess an extraordinary ability to selectively focus on the sound
source of interest amidst complex acoustic environments, commonly referred to
as cocktail party scenarios. In an attempt to replicate this remarkable
auditory attention capability in machines, target speaker extraction (TSE)
models have been developed. These models leverage the pre-registered cues of
the target speaker to extract the sound source of interest. However, the
effectiveness of these models is hindered in real-world scenarios due to the
unreliable or even absence of pre-registered cues. To address this limitation,
this study investigates the integration of natural language description to
enhance the feasibility, controllability, and performance of existing TSE
models. Specifically, we propose a model named LLM-TSE, wherein a large
language model (LLM) to extract useful semantic cues from the user's typed text
input. These cues can serve as independent extraction cues, task selectors to
control the TSE process, or complement the pre-registered cues. Our
experimental results demonstrate competitive performance when only text-based
cues are presented, the effectiveness of using input text as a task selector,
and a new state-of-the-art when combining text-based cues with pre-registered
cues. To our knowledge, this is the first study to successfully incorporate
LLMs to guide target speaker extraction, which can be a cornerstone for
cocktail party problem research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, https://github.com/haoxiangsnr/llm-tse</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Vec-Tok Speech: speech vectorization and tokenization for neural speech
  generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfa Zhu, Yuanjun Lv, Yi Lei, Tao Li, Wendi He, Hongbin Zhou, Heng Lu, <span class="highlight-author">Lei Xie</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have recently flourished in natural language processing
and computer vision, generating high-fidelity texts or images in various tasks.
In contrast, the current speech generative models are still struggling
regarding speech quality and task generalization. This paper presents Vec-Tok
Speech, an extensible framework that resembles multiple speech generation
tasks, generating expressive and high-fidelity speech. Specifically, we propose
a novel speech codec based on speech vectors and semantic tokens. Speech
vectors contain acoustic details contributing to high-fidelity speech
reconstruction, while semantic tokens focus on the linguistic content of
speech, facilitating language modeling. Based on the proposed speech codec,
Vec-Tok Speech leverages an LM to undertake the core of speech generation.
Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and
bit rate for lower exposure bias and longer context coverage, improving the
performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual
zero-shot voice conversion (VC), zero-shot speaking style transfer
text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising,
and speaker de-identification and anonymization. Experiments show that Vec-Tok
Speech, built on 50k hours of speech, performs better than other SOTA models.
Code will be available at https://github.com/BakerBunker/VecTok .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Initial Investigation of Neural Replay Simulator for Over-the-Air
  Adversarial Perturbations to Automatic Speaker Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05354v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05354v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Li, Li Wang, Liumeng Xue, Lei Wang, Zhizheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning has advanced Automatic Speaker Verification (ASV) in the past
few years. Although it is known that deep learning-based ASV systems are
vulnerable to adversarial examples in digital access, there are few studies on
adversarial attacks in the context of physical access, where a replay process
(i.e., over the air) is involved. An over-the-air attack involves a
loudspeaker, a microphone, and a replaying environment that impacts the
movement of the sound wave. Our initial experiment confirms that the replay
process impacts the effectiveness of the over-the-air attack performance. This
study performs an initial investigation towards utilizing a neural replay
simulator to improve over-the-air adversarial attack robustness. This is
achieved by using a neural waveform synthesizer to simulate the replay process
when estimating the adversarial perturbations. Experiments conducted on the
ASVspoof2019 dataset confirm that the neural replay simulator can considerably
increase the success rates of over-the-air adversarial attacks. This raises the
concern for adversarial attacks on speaker verification in physical access
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soundify: Matching Sound Effects to Video <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chuan-En Lin, Anastasis Germanidis, Cristóbal Valenzuela, Yining Shi, Nikolas Martelaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the art of video editing, sound helps add character to an object and
immerse the viewer within a space. Through formative interviews with
professional editors (N=10), we found that the task of adding sounds to video
can be challenging. This paper presents Soundify, a system that assists editors
in matching sounds to video. Given a video, Soundify identifies matching
sounds, synchronizes the sounds to the video, and dynamically adjusts panning
and volume to create spatial audio. In a human evaluation study (N=889), we
show that Soundify is capable of matching sounds to video out-of-the-box for a
diverse range of audio categories. In a within-subjects expert study (N=12), we
demonstrate the usefulness of Soundify in helping video editors match sounds to
video with lighter workload, reduced task completion time, and improved
usability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;
  Online demo: https://soundify.cc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04946v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04946v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking-head synthesis is a popular research topic for virtual
human-related applications. However, the inflexibility and inefficiency of
existing methods, which necessitate expensive end-to-end training to transfer
emotions from guidance videos to talking-head predictions, are significant
limitations. In this work, we propose the Emotional Adaptation for Audio-driven
Talking-head (EAT) method, which transforms emotion-agnostic talking-head
models into emotion-controllable ones in a cost-effective and efficient manner
through parameter-efficient adaptations. Our approach utilizes a pretrained
emotion-agnostic talking-head transformer and introduces three lightweight
adaptations (the Deep Emotional Prompts, Emotional Deformation Network, and
Emotional Adaptation Module) from different perspectives to enable precise and
realistic emotion controls. Our experiments demonstrate that our approach
achieves state-of-the-art performance on widely-used benchmarks, including LRW
and MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable
generalization ability, even in scenarios where emotional training videos are
scarce or nonexistent. Project website: https://yuangan.github.io/eat/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2023. Project page: https://yuangan.github.io/eat/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasiia Iashchenko, Pavel Andreev, Ivan Shchekotov, Nicholas Babaev, Dmitry Vetrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces UnDiff, a diffusion probabilistic model capable of
solving various speech inverse tasks. Being once trained for speech waveform
generation in an unconditional manner, it can be adapted to different tasks
including degradation inversion, neural vocoding, and source separation. In
this paper, we, first, tackle the challenging problem of unconditional waveform
generation by comparing different neural architectures and preconditioning
domains. After that, we demonstrate how the trained unconditional diffusion
could be adapted to different tasks of speech processing by the means of recent
developments in post-training conditioning of diffusion models. Finally, we
demonstrate the performance of the proposed technique on the tasks of bandwidth
extension, declipping, vocoding, and speech source separation and compare it to
the baselines. The codes are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> PromptTTS 2: Describing and Generating Voices with Text Prompt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02285v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02285v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichong Leng, Zhifang Guo, Kai Shen, <span class="highlight-author">Xu Tan</span>, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech conveys more information than text, as the same word can be uttered in
various voices to convey diverse information. Compared to traditional
text-to-speech (TTS) methods relying on speech prompts (reference speech) for
voice variability, using text prompts (descriptions) is more user-friendly
since speech prompts can be hard to find or may not exist at all. TTS
approaches based on the text prompt face two main challenges: 1) the
one-to-many problem, where not all details about voice variability can be
described in the text prompt, and 2) the limited availability of text prompt
datasets, where vendors and large cost of data labeling are required to write
text prompts for speech. In this work, we introduce PromptTTS 2 to address
these challenges with a variation network to provide variability information of
voice not captured by text prompts, and a prompt generation pipeline to utilize
the large language models (LLM) to compose high quality text prompts.
Specifically, the variation network predicts the representation extracted from
the reference speech (which contains full information about voice variability)
based on the text prompt representation. For the prompt generation pipeline, it
generates text prompts for speech with a speech language understanding model to
recognize voice attributes (e.g., gender, speed) from speech and a large
language model to formulate text prompts based on the recognition results.
Experiments on a large-scale (44K hours) speech dataset demonstrate that
compared to the previous works, PromptTTS 2 generates voices more consistent
with text prompts and supports the sampling of diverse voice variability,
thereby offering users more choices on voice generation. Additionally, the
prompt generation pipeline produces high-quality text prompts, eliminating the
large labeling cost. The demo page of PromptTTS 2 is available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo page: https://speechresearch.github.io/prompttts2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MusiLingo: Bridging Music and Text with Pre-trained Language Models for
  Music Captioning and Query Response 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Deng, Yinghao Ma, Yudong Liu, Rongchen Guo, Ge Zhang, Wenhu Chen, Wenhao Huang, Emmanouil Benetos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown immense potential in multimodal
applications, yet the convergence of textual and musical domains remains
relatively unexplored. To address this gap, we present MusiLingo, a novel
system for music caption generation and music-related query responses.
MusiLingo employs a single projection layer to align music representations from
the pre-trained frozen music audio model MERT with the frozen Vicuna-7B
language model (an adaption of LLaMA), bridging the gap between music audio and
textual contexts. We train it on an extensive music caption dataset and
fine-tune it with instructional data. Due to the scarcity of high-quality music
Q\&A datasets, we created the Music Instruct (MI) dataset from captions in the
MusicCaps datasets, tailored for open-ended music inquiries. Empirical
evaluations demonstrate its competitive performance in generating music
captions and composing music-related Q&A pairs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-10-28T01:40:26.663773810Z">
            2023-10-28 01:40:26 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
