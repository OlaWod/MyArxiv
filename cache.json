{"2023-10-12T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2310.07246v2","updated":"2023-10-12T05:49:52Z","published":"2023-10-11T07:23:27Z","title":"Vec-Tok Speech: speech vectorization and tokenization for neural speech\n  generation","summary":"  Language models (LMs) have recently flourished in natural language processing\nand computer vision, generating high-fidelity texts or images in various tasks.\nIn contrast, the current speech generative models are still struggling\nregarding speech quality and task generalization. This paper presents Vec-Tok\nSpeech, an extensible framework that resembles multiple speech generation\ntasks, generating expressive and high-fidelity speech. Specifically, we propose\na novel speech codec based on speech vectors and semantic tokens. Speech\nvectors contain acoustic details contributing to high-fidelity speech\nreconstruction, while semantic tokens focus on the linguistic content of\nspeech, facilitating language modeling. Based on the proposed speech codec,\nVec-Tok Speech leverages an LM to undertake the core of speech generation.\nMoreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and\nbit rate for lower exposure bias and longer context coverage, improving the\nperformance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual\nzero-shot voice conversion (VC), zero-shot speaking style transfer\ntext-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising,\nand speaker de-identification and anonymization. Experiments show that Vec-Tok\nSpeech, built on 50k hours of speech, performs better than other SOTA models.\nCode will be available at https://github.com/BakerBunker/VecTok .\n","authors":["Xinfa Zhu","Yuanjun Lv","Yi Lei","Tao Li","Wendi He","Hongbin Zhou","Heng Lu","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2310.07246v2.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.05354v2","updated":"2023-10-12T14:22:49Z","published":"2023-10-09T02:31:05Z","title":"An Initial Investigation of Neural Replay Simulator for Over-the-Air\n  Adversarial Perturbations to Automatic Speaker Verification","summary":"  Deep Learning has advanced Automatic Speaker Verification (ASV) in the past\nfew years. Although it is known that deep learning-based ASV systems are\nvulnerable to adversarial examples in digital access, there are few studies on\nadversarial attacks in the context of physical access, where a replay process\n(i.e., over the air) is involved. An over-the-air attack involves a\nloudspeaker, a microphone, and a replaying environment that impacts the\nmovement of the sound wave. Our initial experiment confirms that the replay\nprocess impacts the effectiveness of the over-the-air attack performance. This\nstudy performs an initial investigation towards utilizing a neural replay\nsimulator to improve over-the-air adversarial attack robustness. This is\nachieved by using a neural waveform synthesizer to simulate the replay process\nwhen estimating the adversarial perturbations. Experiments conducted on the\nASVspoof2019 dataset confirm that the neural replay simulator can considerably\nincrease the success rates of over-the-air adversarial attacks. This raises the\nconcern for adversarial attacks on speaker verification in physical access\napplications.\n","authors":["Jiaqi Li","Li Wang","Liumeng Xue","Lei Wang","Zhizheng Wu"],"pdf_url":"https://arxiv.org/pdf/2310.05354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09726v2","updated":"2023-10-12T17:57:51Z","published":"2021-12-17T19:22:01Z","title":"Soundify: Matching Sound Effects to Video","summary":"  In the art of video editing, sound helps add character to an object and\nimmerse the viewer within a space. Through formative interviews with\nprofessional editors (N=10), we found that the task of adding sounds to video\ncan be challenging. This paper presents Soundify, a system that assists editors\nin matching sounds to video. Given a video, Soundify identifies matching\nsounds, synchronizes the sounds to the video, and dynamically adjusts panning\nand volume to create spatial audio. In a human evaluation study (N=889), we\nshow that Soundify is capable of matching sounds to video out-of-the-box for a\ndiverse range of audio categories. In a within-subjects expert study (N=12), we\ndemonstrate the usefulness of Soundify in helping video editors match sounds to\nvideo with lighter workload, reduced task completion time, and improved\nusability.\n","authors":["David Chuan-En Lin","Anastasis Germanidis","Crist√≥bal Valenzuela","Yining Shi","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2112.09726v2.pdf","comment":"Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;\n  Online demo: https://soundify.cc"},{"id":"http://arxiv.org/abs/2310.08497v1","updated":"2023-10-12T16:56:37Z","published":"2023-10-12T16:56:37Z","title":"Impact of time and note duration tokenizations on deep learning symbolic\n  music modeling","summary":"  Symbolic music is widely used in various deep learning tasks, including\ngeneration, transcription, synthesis, and Music Information Retrieval (MIR). It\nis mostly employed with discrete models like Transformers, which require music\nto be tokenized, i.e., formatted into sequences of distinct elements called\ntokens. Tokenization can be performed in different ways. As Transformer can\nstruggle at reasoning, but capture more easily explicit information, it is\nimportant to study how the way the information is represented for such model\nimpact their performances. In this work, we analyze the common tokenization\nmethods and experiment with time and note duration representations. We compare\nthe performances of these two impactful criteria on several tasks, including\ncomposer and emotion classification, music generation, and sequence\nrepresentation learning. We demonstrate that explicit information leads to\nbetter results depending on the task.\n","authors":["Nathan Fradet","Nicolas Gutowski","Fabien Chhel","Jean-Pierre Briot"],"pdf_url":"https://arxiv.org/pdf/2310.08497v1.pdf","comment":"ISMIR 2023"},{"id":"http://arxiv.org/abs/2310.08464v1","updated":"2023-10-12T16:23:28Z","published":"2023-10-12T16:23:28Z","title":"Crowdsourced and Automatic Speech Prominence Estimation","summary":"  The prominence of a spoken word is the degree to which an average native\nlistener perceives the word as salient or emphasized relative to its context.\nSpeech prominence estimation is the process of assigning a numeric value to the\nprominence of each word in an utterance. These prominence labels are useful for\nlinguistic analysis, as well as training automated systems to perform\nemphasis-controlled text-to-speech or emotion recognition. Manually annotating\nprominence is time-consuming and expensive, which motivates the development of\nautomated methods for speech prominence estimation. However, developing such an\nautomated system using machine-learning methods requires human-annotated\ntraining data. Using our system for acquiring such human annotations, we\ncollect and open-source crowdsourced annotations of a portion of the LibriTTS\ndataset. We use these annotations as ground truth to train a neural speech\nprominence estimator that generalizes to unseen speakers, datasets, and\nspeaking styles. We investigate design decisions for neural prominence\nestimation as well as how neural prominence estimation improves as a function\nof two key factors of annotation cost: dataset size and the number of\nannotations per utterance.\n","authors":["Max Morrison","Pranav Pawar","Nathan Pruyne","Jennifer Cole","Bryan Pardo"],"pdf_url":"https://arxiv.org/pdf/2310.08464v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2309.04946v2","updated":"2023-10-12T15:04:30Z","published":"2023-09-10T06:33:17Z","title":"Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation","summary":"  Audio-driven talking-head synthesis is a popular research topic for virtual\nhuman-related applications. However, the inflexibility and inefficiency of\nexisting methods, which necessitate expensive end-to-end training to transfer\nemotions from guidance videos to talking-head predictions, are significant\nlimitations. In this work, we propose the Emotional Adaptation for Audio-driven\nTalking-head (EAT) method, which transforms emotion-agnostic talking-head\nmodels into emotion-controllable ones in a cost-effective and efficient manner\nthrough parameter-efficient adaptations. Our approach utilizes a pretrained\nemotion-agnostic talking-head transformer and introduces three lightweight\nadaptations (the Deep Emotional Prompts, Emotional Deformation Network, and\nEmotional Adaptation Module) from different perspectives to enable precise and\nrealistic emotion controls. Our experiments demonstrate that our approach\nachieves state-of-the-art performance on widely-used benchmarks, including LRW\nand MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable\ngeneralization ability, even in scenarios where emotional training videos are\nscarce or nonexistent. Project website: https://yuangan.github.io/eat/\n","authors":["Yuan Gan","Zongxin Yang","Xihang Yue","Lingyun Sun","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2309.04946v2.pdf","comment":"Accepted to ICCV 2023. Project page: https://yuangan.github.io/eat/"},{"id":"http://arxiv.org/abs/2310.08338v1","updated":"2023-10-12T13:56:42Z","published":"2023-10-12T13:56:42Z","title":"A cry for help: Early detection of brain injury in newborns","summary":"  Since the 1960s, neonatal clinicians have known that newborns suffering from\ncertain neurological conditions exhibit altered crying patterns such as the\nhigh-pitched cry in birth asphyxia. Despite an annual burden of over 1.5\nmillion infant deaths and disabilities, early detection of neonatal brain\ninjuries due to asphyxia remains a challenge, particularly in developing\ncountries where the majority of births are not attended by a trained physician.\nHere we report on the first inter-continental clinical study to demonstrate\nthat neonatal brain injury can be reliably determined from recorded infant\ncries using an AI algorithm we call Roseline. Previous and recent work has been\nlimited by the lack of a large, high-quality clinical database of cry\nrecordings, constraining the application of state-of-the-art machine learning.\nWe develop a new training methodology for audio-based pathology detection\nmodels and evaluate this system on a large database of newborn cry sounds\nacquired from geographically diverse settings -- 5 hospitals across 3\ncontinents. Our system extracts interpretable acoustic biomarkers that support\nclinical decisions and is able to accurately detect neurological injury from\nnewborns' cries with an AUC of 92.5% (88.7% sensitivity at 80% specificity).\nCry-based neurological monitoring opens the door for low-cost, easy-to-use,\nnon-invasive and contact-free screening of at-risk babies, especially when\nintegrated into simple devices like smartphones or neonatal ICU monitors. This\nwould provide a reliable tool where there are no alternatives, but also curtail\nthe need to regularly exert newborns to physically-exhausting or\nradiation-exposing assessments such as brain CT scans. This work sets the stage\nfor embracing the infant cry as a vital sign and indicates the potential of\nAI-driven sound monitoring for the future of affordable healthcare.\n","authors":["Charles C. Onu","Samantha Latremouille","Arsenii Gorin","Junhao Wang","Uchenna Ekwochi","Peter O. Ubuane","Omolara A. Kehinde","Muhammad A. Salisu","Datonye Briggs","Yoshua Bengio","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2310.08338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08303v1","updated":"2023-10-12T13:09:40Z","published":"2023-10-12T13:09:40Z","title":"Multimodal Variational Auto-encoder based Audio-Visual Segmentation","summary":"  We propose an Explicit Conditional Multimodal Variational Auto-Encoder\n(ECMVAE) for audio-visual segmentation (AVS), aiming to segment sound sources\nin the video sequence. Existing AVS methods focus on implicit feature fusion\nstrategies, where models are trained to fit the discrete samples in the\ndataset. With a limited and less diverse dataset, the resulting performance is\nusually unsatisfactory. In contrast, we address this problem from an effective\nrepresentation learning perspective, aiming to model the contribution of each\nmodality explicitly. Specifically, we find that audio contains critical\ncategory information of the sound producers, and visual data provides candidate\nsound producer(s). Their shared information corresponds to the target sound\nproducer(s) shown in the visual data. In this case, cross-modal shared\nrepresentation learning is especially important for AVS. To achieve this, our\nECMVAE factorizes the representations of each modality with a modality-shared\nrepresentation and a modality-specific representation. An orthogonality\nconstraint is applied between the shared and specific representations to\nmaintain the exclusive attribute of the factorized latent code. Further, a\nmutual information maximization regularizer is introduced to achieve extensive\nexploration of each modality. Quantitative and qualitative evaluations on the\nAVSBench demonstrate the effectiveness of our approach, leading to a new\nstate-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging\nMS3 subset for multiple sound source segmentation.\n","authors":["Yuxin Mao","Jing Zhang","Mochu Xiang","Yiran Zhong","Yuchao Dai"],"pdf_url":"https://arxiv.org/pdf/2310.08303v1.pdf","comment":"Accepted by ICCV2023,Project\n  page(https://npucvr.github.io/MMVAE-AVS),Code(https://github.com/OpenNLPLab/MMVAE-AVS)"},{"id":"http://arxiv.org/abs/2310.08277v1","updated":"2023-10-12T12:28:51Z","published":"2023-10-12T12:28:51Z","title":"A Single Speech Enhancement Model Unifying Dereverberation, Denoising,\n  Speaker Counting, Separation, and Extraction","summary":"  We propose a multi-task universal speech enhancement (MUSE) model that can\nperform five speech enhancement (SE) tasks: dereverberation, denoising, speech\nseparation (SS), target speaker extraction (TSE), and speaker counting. This is\nachieved by integrating two modules into an SE model: 1) an internal separation\nmodule that does both speaker counting and separation; and 2) a TSE module that\nextracts the target speech from the internal separation outputs using target\nspeaker cues. The model is trained to perform TSE if the target speaker cue is\ngiven and SS otherwise. By training the model to remove noise and\nreverberation, we allow the model to tackle the five tasks mentioned above with\na single model, which has not been accomplished yet. Evaluation results\ndemonstrate that the proposed MUSE model can successfully handle multiple tasks\nwith a single model.\n","authors":["Kohei Saijo","Wangyou Zhang","Zhong-Qiu Wang","Shinji Watanabe","Tetsunori Kobayashi","Tetsuji Ogawa"],"pdf_url":"https://arxiv.org/pdf/2310.08277v1.pdf","comment":"6 pages, 4 figures, 2 tables, accepted by ASRU2023"},{"id":"http://arxiv.org/abs/2310.08225v1","updated":"2023-10-12T11:17:40Z","published":"2023-10-12T11:17:40Z","title":"Fast Word Error Rate Estimation Using Self-Supervised Representations\n  For Speech And Text","summary":"  The quality of automatic speech recognition (ASR) is typically measured by\nword error rate (WER). WER estimation is a task aiming to predict the WER of an\nASR system, given a speech utterance and a transcription. This task has gained\nincreasing attention while advanced ASR systems are trained on large amounts of\ndata. In this case, WER estimation becomes necessary in many scenarios, for\nexample, selecting training data with unknown transcription quality or\nestimating the testing performance of an ASR system without ground truth\ntranscriptions. Facing large amounts of data, the computation efficiency of a\nWER estimator becomes essential in practical applications. However, previous\nworks usually did not consider it as a priority. In this paper, a Fast WER\nestimator (Fe-WER) using self-supervised learning representation (SSLR) is\nintroduced. The estimator is built upon SSLR aggregated by average pooling. The\nresults show that Fe-WER outperformed the e-WER3 baseline relatively by 19.69%\nand 7.16% on Ted-Lium3 in both evaluation metrics of root mean square error and\nPearson correlation coefficient, respectively. Moreover, the estimation\nweighted by duration was 10.43% when the target was 10.88%. Lastly, the\ninference speed was about 4x in terms of a real-time factor.\n","authors":["Chanho Park","Chengsong Lu","Mingjie Chen","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2310.08225v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2306.00721v2","updated":"2023-10-12T10:32:01Z","published":"2023-06-01T14:22:55Z","title":"UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion\n  Model","summary":"  This paper introduces UnDiff, a diffusion probabilistic model capable of\nsolving various speech inverse tasks. Being once trained for speech waveform\ngeneration in an unconditional manner, it can be adapted to different tasks\nincluding degradation inversion, neural vocoding, and source separation. In\nthis paper, we, first, tackle the challenging problem of unconditional waveform\ngeneration by comparing different neural architectures and preconditioning\ndomains. After that, we demonstrate how the trained unconditional diffusion\ncould be adapted to different tasks of speech processing by the means of recent\ndevelopments in post-training conditioning of diffusion models. Finally, we\ndemonstrate the performance of the proposed technique on the tasks of bandwidth\nextension, declipping, vocoding, and speech source separation and compare it to\nthe baselines. The codes are publicly available.\n","authors":["Anastasiia Iashchenko","Pavel Andreev","Ivan Shchekotov","Nicholas Babaev","Dmitry Vetrov"],"pdf_url":"https://arxiv.org/pdf/2306.00721v2.pdf","comment":"Accepted to Interspeech 2023"},{"id":"http://arxiv.org/abs/2310.08132v1","updated":"2023-10-12T08:45:21Z","published":"2023-10-12T08:45:21Z","title":"On the Relevance of Phoneme Duration Variability of Synthesized Training\n  Data for Automatic Speech Recognition","summary":"  Synthetic data generated by text-to-speech (TTS) systems can be used to\nimprove automatic speech recognition (ASR) systems in low-resource or domain\nmismatch tasks. It has been shown that TTS-generated outputs still do not have\nthe same qualities as real data. In this work we focus on the temporal\nstructure of synthetic data and its relation to ASR training. By using a novel\noracle setup we show how much the degradation of synthetic data quality is\ninfluenced by duration modeling in non-autoregressive (NAR) TTS. To get\nreference phoneme durations we use two common alignment methods, a hidden\nMarkov Gaussian-mixture model (HMM-GMM) aligner and a neural connectionist\ntemporal classification (CTC) aligner. Using a simple algorithm based on random\nwalks we shift phoneme duration distributions of the TTS system closer to real\ndurations, resulting in an improvement of an ASR system using synthetic data in\na semi-supervised setting.\n","authors":["Nick Rossenbach","Benedikt Hilmes","Ralf Schl√ºter"],"pdf_url":"https://arxiv.org/pdf/2310.08132v1.pdf","comment":"To appear at ASRU 2023"},{"id":"http://arxiv.org/abs/2310.08104v1","updated":"2023-10-12T08:00:25Z","published":"2023-10-12T08:00:25Z","title":"Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and\n  Textually Described Voices","summary":"  Voice conversion aims to convert source speech into a target voice using\nrecordings of the target speaker as a reference. Newer models are producing\nincreasingly realistic output. But what happens when models are fed with\nnon-standard data, such as speech from a user with a speech impairment? We\ninvestigate how a recent voice conversion model performs on non-standard\ndownstream voice conversion tasks. We use a simple but robust approach called\nk-nearest neighbors voice conversion (kNN-VC). We look at four non-standard\napplications: stuttered voice conversion, cross-lingual voice conversion,\nmusical instrument conversion, and text-to-voice conversion. The latter\ninvolves converting to a target voice specified through a text description,\ne.g. \"a young man with a high-pitched voice\". Compared to an established\nbaseline, we find that kNN-VC retains high performance in stuttered and\ncross-lingual voice conversion. Results are more mixed for the musical\ninstrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some\ninstruments like drums but not on others. Nevertheless, this shows that voice\nconversion models - and kNN-VC in particular - are increasingly applicable in a\nrange of non-standard downstream tasks. But there are still limitations when\nsamples are very far from the training distribution. Code, samples, trained\nmodels: https://rf5.github.io/sacair2023-knnvc-demo/.\n","authors":["Matthew Baas","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2310.08104v1.pdf","comment":"11 pages, 1 figure, 5 tables. Accepted at SACAIR 2023"},{"id":"http://arxiv.org/abs/2309.02285v2","updated":"2023-10-12T03:05:36Z","published":"2023-09-05T14:45:27Z","title":"PromptTTS 2: Describing and Generating Voices with Text Prompt","summary":"  Speech conveys more information than text, as the same word can be uttered in\nvarious voices to convey diverse information. Compared to traditional\ntext-to-speech (TTS) methods relying on speech prompts (reference speech) for\nvoice variability, using text prompts (descriptions) is more user-friendly\nsince speech prompts can be hard to find or may not exist at all. TTS\napproaches based on the text prompt face two main challenges: 1) the\none-to-many problem, where not all details about voice variability can be\ndescribed in the text prompt, and 2) the limited availability of text prompt\ndatasets, where vendors and large cost of data labeling are required to write\ntext prompts for speech. In this work, we introduce PromptTTS 2 to address\nthese challenges with a variation network to provide variability information of\nvoice not captured by text prompts, and a prompt generation pipeline to utilize\nthe large language models (LLM) to compose high quality text prompts.\nSpecifically, the variation network predicts the representation extracted from\nthe reference speech (which contains full information about voice variability)\nbased on the text prompt representation. For the prompt generation pipeline, it\ngenerates text prompts for speech with a speech language understanding model to\nrecognize voice attributes (e.g., gender, speed) from speech and a large\nlanguage model to formulate text prompts based on the recognition results.\nExperiments on a large-scale (44K hours) speech dataset demonstrate that\ncompared to the previous works, PromptTTS 2 generates voices more consistent\nwith text prompts and supports the sampling of diverse voice variability,\nthereby offering users more choices on voice generation. Additionally, the\nprompt generation pipeline produces high-quality text prompts, eliminating the\nlarge labeling cost. The demo page of PromptTTS 2 is available online.\n","authors":["Yichong Leng","Zhifang Guo","Kai Shen","Xu Tan","Zeqian Ju","Yanqing Liu","Yufei Liu","Dongchao Yang","Leying Zhang","Kaitao Song","Lei He","Xiang-Yang Li","Sheng Zhao","Tao Qin","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2309.02285v2.pdf","comment":"Demo page: https://speechresearch.github.io/prompttts2"},{"id":"http://arxiv.org/abs/2310.08753v1","updated":"2023-10-12T22:43:38Z","published":"2023-10-12T22:43:38Z","title":"CompA: Addressing the Gap in Compositional Reasoning in Audio-Language\n  Models","summary":"  A fundamental characteristic of audio is its compositional nature.\nAudio-language models (ALMs) trained using a contrastive approach (e.g., CLAP)\nthat learns a shared representation between audio and language modalities have\nimproved performance in many downstream applications, including zero-shot audio\nclassification, audio retrieval, etc. However, the ability of these models to\neffectively perform compositional reasoning remains largely unexplored and\nnecessitates additional research. In this paper, we propose CompA, a collection\nof two expert-annotated benchmarks with a majority of real-world audio samples,\nto evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates\nhow well an ALM understands the order or occurrence of acoustic events in\naudio, and CompA-attribute evaluates attribute binding of acoustic events. An\ninstance from either benchmark consists of two audio-caption pairs, where both\naudios have the same acoustic events but with different compositions. An ALM is\nevaluated on how well it matches the right audio to the right caption. Using\nthis benchmark, we first show that current ALMs perform only marginally better\nthan random chance, thereby struggling with compositional reasoning. Next, we\npropose CompA-CLAP, where we fine-tune CLAP using a novel learning method to\nimprove its compositional reasoning abilities. To train CompA-CLAP, we first\npropose improvements to contrastive training with composition-aware hard\nnegatives, allowing for more focused training. Next, we propose a novel modular\ncontrastive loss that helps the model learn fine-grained compositional\nunderstanding and overcomes the acute scarcity of openly available\ncompositional audios. CompA-CLAP significantly improves over all our baseline\nmodels on the CompA benchmark, indicating its superior compositional reasoning\ncapabilities.\n","authors":["Sreyan Ghosh","Ashish Seth","Sonal Kumar","Utkarsh Tyagi","Chandra Kiran Evuru","S. Ramaneswaran","S. Sakshi","Oriol Nieto","Ramani Duraiswami","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2310.08753v1.pdf","comment":"Pre-print under review"},{"id":"http://arxiv.org/abs/2309.08730v2","updated":"2023-10-12T21:28:02Z","published":"2023-09-15T19:31:40Z","title":"MusiLingo: Bridging Music and Text with Pre-trained Language Models for\n  Music Captioning and Query Response","summary":"  Large Language Models (LLMs) have shown immense potential in multimodal\napplications, yet the convergence of textual and musical domains remains\nrelatively unexplored. To address this gap, we present MusiLingo, a novel\nsystem for music caption generation and music-related query responses.\nMusiLingo employs a single projection layer to align music representations from\nthe pre-trained frozen music audio model MERT with the frozen Vicuna-7B\nlanguage model (an adaption of LLaMA), bridging the gap between music audio and\ntextual contexts. We train it on an extensive music caption dataset and\nfine-tune it with instructional data. Due to the scarcity of high-quality music\nQ\\&A datasets, we created the Music Instruct (MI) dataset from captions in the\nMusicCaps datasets, tailored for open-ended music inquiries. Empirical\nevaluations demonstrate its competitive performance in generating music\ncaptions and composing music-related Q&A pairs.\n","authors":["Zihao Deng","Yinghao Ma","Yudong Liu","Rongchen Guo","Ge Zhang","Wenhu Chen","Wenhao Huang","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2309.08730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08715v1","updated":"2023-10-12T20:53:39Z","published":"2023-10-12T20:53:39Z","title":"Toward Joint Language Modeling for Speech Units and Text","summary":"  Speech and text are two major forms of human language. The research community\nhas been focusing on mapping speech to text or vice versa for many years.\nHowever, in the field of language modeling, very little effort has been made to\nmodel them jointly. In light of this, we explore joint language modeling for\nspeech units and text. Specifically, we compare different speech tokenizers to\ntransform continuous speech signals into discrete units and use different\nmethods to construct mixed speech-text data. We introduce automatic metrics to\nevaluate how well the joint LM mixes speech and text. We also fine-tune the LM\non downstream spoken language understanding (SLU) tasks with different\nmodalities (speech or text) and test its performance to assess the model's\nlearning of shared representations. Our results show that by mixing speech\nunits and text with our proposed mixing techniques, the joint LM improves over\na speech-only baseline on SLU tasks and shows zero-shot cross-modal\ntransferability.\n","authors":["Ju-Chieh Chou","Chung-Ming Chien","Wei-Ning Hsu","Karen Livescu","Arun Babu","Alexis Conneau","Alexei Baevski","Michael Auli"],"pdf_url":"https://arxiv.org/pdf/2310.08715v1.pdf","comment":"EMNLP findings 2023"},{"id":"http://arxiv.org/abs/2310.08696v1","updated":"2023-10-12T20:02:07Z","published":"2023-10-12T20:02:07Z","title":"End-to-end Online Speaker Diarization with Target Speaker Tracking","summary":"  This paper proposes an online target speaker voice activity detection system\nfor speaker diarization tasks, which does not require a priori knowledge from\nthe clustering-based diarization system to obtain the target speaker\nembeddings. By adapting the conventional target speaker voice activity\ndetection for real-time operation, this framework can identify speaker\nactivities using self-generated embeddings, resulting in consistent performance\nwithout permutation inconsistencies in the inference phase. During the\ninference process, we employ a front-end model to extract the frame-level\nspeaker embeddings for each coming block of a signal. Next, we predict the\ndetection state of each speaker based on these frame-level speaker embeddings\nand the previously estimated target speaker embedding. Then, the target speaker\nembeddings are updated by aggregating these frame-level speaker embeddings\naccording to the predictions in the current block. Our model predicts the\nresults for each block and updates the target speakers' embeddings until\nreaching the end of the signal. Experimental results show that the proposed\nmethod outperforms the offline clustering-based diarization system on the\nDIHARD III and AliMeeting datasets. The proposed method is further extended to\nmulti-channel data, which achieves similar performance with the\nstate-of-the-art offline diarization systems.\n","authors":["Weiqing Wang","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2310.08696v1.pdf","comment":"Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.07284v2","updated":"2023-10-12T01:40:37Z","published":"2023-10-11T08:17:54Z","title":"Typing to Listen at the Cocktail Party: Text-Guided Target Speaker\n  Extraction","summary":"  Humans possess an extraordinary ability to selectively focus on the sound\nsource of interest amidst complex acoustic environments, commonly referred to\nas cocktail party scenarios. In an attempt to replicate this remarkable\nauditory attention capability in machines, target speaker extraction (TSE)\nmodels have been developed. These models leverage the pre-registered cues of\nthe target speaker to extract the sound source of interest. However, the\neffectiveness of these models is hindered in real-world scenarios due to the\nunreliable or even absence of pre-registered cues. To address this limitation,\nthis study investigates the integration of natural language description to\nenhance the feasibility, controllability, and performance of existing TSE\nmodels. Specifically, we propose a model named LLM-TSE, wherein a large\nlanguage model (LLM) to extract useful semantic cues from the user's typed text\ninput. These cues can serve as independent extraction cues, task selectors to\ncontrol the TSE process, or complement the pre-registered cues. Our\nexperimental results demonstrate competitive performance when only text-based\ncues are presented, the effectiveness of using input text as a task selector,\nand a new state-of-the-art when combining text-based cues with pre-registered\ncues. To our knowledge, this is the first study to successfully incorporate\nLLMs to guide target speaker extraction, which can be a cornerstone for\ncocktail party problem research.\n","authors":["Xiang Hao","Jibin Wu","Jianwei Yu","Chenglin Xu","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07284v2.pdf","comment":"Under review, https://github.com/haoxiangsnr/llm-tse"},{"id":"http://arxiv.org/abs/2310.07246v2","updated":"2023-10-12T05:49:52Z","published":"2023-10-11T07:23:27Z","title":"Vec-Tok Speech: speech vectorization and tokenization for neural speech\n  generation","summary":"  Language models (LMs) have recently flourished in natural language processing\nand computer vision, generating high-fidelity texts or images in various tasks.\nIn contrast, the current speech generative models are still struggling\nregarding speech quality and task generalization. This paper presents Vec-Tok\nSpeech, an extensible framework that resembles multiple speech generation\ntasks, generating expressive and high-fidelity speech. Specifically, we propose\na novel speech codec based on speech vectors and semantic tokens. Speech\nvectors contain acoustic details contributing to high-fidelity speech\nreconstruction, while semantic tokens focus on the linguistic content of\nspeech, facilitating language modeling. Based on the proposed speech codec,\nVec-Tok Speech leverages an LM to undertake the core of speech generation.\nMoreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and\nbit rate for lower exposure bias and longer context coverage, improving the\nperformance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual\nzero-shot voice conversion (VC), zero-shot speaking style transfer\ntext-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising,\nand speaker de-identification and anonymization. Experiments show that Vec-Tok\nSpeech, built on 50k hours of speech, performs better than other SOTA models.\nCode will be available at https://github.com/BakerBunker/VecTok .\n","authors":["Xinfa Zhu","Yuanjun Lv","Yi Lei","Tao Li","Wendi He","Hongbin Zhou","Heng Lu","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2310.07246v2.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.05354v2","updated":"2023-10-12T14:22:49Z","published":"2023-10-09T02:31:05Z","title":"An Initial Investigation of Neural Replay Simulator for Over-the-Air\n  Adversarial Perturbations to Automatic Speaker Verification","summary":"  Deep Learning has advanced Automatic Speaker Verification (ASV) in the past\nfew years. Although it is known that deep learning-based ASV systems are\nvulnerable to adversarial examples in digital access, there are few studies on\nadversarial attacks in the context of physical access, where a replay process\n(i.e., over the air) is involved. An over-the-air attack involves a\nloudspeaker, a microphone, and a replaying environment that impacts the\nmovement of the sound wave. Our initial experiment confirms that the replay\nprocess impacts the effectiveness of the over-the-air attack performance. This\nstudy performs an initial investigation towards utilizing a neural replay\nsimulator to improve over-the-air adversarial attack robustness. This is\nachieved by using a neural waveform synthesizer to simulate the replay process\nwhen estimating the adversarial perturbations. Experiments conducted on the\nASVspoof2019 dataset confirm that the neural replay simulator can considerably\nincrease the success rates of over-the-air adversarial attacks. This raises the\nconcern for adversarial attacks on speaker verification in physical access\napplications.\n","authors":["Jiaqi Li","Li Wang","Liumeng Xue","Lei Wang","Zhizheng Wu"],"pdf_url":"https://arxiv.org/pdf/2310.05354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09726v2","updated":"2023-10-12T17:57:51Z","published":"2021-12-17T19:22:01Z","title":"Soundify: Matching Sound Effects to Video","summary":"  In the art of video editing, sound helps add character to an object and\nimmerse the viewer within a space. Through formative interviews with\nprofessional editors (N=10), we found that the task of adding sounds to video\ncan be challenging. This paper presents Soundify, a system that assists editors\nin matching sounds to video. Given a video, Soundify identifies matching\nsounds, synchronizes the sounds to the video, and dynamically adjusts panning\nand volume to create spatial audio. In a human evaluation study (N=889), we\nshow that Soundify is capable of matching sounds to video out-of-the-box for a\ndiverse range of audio categories. In a within-subjects expert study (N=12), we\ndemonstrate the usefulness of Soundify in helping video editors match sounds to\nvideo with lighter workload, reduced task completion time, and improved\nusability.\n","authors":["David Chuan-En Lin","Anastasis Germanidis","Crist√≥bal Valenzuela","Yining Shi","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2112.09726v2.pdf","comment":"Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;\n  Online demo: https://soundify.cc"},{"id":"http://arxiv.org/abs/2310.08497v1","updated":"2023-10-12T16:56:37Z","published":"2023-10-12T16:56:37Z","title":"Impact of time and note duration tokenizations on deep learning symbolic\n  music modeling","summary":"  Symbolic music is widely used in various deep learning tasks, including\ngeneration, transcription, synthesis, and Music Information Retrieval (MIR). It\nis mostly employed with discrete models like Transformers, which require music\nto be tokenized, i.e., formatted into sequences of distinct elements called\ntokens. Tokenization can be performed in different ways. As Transformer can\nstruggle at reasoning, but capture more easily explicit information, it is\nimportant to study how the way the information is represented for such model\nimpact their performances. In this work, we analyze the common tokenization\nmethods and experiment with time and note duration representations. We compare\nthe performances of these two impactful criteria on several tasks, including\ncomposer and emotion classification, music generation, and sequence\nrepresentation learning. We demonstrate that explicit information leads to\nbetter results depending on the task.\n","authors":["Nathan Fradet","Nicolas Gutowski","Fabien Chhel","Jean-Pierre Briot"],"pdf_url":"https://arxiv.org/pdf/2310.08497v1.pdf","comment":"ISMIR 2023"},{"id":"http://arxiv.org/abs/2310.08464v1","updated":"2023-10-12T16:23:28Z","published":"2023-10-12T16:23:28Z","title":"Crowdsourced and Automatic Speech Prominence Estimation","summary":"  The prominence of a spoken word is the degree to which an average native\nlistener perceives the word as salient or emphasized relative to its context.\nSpeech prominence estimation is the process of assigning a numeric value to the\nprominence of each word in an utterance. These prominence labels are useful for\nlinguistic analysis, as well as training automated systems to perform\nemphasis-controlled text-to-speech or emotion recognition. Manually annotating\nprominence is time-consuming and expensive, which motivates the development of\nautomated methods for speech prominence estimation. However, developing such an\nautomated system using machine-learning methods requires human-annotated\ntraining data. Using our system for acquiring such human annotations, we\ncollect and open-source crowdsourced annotations of a portion of the LibriTTS\ndataset. We use these annotations as ground truth to train a neural speech\nprominence estimator that generalizes to unseen speakers, datasets, and\nspeaking styles. We investigate design decisions for neural prominence\nestimation as well as how neural prominence estimation improves as a function\nof two key factors of annotation cost: dataset size and the number of\nannotations per utterance.\n","authors":["Max Morrison","Pranav Pawar","Nathan Pruyne","Jennifer Cole","Bryan Pardo"],"pdf_url":"https://arxiv.org/pdf/2310.08464v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2309.04946v2","updated":"2023-10-12T15:04:30Z","published":"2023-09-10T06:33:17Z","title":"Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation","summary":"  Audio-driven talking-head synthesis is a popular research topic for virtual\nhuman-related applications. However, the inflexibility and inefficiency of\nexisting methods, which necessitate expensive end-to-end training to transfer\nemotions from guidance videos to talking-head predictions, are significant\nlimitations. In this work, we propose the Emotional Adaptation for Audio-driven\nTalking-head (EAT) method, which transforms emotion-agnostic talking-head\nmodels into emotion-controllable ones in a cost-effective and efficient manner\nthrough parameter-efficient adaptations. Our approach utilizes a pretrained\nemotion-agnostic talking-head transformer and introduces three lightweight\nadaptations (the Deep Emotional Prompts, Emotional Deformation Network, and\nEmotional Adaptation Module) from different perspectives to enable precise and\nrealistic emotion controls. Our experiments demonstrate that our approach\nachieves state-of-the-art performance on widely-used benchmarks, including LRW\nand MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable\ngeneralization ability, even in scenarios where emotional training videos are\nscarce or nonexistent. Project website: https://yuangan.github.io/eat/\n","authors":["Yuan Gan","Zongxin Yang","Xihang Yue","Lingyun Sun","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2309.04946v2.pdf","comment":"Accepted to ICCV 2023. Project page: https://yuangan.github.io/eat/"},{"id":"http://arxiv.org/abs/2310.08338v1","updated":"2023-10-12T13:56:42Z","published":"2023-10-12T13:56:42Z","title":"A cry for help: Early detection of brain injury in newborns","summary":"  Since the 1960s, neonatal clinicians have known that newborns suffering from\ncertain neurological conditions exhibit altered crying patterns such as the\nhigh-pitched cry in birth asphyxia. Despite an annual burden of over 1.5\nmillion infant deaths and disabilities, early detection of neonatal brain\ninjuries due to asphyxia remains a challenge, particularly in developing\ncountries where the majority of births are not attended by a trained physician.\nHere we report on the first inter-continental clinical study to demonstrate\nthat neonatal brain injury can be reliably determined from recorded infant\ncries using an AI algorithm we call Roseline. Previous and recent work has been\nlimited by the lack of a large, high-quality clinical database of cry\nrecordings, constraining the application of state-of-the-art machine learning.\nWe develop a new training methodology for audio-based pathology detection\nmodels and evaluate this system on a large database of newborn cry sounds\nacquired from geographically diverse settings -- 5 hospitals across 3\ncontinents. Our system extracts interpretable acoustic biomarkers that support\nclinical decisions and is able to accurately detect neurological injury from\nnewborns' cries with an AUC of 92.5% (88.7% sensitivity at 80% specificity).\nCry-based neurological monitoring opens the door for low-cost, easy-to-use,\nnon-invasive and contact-free screening of at-risk babies, especially when\nintegrated into simple devices like smartphones or neonatal ICU monitors. This\nwould provide a reliable tool where there are no alternatives, but also curtail\nthe need to regularly exert newborns to physically-exhausting or\nradiation-exposing assessments such as brain CT scans. This work sets the stage\nfor embracing the infant cry as a vital sign and indicates the potential of\nAI-driven sound monitoring for the future of affordable healthcare.\n","authors":["Charles C. Onu","Samantha Latremouille","Arsenii Gorin","Junhao Wang","Uchenna Ekwochi","Peter O. Ubuane","Omolara A. Kehinde","Muhammad A. Salisu","Datonye Briggs","Yoshua Bengio","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2310.08338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08303v1","updated":"2023-10-12T13:09:40Z","published":"2023-10-12T13:09:40Z","title":"Multimodal Variational Auto-encoder based Audio-Visual Segmentation","summary":"  We propose an Explicit Conditional Multimodal Variational Auto-Encoder\n(ECMVAE) for audio-visual segmentation (AVS), aiming to segment sound sources\nin the video sequence. Existing AVS methods focus on implicit feature fusion\nstrategies, where models are trained to fit the discrete samples in the\ndataset. With a limited and less diverse dataset, the resulting performance is\nusually unsatisfactory. In contrast, we address this problem from an effective\nrepresentation learning perspective, aiming to model the contribution of each\nmodality explicitly. Specifically, we find that audio contains critical\ncategory information of the sound producers, and visual data provides candidate\nsound producer(s). Their shared information corresponds to the target sound\nproducer(s) shown in the visual data. In this case, cross-modal shared\nrepresentation learning is especially important for AVS. To achieve this, our\nECMVAE factorizes the representations of each modality with a modality-shared\nrepresentation and a modality-specific representation. An orthogonality\nconstraint is applied between the shared and specific representations to\nmaintain the exclusive attribute of the factorized latent code. Further, a\nmutual information maximization regularizer is introduced to achieve extensive\nexploration of each modality. Quantitative and qualitative evaluations on the\nAVSBench demonstrate the effectiveness of our approach, leading to a new\nstate-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging\nMS3 subset for multiple sound source segmentation.\n","authors":["Yuxin Mao","Jing Zhang","Mochu Xiang","Yiran Zhong","Yuchao Dai"],"pdf_url":"https://arxiv.org/pdf/2310.08303v1.pdf","comment":"Accepted by ICCV2023,Project\n  page(https://npucvr.github.io/MMVAE-AVS),Code(https://github.com/OpenNLPLab/MMVAE-AVS)"},{"id":"http://arxiv.org/abs/2310.08277v1","updated":"2023-10-12T12:28:51Z","published":"2023-10-12T12:28:51Z","title":"A Single Speech Enhancement Model Unifying Dereverberation, Denoising,\n  Speaker Counting, Separation, and Extraction","summary":"  We propose a multi-task universal speech enhancement (MUSE) model that can\nperform five speech enhancement (SE) tasks: dereverberation, denoising, speech\nseparation (SS), target speaker extraction (TSE), and speaker counting. This is\nachieved by integrating two modules into an SE model: 1) an internal separation\nmodule that does both speaker counting and separation; and 2) a TSE module that\nextracts the target speech from the internal separation outputs using target\nspeaker cues. The model is trained to perform TSE if the target speaker cue is\ngiven and SS otherwise. By training the model to remove noise and\nreverberation, we allow the model to tackle the five tasks mentioned above with\na single model, which has not been accomplished yet. Evaluation results\ndemonstrate that the proposed MUSE model can successfully handle multiple tasks\nwith a single model.\n","authors":["Kohei Saijo","Wangyou Zhang","Zhong-Qiu Wang","Shinji Watanabe","Tetsunori Kobayashi","Tetsuji Ogawa"],"pdf_url":"https://arxiv.org/pdf/2310.08277v1.pdf","comment":"6 pages, 4 figures, 2 tables, accepted by ASRU2023"},{"id":"http://arxiv.org/abs/2310.08225v1","updated":"2023-10-12T11:17:40Z","published":"2023-10-12T11:17:40Z","title":"Fast Word Error Rate Estimation Using Self-Supervised Representations\n  For Speech And Text","summary":"  The quality of automatic speech recognition (ASR) is typically measured by\nword error rate (WER). WER estimation is a task aiming to predict the WER of an\nASR system, given a speech utterance and a transcription. This task has gained\nincreasing attention while advanced ASR systems are trained on large amounts of\ndata. In this case, WER estimation becomes necessary in many scenarios, for\nexample, selecting training data with unknown transcription quality or\nestimating the testing performance of an ASR system without ground truth\ntranscriptions. Facing large amounts of data, the computation efficiency of a\nWER estimator becomes essential in practical applications. However, previous\nworks usually did not consider it as a priority. In this paper, a Fast WER\nestimator (Fe-WER) using self-supervised learning representation (SSLR) is\nintroduced. The estimator is built upon SSLR aggregated by average pooling. The\nresults show that Fe-WER outperformed the e-WER3 baseline relatively by 19.69%\nand 7.16% on Ted-Lium3 in both evaluation metrics of root mean square error and\nPearson correlation coefficient, respectively. Moreover, the estimation\nweighted by duration was 10.43% when the target was 10.88%. Lastly, the\ninference speed was about 4x in terms of a real-time factor.\n","authors":["Chanho Park","Chengsong Lu","Mingjie Chen","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2310.08225v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2306.00721v2","updated":"2023-10-12T10:32:01Z","published":"2023-06-01T14:22:55Z","title":"UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion\n  Model","summary":"  This paper introduces UnDiff, a diffusion probabilistic model capable of\nsolving various speech inverse tasks. Being once trained for speech waveform\ngeneration in an unconditional manner, it can be adapted to different tasks\nincluding degradation inversion, neural vocoding, and source separation. In\nthis paper, we, first, tackle the challenging problem of unconditional waveform\ngeneration by comparing different neural architectures and preconditioning\ndomains. After that, we demonstrate how the trained unconditional diffusion\ncould be adapted to different tasks of speech processing by the means of recent\ndevelopments in post-training conditioning of diffusion models. Finally, we\ndemonstrate the performance of the proposed technique on the tasks of bandwidth\nextension, declipping, vocoding, and speech source separation and compare it to\nthe baselines. The codes are publicly available.\n","authors":["Anastasiia Iashchenko","Pavel Andreev","Ivan Shchekotov","Nicholas Babaev","Dmitry Vetrov"],"pdf_url":"https://arxiv.org/pdf/2306.00721v2.pdf","comment":"Accepted to Interspeech 2023"},{"id":"http://arxiv.org/abs/2310.08132v1","updated":"2023-10-12T08:45:21Z","published":"2023-10-12T08:45:21Z","title":"On the Relevance of Phoneme Duration Variability of Synthesized Training\n  Data for Automatic Speech Recognition","summary":"  Synthetic data generated by text-to-speech (TTS) systems can be used to\nimprove automatic speech recognition (ASR) systems in low-resource or domain\nmismatch tasks. It has been shown that TTS-generated outputs still do not have\nthe same qualities as real data. In this work we focus on the temporal\nstructure of synthetic data and its relation to ASR training. By using a novel\noracle setup we show how much the degradation of synthetic data quality is\ninfluenced by duration modeling in non-autoregressive (NAR) TTS. To get\nreference phoneme durations we use two common alignment methods, a hidden\nMarkov Gaussian-mixture model (HMM-GMM) aligner and a neural connectionist\ntemporal classification (CTC) aligner. Using a simple algorithm based on random\nwalks we shift phoneme duration distributions of the TTS system closer to real\ndurations, resulting in an improvement of an ASR system using synthetic data in\na semi-supervised setting.\n","authors":["Nick Rossenbach","Benedikt Hilmes","Ralf Schl√ºter"],"pdf_url":"https://arxiv.org/pdf/2310.08132v1.pdf","comment":"To appear at ASRU 2023"},{"id":"http://arxiv.org/abs/2310.08104v1","updated":"2023-10-12T08:00:25Z","published":"2023-10-12T08:00:25Z","title":"Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and\n  Textually Described Voices","summary":"  Voice conversion aims to convert source speech into a target voice using\nrecordings of the target speaker as a reference. Newer models are producing\nincreasingly realistic output. But what happens when models are fed with\nnon-standard data, such as speech from a user with a speech impairment? We\ninvestigate how a recent voice conversion model performs on non-standard\ndownstream voice conversion tasks. We use a simple but robust approach called\nk-nearest neighbors voice conversion (kNN-VC). We look at four non-standard\napplications: stuttered voice conversion, cross-lingual voice conversion,\nmusical instrument conversion, and text-to-voice conversion. The latter\ninvolves converting to a target voice specified through a text description,\ne.g. \"a young man with a high-pitched voice\". Compared to an established\nbaseline, we find that kNN-VC retains high performance in stuttered and\ncross-lingual voice conversion. Results are more mixed for the musical\ninstrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some\ninstruments like drums but not on others. Nevertheless, this shows that voice\nconversion models - and kNN-VC in particular - are increasingly applicable in a\nrange of non-standard downstream tasks. But there are still limitations when\nsamples are very far from the training distribution. Code, samples, trained\nmodels: https://rf5.github.io/sacair2023-knnvc-demo/.\n","authors":["Matthew Baas","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2310.08104v1.pdf","comment":"11 pages, 1 figure, 5 tables. Accepted at SACAIR 2023"},{"id":"http://arxiv.org/abs/2309.02285v2","updated":"2023-10-12T03:05:36Z","published":"2023-09-05T14:45:27Z","title":"PromptTTS 2: Describing and Generating Voices with Text Prompt","summary":"  Speech conveys more information than text, as the same word can be uttered in\nvarious voices to convey diverse information. Compared to traditional\ntext-to-speech (TTS) methods relying on speech prompts (reference speech) for\nvoice variability, using text prompts (descriptions) is more user-friendly\nsince speech prompts can be hard to find or may not exist at all. TTS\napproaches based on the text prompt face two main challenges: 1) the\none-to-many problem, where not all details about voice variability can be\ndescribed in the text prompt, and 2) the limited availability of text prompt\ndatasets, where vendors and large cost of data labeling are required to write\ntext prompts for speech. In this work, we introduce PromptTTS 2 to address\nthese challenges with a variation network to provide variability information of\nvoice not captured by text prompts, and a prompt generation pipeline to utilize\nthe large language models (LLM) to compose high quality text prompts.\nSpecifically, the variation network predicts the representation extracted from\nthe reference speech (which contains full information about voice variability)\nbased on the text prompt representation. For the prompt generation pipeline, it\ngenerates text prompts for speech with a speech language understanding model to\nrecognize voice attributes (e.g., gender, speed) from speech and a large\nlanguage model to formulate text prompts based on the recognition results.\nExperiments on a large-scale (44K hours) speech dataset demonstrate that\ncompared to the previous works, PromptTTS 2 generates voices more consistent\nwith text prompts and supports the sampling of diverse voice variability,\nthereby offering users more choices on voice generation. Additionally, the\nprompt generation pipeline produces high-quality text prompts, eliminating the\nlarge labeling cost. The demo page of PromptTTS 2 is available online.\n","authors":["Yichong Leng","Zhifang Guo","Kai Shen","Xu Tan","Zeqian Ju","Yanqing Liu","Yufei Liu","Dongchao Yang","Leying Zhang","Kaitao Song","Lei He","Xiang-Yang Li","Sheng Zhao","Tao Qin","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2309.02285v2.pdf","comment":"Demo page: https://speechresearch.github.io/prompttts2"},{"id":"http://arxiv.org/abs/2310.08753v1","updated":"2023-10-12T22:43:38Z","published":"2023-10-12T22:43:38Z","title":"CompA: Addressing the Gap in Compositional Reasoning in Audio-Language\n  Models","summary":"  A fundamental characteristic of audio is its compositional nature.\nAudio-language models (ALMs) trained using a contrastive approach (e.g., CLAP)\nthat learns a shared representation between audio and language modalities have\nimproved performance in many downstream applications, including zero-shot audio\nclassification, audio retrieval, etc. However, the ability of these models to\neffectively perform compositional reasoning remains largely unexplored and\nnecessitates additional research. In this paper, we propose CompA, a collection\nof two expert-annotated benchmarks with a majority of real-world audio samples,\nto evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates\nhow well an ALM understands the order or occurrence of acoustic events in\naudio, and CompA-attribute evaluates attribute binding of acoustic events. An\ninstance from either benchmark consists of two audio-caption pairs, where both\naudios have the same acoustic events but with different compositions. An ALM is\nevaluated on how well it matches the right audio to the right caption. Using\nthis benchmark, we first show that current ALMs perform only marginally better\nthan random chance, thereby struggling with compositional reasoning. Next, we\npropose CompA-CLAP, where we fine-tune CLAP using a novel learning method to\nimprove its compositional reasoning abilities. To train CompA-CLAP, we first\npropose improvements to contrastive training with composition-aware hard\nnegatives, allowing for more focused training. Next, we propose a novel modular\ncontrastive loss that helps the model learn fine-grained compositional\nunderstanding and overcomes the acute scarcity of openly available\ncompositional audios. CompA-CLAP significantly improves over all our baseline\nmodels on the CompA benchmark, indicating its superior compositional reasoning\ncapabilities.\n","authors":["Sreyan Ghosh","Ashish Seth","Sonal Kumar","Utkarsh Tyagi","Chandra Kiran Evuru","S. Ramaneswaran","S. Sakshi","Oriol Nieto","Ramani Duraiswami","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2310.08753v1.pdf","comment":"Pre-print under review"},{"id":"http://arxiv.org/abs/2309.08730v2","updated":"2023-10-12T21:28:02Z","published":"2023-09-15T19:31:40Z","title":"MusiLingo: Bridging Music and Text with Pre-trained Language Models for\n  Music Captioning and Query Response","summary":"  Large Language Models (LLMs) have shown immense potential in multimodal\napplications, yet the convergence of textual and musical domains remains\nrelatively unexplored. To address this gap, we present MusiLingo, a novel\nsystem for music caption generation and music-related query responses.\nMusiLingo employs a single projection layer to align music representations from\nthe pre-trained frozen music audio model MERT with the frozen Vicuna-7B\nlanguage model (an adaption of LLaMA), bridging the gap between music audio and\ntextual contexts. We train it on an extensive music caption dataset and\nfine-tune it with instructional data. Due to the scarcity of high-quality music\nQ\\&A datasets, we created the Music Instruct (MI) dataset from captions in the\nMusicCaps datasets, tailored for open-ended music inquiries. Empirical\nevaluations demonstrate its competitive performance in generating music\ncaptions and composing music-related Q&A pairs.\n","authors":["Zihao Deng","Yinghao Ma","Yudong Liu","Rongchen Guo","Ge Zhang","Wenhu Chen","Wenhao Huang","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2309.08730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08715v1","updated":"2023-10-12T20:53:39Z","published":"2023-10-12T20:53:39Z","title":"Toward Joint Language Modeling for Speech Units and Text","summary":"  Speech and text are two major forms of human language. The research community\nhas been focusing on mapping speech to text or vice versa for many years.\nHowever, in the field of language modeling, very little effort has been made to\nmodel them jointly. In light of this, we explore joint language modeling for\nspeech units and text. Specifically, we compare different speech tokenizers to\ntransform continuous speech signals into discrete units and use different\nmethods to construct mixed speech-text data. We introduce automatic metrics to\nevaluate how well the joint LM mixes speech and text. We also fine-tune the LM\non downstream spoken language understanding (SLU) tasks with different\nmodalities (speech or text) and test its performance to assess the model's\nlearning of shared representations. Our results show that by mixing speech\nunits and text with our proposed mixing techniques, the joint LM improves over\na speech-only baseline on SLU tasks and shows zero-shot cross-modal\ntransferability.\n","authors":["Ju-Chieh Chou","Chung-Ming Chien","Wei-Ning Hsu","Karen Livescu","Arun Babu","Alexis Conneau","Alexei Baevski","Michael Auli"],"pdf_url":"https://arxiv.org/pdf/2310.08715v1.pdf","comment":"EMNLP findings 2023"},{"id":"http://arxiv.org/abs/2310.08696v1","updated":"2023-10-12T20:02:07Z","published":"2023-10-12T20:02:07Z","title":"End-to-end Online Speaker Diarization with Target Speaker Tracking","summary":"  This paper proposes an online target speaker voice activity detection system\nfor speaker diarization tasks, which does not require a priori knowledge from\nthe clustering-based diarization system to obtain the target speaker\nembeddings. By adapting the conventional target speaker voice activity\ndetection for real-time operation, this framework can identify speaker\nactivities using self-generated embeddings, resulting in consistent performance\nwithout permutation inconsistencies in the inference phase. During the\ninference process, we employ a front-end model to extract the frame-level\nspeaker embeddings for each coming block of a signal. Next, we predict the\ndetection state of each speaker based on these frame-level speaker embeddings\nand the previously estimated target speaker embedding. Then, the target speaker\nembeddings are updated by aggregating these frame-level speaker embeddings\naccording to the predictions in the current block. Our model predicts the\nresults for each block and updates the target speakers' embeddings until\nreaching the end of the signal. Experimental results show that the proposed\nmethod outperforms the offline clustering-based diarization system on the\nDIHARD III and AliMeeting datasets. The proposed method is further extended to\nmulti-channel data, which achieves similar performance with the\nstate-of-the-art offline diarization systems.\n","authors":["Weiqing Wang","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2310.08696v1.pdf","comment":"Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing"}]},"2023-10-13T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2112.09726v3","updated":"2023-10-13T08:10:41Z","published":"2021-12-17T19:22:01Z","title":"Soundify: Matching Sound Effects to Video","summary":"  In the art of video editing, sound helps add character to an object and\nimmerse the viewer within a space. Through formative interviews with\nprofessional editors (N=10), we found that the task of adding sounds to video\ncan be challenging. This paper presents Soundify, a system that assists editors\nin matching sounds to video. Given a video, Soundify identifies matching\nsounds, synchronizes the sounds to the video, and dynamically adjusts panning\nand volume to create spatial audio. In a human evaluation study (N=889), we\nshow that Soundify is capable of matching sounds to video out-of-the-box for a\ndiverse range of audio categories. In a within-subjects expert study (N=12), we\ndemonstrate the usefulness of Soundify in helping video editors match sounds to\nvideo with lighter workload, reduced task completion time, and improved\nusability.\n","authors":["David Chuan-En Lin","Anastasis Germanidis","Crist√≥bal Valenzuela","Yining Shi","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2112.09726v3.pdf","comment":"Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;\n  Online demo: http://soundify.cc"},{"id":"http://arxiv.org/abs/2310.08338v2","updated":"2023-10-13T16:20:21Z","published":"2023-10-12T13:56:42Z","title":"A cry for help: Early detection of brain injury in newborns","summary":"  Since the 1960s, neonatal clinicians have known that newborns suffering from\ncertain neurological conditions exhibit altered crying patterns such as the\nhigh-pitched cry in birth asphyxia. Despite an annual burden of over 1.5\nmillion infant deaths and disabilities, early detection of neonatal brain\ninjuries due to asphyxia remains a challenge, particularly in developing\ncountries where the majority of births are not attended by a trained physician.\nHere we report on the first inter-continental clinical study to demonstrate\nthat neonatal brain injury can be reliably determined from recorded infant\ncries using an AI algorithm we call Roseline. Previous and recent work has been\nlimited by the lack of a large, high-quality clinical database of cry\nrecordings, constraining the application of state-of-the-art machine learning.\nWe develop a new training methodology for audio-based pathology detection\nmodels and evaluate this system on a large database of newborn cry sounds\nacquired from geographically diverse settings -- 5 hospitals across 3\ncontinents. Our system extracts interpretable acoustic biomarkers that support\nclinical decisions and is able to accurately detect neurological injury from\nnewborns' cries with an AUC of 92.5% (88.7% sensitivity at 80% specificity).\nCry-based neurological monitoring opens the door for low-cost, easy-to-use,\nnon-invasive and contact-free screening of at-risk babies, especially when\nintegrated into simple devices like smartphones or neonatal ICU monitors. This\nwould provide a reliable tool where there are no alternatives, but also curtail\nthe need to regularly exert newborns to physically-exhausting or\nradiation-exposing assessments such as brain CT scans. This work sets the stage\nfor embracing the infant cry as a vital sign and indicates the potential of\nAI-driven sound monitoring for the future of affordable healthcare.\n","authors":["Charles C. Onu","Samantha Latremouille","Arsenii Gorin","Junhao Wang","Uchenna Ekwochi","Peter O. Ubuane","Omolara A. Kehinde","Muhammad A. Salisu","Datonye Briggs","Yoshua Bengio","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2310.08338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08981v1","updated":"2023-10-13T09:57:09Z","published":"2023-10-13T09:57:09Z","title":"Low-latency Speech Enhancement via Speech Token Generation","summary":"  Existing deep learning based speech enhancement mainly employ a data-driven\napproach, which leverage large amounts of data with a variety of noise types to\nachieve noise removal from noisy signal. However, the high dependence on the\ndata limits its generalization on the unseen complex noises in real-life\nenvironment. In this paper, we focus on the low-latency scenario and regard\nspeech enhancement as a speech generation problem conditioned on the noisy\nsignal, where we generate clean speech instead of identifying and removing\nnoises. Specifically, we propose a conditional generative framework for speech\nenhancement, which models clean speech by acoustic codes of a neural speech\ncodec and generates the speech codes conditioned on past noisy frames in an\nauto-regressive way. Moreover, we propose an explicit-alignment approach to\nalign noisy frames with the generated speech tokens to improve the robustness\nand scalability to different input lengths. Different from other methods that\nleverage multiple stages to generate speech codes, we leverage a single-stage\nspeech generation approach based on the TF-Codec neural codec to achieve high\nspeech quality with low latency. Extensive results on both synthetic and\nreal-recorded test set show its superiority over data-driven approaches in\nterms of noise robustness and temporal speech coherence.\n","authors":["Huaying Xue","Xiulian Peng","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2310.08981v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2310.08950v1","updated":"2023-10-13T08:49:17Z","published":"2023-10-13T08:49:17Z","title":"Transformer-based Autoencoder with ID Constraint for Unsupervised\n  Anomalous Sound Detection","summary":"  Unsupervised anomalous sound detection (ASD) aims to detect unknown anomalous\nsounds of devices when only normal sound data is available. The autoencoder\n(AE) and self-supervised learning based methods are two mainstream methods.\nHowever, the AE-based methods could be limited as the feature learned from\nnormal sounds can also fit with anomalous sounds, reducing the ability of the\nmodel in detecting anomalies from sound. The self-supervised methods are not\nalways stable and perform differently, even for machines of the same type. In\naddition, the anomalous sound may be short-lived, making it even harder to\ndistinguish from normal sound. This paper proposes an ID constrained\nTransformer-based autoencoder (IDC-TransAE) architecture with weighted anomaly\nscore computation for unsupervised ASD. Machine ID is employed to constrain the\nlatent space of the Transformer-based autoencoder (TransAE) by introducing a\nsimple ID classifier to learn the difference in the distribution for the same\nmachine type and enhance the ability of the model in distinguishing anomalous\nsound. Moreover, weighted anomaly score computation is introduced to highlight\nthe anomaly scores of anomalous events that only appear for a short time.\nExperiments performed on DCASE 2020 Challenge Task2 development dataset\ndemonstrate the effectiveness and superiority of our proposed method.\n","authors":["Jian Guan","Youde Liu","Qiuqiang Kong","Feiyang Xiao","Qiaoxi Zhu","Jiantong Tian","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08950v1.pdf","comment":"Accepted by EURASIP Journal on Audio, Speech, and Music Processing"},{"id":"http://arxiv.org/abs/2310.08914v1","updated":"2023-10-13T07:38:03Z","published":"2023-10-13T07:38:03Z","title":"Differential Evolution Algorithm based Hyper-Parameters Selection of\n  Convolutional Neural Network for Speech Command Recognition","summary":"  Speech Command Recognition (SCR), which deals with identification of short\nuttered speech commands, is crucial for various applications, including IoT\ndevices and assistive technology. Despite the promise shown by Convolutional\nNeural Networks (CNNs) in SCR tasks, their efficacy relies heavily on\nhyper-parameter selection, which is typically laborious and time-consuming when\ndone manually. This paper introduces a hyper-parameter selection method for\nCNNs based on the Differential Evolution (DE) algorithm, aiming to enhance\nperformance in SCR tasks. Training and testing with the Google Speech Command\n(GSC) dataset, the proposed approach showed effectiveness in classifying speech\ncommands. Moreover, a comparative analysis with Genetic Algorithm based\nselections and other deep CNN (DCNN) models highlighted the efficiency of the\nproposed DE algorithm in hyper-parameter selection for CNNs in SCR tasks.\n","authors":["Sandipan Dhar","Anuvab Sen","Aritra Bandyopadhyay","Nanda Dulal Jana","Arjun Ghosh","Zahra Sarayloo"],"pdf_url":"https://arxiv.org/pdf/2310.08914v1.pdf","comment":"8 Pages, 7 Figures, 4 Tables, Accepted by the 15th International\n  Joint Conference on Computational Intelligence (IJCCI 2023), November 13-15,\n  2023, Rome, Italy"},{"id":"http://arxiv.org/abs/2310.08869v1","updated":"2023-10-13T05:37:29Z","published":"2023-10-13T05:37:29Z","title":"Learning to Behave Like Clean Speech: Dual-Branch Knowledge Distillation\n  for Noise-Robust Fake Audio Detection","summary":"  Most research in fake audio detection (FAD) focuses on improving performance\non standard noise-free datasets. However, in actual situations, there is\nusually noise interference, which will cause significant performance\ndegradation in FAD systems. To improve the noise robustness, we propose a\ndual-branch knowledge distillation fake audio detection (DKDFAD) method.\nSpecifically, a parallel data flow of the clean teacher branch and the noisy\nstudent branch is designed, and interactive fusion and response-based\nteacher-student paradigms are proposed to guide the training of noisy data from\nthe data distribution and decision-making perspectives. In the noise branch,\nspeech enhancement is first introduced for denoising, which reduces the\ninterference of strong noise. The proposed interactive fusion combines\ndenoising features and noise features to reduce the impact of speech distortion\nand seek consistency with the data distribution of clean branch. The\nteacher-student paradigm maps the student's decision space to the teacher's\ndecision space, making noisy speech behave as clean. In addition, a joint\ntraining method is used to optimize the two branches to achieve global\noptimality. Experimental results based on multiple datasets show that the\nproposed method performs well in noisy environments and maintains performance\nin cross-dataset experiments.\n","authors":["Cunhang Fan","Mingming Ding","Jianhua Tao","Ruibo Fu","Jiangyan Yi","Zhengqi Wen","Zhao Lv"],"pdf_url":"https://arxiv.org/pdf/2310.08869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09424v1","updated":"2023-10-13T22:07:33Z","published":"2023-10-13T22:07:33Z","title":"SALM: Speech-augmented Language Model with In-context Learning for\n  Speech Recognition and Translation","summary":"  We present a novel Speech Augmented Language Model (SALM) with {\\em\nmultitask} and {\\em in-context} learning capabilities. SALM comprises a frozen\ntext LLM, a audio encoder, a modality adapter module, and LoRA layers to\naccommodate speech input and associated task instructions. The unified SALM not\nonly achieves performance on par with task-specific Conformer baselines for\nAutomatic Speech Recognition (ASR) and Speech Translation (AST), but also\nexhibits zero-shot in-context learning capabilities, demonstrated through\nkeyword-boosting task for ASR and AST. Moreover, {\\em speech supervised\nin-context training} is proposed to bridge the gap between LLM training and\ndownstream speech tasks, which further boosts the in-context learning ability\nof speech-to-text models. Proposed model is open-sourced via NeMo toolkit.\n","authors":["Zhehuai Chen","He Huang","Andrei Andrusenko","Oleksii Hrinchuk","Krishna C. Puvvada","Jason Li","Subhankar Ghosh","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2310.09424v1.pdf","comment":"submit to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.09388v1","updated":"2023-10-13T20:17:44Z","published":"2023-10-13T20:17:44Z","title":"CORN: Co-Trained Full-Reference And No-Reference Audio Metrics","summary":"  Perceptual evaluation constitutes a crucial aspect of various\naudio-processing tasks. Full reference (FR) or similarity-based metrics rely on\nhigh-quality reference recordings, to which lower-quality or corrupted versions\nof the recording may be compared for evaluation. In contrast, no-reference (NR)\nmetrics evaluate a recording without relying on a reference. Both the FR and NR\napproaches exhibit advantages and drawbacks relative to each other. In this\npaper, we present a novel framework called CORN that amalgamates these dual\napproaches, concurrently training both FR and NR models together. After\ntraining, the models can be applied independently. We evaluate CORN by\npredicting several common objective metrics and across two different\narchitectures. The NR model trained using CORN has access to a reference\nrecording during training, and thus, as one would expect, it consistently\noutperforms baseline NR models trained independently. Perhaps even more\nremarkable is that the CORN FR model also outperforms its baseline counterpart,\neven though it relies on the same training data and the same model\narchitecture. Thus, a single training regime produces two independently useful\nmodels, each outperforming independently trained models.\n","authors":["Pranay Manocha","Donald Williamson","Adam Finkelstein"],"pdf_url":"https://arxiv.org/pdf/2310.09388v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2112.09726v3","updated":"2023-10-13T08:10:41Z","published":"2021-12-17T19:22:01Z","title":"Soundify: Matching Sound Effects to Video","summary":"  In the art of video editing, sound helps add character to an object and\nimmerse the viewer within a space. Through formative interviews with\nprofessional editors (N=10), we found that the task of adding sounds to video\ncan be challenging. This paper presents Soundify, a system that assists editors\nin matching sounds to video. Given a video, Soundify identifies matching\nsounds, synchronizes the sounds to the video, and dynamically adjusts panning\nand volume to create spatial audio. In a human evaluation study (N=889), we\nshow that Soundify is capable of matching sounds to video out-of-the-box for a\ndiverse range of audio categories. In a within-subjects expert study (N=12), we\ndemonstrate the usefulness of Soundify in helping video editors match sounds to\nvideo with lighter workload, reduced task completion time, and improved\nusability.\n","authors":["David Chuan-En Lin","Anastasis Germanidis","Crist√≥bal Valenzuela","Yining Shi","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2112.09726v3.pdf","comment":"Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;\n  Online demo: http://soundify.cc"},{"id":"http://arxiv.org/abs/2310.08338v2","updated":"2023-10-13T16:20:21Z","published":"2023-10-12T13:56:42Z","title":"A cry for help: Early detection of brain injury in newborns","summary":"  Since the 1960s, neonatal clinicians have known that newborns suffering from\ncertain neurological conditions exhibit altered crying patterns such as the\nhigh-pitched cry in birth asphyxia. Despite an annual burden of over 1.5\nmillion infant deaths and disabilities, early detection of neonatal brain\ninjuries due to asphyxia remains a challenge, particularly in developing\ncountries where the majority of births are not attended by a trained physician.\nHere we report on the first inter-continental clinical study to demonstrate\nthat neonatal brain injury can be reliably determined from recorded infant\ncries using an AI algorithm we call Roseline. Previous and recent work has been\nlimited by the lack of a large, high-quality clinical database of cry\nrecordings, constraining the application of state-of-the-art machine learning.\nWe develop a new training methodology for audio-based pathology detection\nmodels and evaluate this system on a large database of newborn cry sounds\nacquired from geographically diverse settings -- 5 hospitals across 3\ncontinents. Our system extracts interpretable acoustic biomarkers that support\nclinical decisions and is able to accurately detect neurological injury from\nnewborns' cries with an AUC of 92.5% (88.7% sensitivity at 80% specificity).\nCry-based neurological monitoring opens the door for low-cost, easy-to-use,\nnon-invasive and contact-free screening of at-risk babies, especially when\nintegrated into simple devices like smartphones or neonatal ICU monitors. This\nwould provide a reliable tool where there are no alternatives, but also curtail\nthe need to regularly exert newborns to physically-exhausting or\nradiation-exposing assessments such as brain CT scans. This work sets the stage\nfor embracing the infant cry as a vital sign and indicates the potential of\nAI-driven sound monitoring for the future of affordable healthcare.\n","authors":["Charles C. Onu","Samantha Latremouille","Arsenii Gorin","Junhao Wang","Uchenna Ekwochi","Peter O. Ubuane","Omolara A. Kehinde","Muhammad A. Salisu","Datonye Briggs","Yoshua Bengio","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2310.08338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06566v4","updated":"2023-10-13T12:35:21Z","published":"2023-03-12T04:29:19Z","title":"ICASSP 2023 Speech Signal Improvement Challenge","summary":"  The ICASSP 2023 Speech Signal Improvement Challenge is intended to stimulate\nresearch in the area of improving the speech signal quality in communication\nsystems. The speech signal quality can be measured with SIG in ITU-T P.835 and\nis still a top issue in audio communication and conferencing systems. For\nexample, in the ICASSP 2022 Deep Noise Suppression challenge, the improvement\nin the background and overall quality is impressive, but the improvement in the\nspeech signal is not statistically significant. To improve the speech signal\nthe following speech impairment areas must be addressed: coloration,\ndiscontinuity, loudness, reverberation, and noise. A training and test set was\nprovided for the challenge, and the winners were determined using an extended\ncrowdsourced implementation of ITU-T P.804's listening phase. The results show\nsignificant improvement was made across all measured dimensions of speech\nquality.\n","authors":["Ross Cutler","Ando Saabas","Babak Naderi","Nicolae-CƒÉtƒÉlin Ristea","Sebastian Braun","Solomiya Branets"],"pdf_url":"https://arxiv.org/pdf/2303.06566v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08981v1","updated":"2023-10-13T09:57:09Z","published":"2023-10-13T09:57:09Z","title":"Low-latency Speech Enhancement via Speech Token Generation","summary":"  Existing deep learning based speech enhancement mainly employ a data-driven\napproach, which leverage large amounts of data with a variety of noise types to\nachieve noise removal from noisy signal. However, the high dependence on the\ndata limits its generalization on the unseen complex noises in real-life\nenvironment. In this paper, we focus on the low-latency scenario and regard\nspeech enhancement as a speech generation problem conditioned on the noisy\nsignal, where we generate clean speech instead of identifying and removing\nnoises. Specifically, we propose a conditional generative framework for speech\nenhancement, which models clean speech by acoustic codes of a neural speech\ncodec and generates the speech codes conditioned on past noisy frames in an\nauto-regressive way. Moreover, we propose an explicit-alignment approach to\nalign noisy frames with the generated speech tokens to improve the robustness\nand scalability to different input lengths. Different from other methods that\nleverage multiple stages to generate speech codes, we leverage a single-stage\nspeech generation approach based on the TF-Codec neural codec to achieve high\nspeech quality with low latency. Extensive results on both synthetic and\nreal-recorded test set show its superiority over data-driven approaches in\nterms of noise robustness and temporal speech coherence.\n","authors":["Huaying Xue","Xiulian Peng","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2310.08981v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2310.08950v1","updated":"2023-10-13T08:49:17Z","published":"2023-10-13T08:49:17Z","title":"Transformer-based Autoencoder with ID Constraint for Unsupervised\n  Anomalous Sound Detection","summary":"  Unsupervised anomalous sound detection (ASD) aims to detect unknown anomalous\nsounds of devices when only normal sound data is available. The autoencoder\n(AE) and self-supervised learning based methods are two mainstream methods.\nHowever, the AE-based methods could be limited as the feature learned from\nnormal sounds can also fit with anomalous sounds, reducing the ability of the\nmodel in detecting anomalies from sound. The self-supervised methods are not\nalways stable and perform differently, even for machines of the same type. In\naddition, the anomalous sound may be short-lived, making it even harder to\ndistinguish from normal sound. This paper proposes an ID constrained\nTransformer-based autoencoder (IDC-TransAE) architecture with weighted anomaly\nscore computation for unsupervised ASD. Machine ID is employed to constrain the\nlatent space of the Transformer-based autoencoder (TransAE) by introducing a\nsimple ID classifier to learn the difference in the distribution for the same\nmachine type and enhance the ability of the model in distinguishing anomalous\nsound. Moreover, weighted anomaly score computation is introduced to highlight\nthe anomaly scores of anomalous events that only appear for a short time.\nExperiments performed on DCASE 2020 Challenge Task2 development dataset\ndemonstrate the effectiveness and superiority of our proposed method.\n","authors":["Jian Guan","Youde Liu","Qiuqiang Kong","Feiyang Xiao","Qiaoxi Zhu","Jiantong Tian","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08950v1.pdf","comment":"Accepted by EURASIP Journal on Audio, Speech, and Music Processing"},{"id":"http://arxiv.org/abs/2310.08914v1","updated":"2023-10-13T07:38:03Z","published":"2023-10-13T07:38:03Z","title":"Differential Evolution Algorithm based Hyper-Parameters Selection of\n  Convolutional Neural Network for Speech Command Recognition","summary":"  Speech Command Recognition (SCR), which deals with identification of short\nuttered speech commands, is crucial for various applications, including IoT\ndevices and assistive technology. Despite the promise shown by Convolutional\nNeural Networks (CNNs) in SCR tasks, their efficacy relies heavily on\nhyper-parameter selection, which is typically laborious and time-consuming when\ndone manually. This paper introduces a hyper-parameter selection method for\nCNNs based on the Differential Evolution (DE) algorithm, aiming to enhance\nperformance in SCR tasks. Training and testing with the Google Speech Command\n(GSC) dataset, the proposed approach showed effectiveness in classifying speech\ncommands. Moreover, a comparative analysis with Genetic Algorithm based\nselections and other deep CNN (DCNN) models highlighted the efficiency of the\nproposed DE algorithm in hyper-parameter selection for CNNs in SCR tasks.\n","authors":["Sandipan Dhar","Anuvab Sen","Aritra Bandyopadhyay","Nanda Dulal Jana","Arjun Ghosh","Zahra Sarayloo"],"pdf_url":"https://arxiv.org/pdf/2310.08914v1.pdf","comment":"8 Pages, 7 Figures, 4 Tables, Accepted by the 15th International\n  Joint Conference on Computational Intelligence (IJCCI 2023), November 13-15,\n  2023, Rome, Italy"},{"id":"http://arxiv.org/abs/2310.08869v1","updated":"2023-10-13T05:37:29Z","published":"2023-10-13T05:37:29Z","title":"Learning to Behave Like Clean Speech: Dual-Branch Knowledge Distillation\n  for Noise-Robust Fake Audio Detection","summary":"  Most research in fake audio detection (FAD) focuses on improving performance\non standard noise-free datasets. However, in actual situations, there is\nusually noise interference, which will cause significant performance\ndegradation in FAD systems. To improve the noise robustness, we propose a\ndual-branch knowledge distillation fake audio detection (DKDFAD) method.\nSpecifically, a parallel data flow of the clean teacher branch and the noisy\nstudent branch is designed, and interactive fusion and response-based\nteacher-student paradigms are proposed to guide the training of noisy data from\nthe data distribution and decision-making perspectives. In the noise branch,\nspeech enhancement is first introduced for denoising, which reduces the\ninterference of strong noise. The proposed interactive fusion combines\ndenoising features and noise features to reduce the impact of speech distortion\nand seek consistency with the data distribution of clean branch. The\nteacher-student paradigm maps the student's decision space to the teacher's\ndecision space, making noisy speech behave as clean. In addition, a joint\ntraining method is used to optimize the two branches to achieve global\noptimality. Experimental results based on multiple datasets show that the\nproposed method performs well in noisy environments and maintains performance\nin cross-dataset experiments.\n","authors":["Cunhang Fan","Mingming Ding","Jianhua Tao","Ruibo Fu","Jiangyan Yi","Zhengqi Wen","Zhao Lv"],"pdf_url":"https://arxiv.org/pdf/2310.08869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08846v1","updated":"2023-10-13T04:13:26Z","published":"2023-10-13T04:13:26Z","title":"Speaking rate attention-based duration prediction for speed control TTS","summary":"  With the advent of high-quality speech synthesis, there is a lot of interest\nin controlling various prosodic attributes of speech. Speaking rate is an\nessential attribute towards modelling the expressivity of speech. In this work,\nwe propose a novel approach to control the speaking rate for non-autoregressive\nTTS. We achieve this by conditioning the speaking rate inside the duration\npredictor, allowing implicit speaking rate control. We show the benefits of\nthis approach by synthesising audio at various speaking rate factors and\nmeasuring the quality of speaking rate-controlled synthesised speech. Further,\nwe study the effect of the speaking rate distribution of the training data\ntowards effective rate control. Finally, we fine-tune a baseline pretrained TTS\nmodel to obtain speaking rate control TTS. We provide various analyses to\nshowcase the benefits of using this proposed approach, along with objective as\nwell as subjective metrics. We find that the proposed methods have higher\nsubjective scores and lower speaker rate errors across many speaking rate\nfactors over the baseline.\n","authors":["Jesuraj Bandekar","Sathvik Udupa","Abhayjeet Singh","Anjali Jayakumar","Deekshitha G","Sandhya Badiger","Saurabh Kumar","Pooja VH","Prasanta Kumar Ghosh"],"pdf_url":"https://arxiv.org/pdf/2310.08846v1.pdf","comment":"\\c{opyright} 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2310.09424v1","updated":"2023-10-13T22:07:33Z","published":"2023-10-13T22:07:33Z","title":"SALM: Speech-augmented Language Model with In-context Learning for\n  Speech Recognition and Translation","summary":"  We present a novel Speech Augmented Language Model (SALM) with {\\em\nmultitask} and {\\em in-context} learning capabilities. SALM comprises a frozen\ntext LLM, a audio encoder, a modality adapter module, and LoRA layers to\naccommodate speech input and associated task instructions. The unified SALM not\nonly achieves performance on par with task-specific Conformer baselines for\nAutomatic Speech Recognition (ASR) and Speech Translation (AST), but also\nexhibits zero-shot in-context learning capabilities, demonstrated through\nkeyword-boosting task for ASR and AST. Moreover, {\\em speech supervised\nin-context training} is proposed to bridge the gap between LLM training and\ndownstream speech tasks, which further boosts the in-context learning ability\nof speech-to-text models. Proposed model is open-sourced via NeMo toolkit.\n","authors":["Zhehuai Chen","He Huang","Andrei Andrusenko","Oleksii Hrinchuk","Krishna C. Puvvada","Jason Li","Subhankar Ghosh","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2310.09424v1.pdf","comment":"submit to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.09404v1","updated":"2023-10-13T21:09:38Z","published":"2023-10-13T21:09:38Z","title":"Protecting Voice-Controlled Devices against LASER Injection Attacks","summary":"  Voice-Controllable Devices (VCDs) have seen an increasing trend towards their\nadoption due to the small form factor of the MEMS microphones and their easy\nintegration into modern gadgets. Recent studies have revealed that MEMS\nmicrophones are vulnerable to audio-modulated laser injection attacks. This\npaper aims to develop countermeasures to detect and prevent laser injection\nattacks on MEMS microphones. A time-frequency decomposition based on discrete\nwavelet transform (DWT) is employed to decompose microphone output audio signal\ninto n + 1 frequency subbands to capture photo-acoustic related artifacts.\nHigher-order statistical features consisting of the first four moments of\nsubband audio signals, e.g., variance, skew, and kurtosis are used to\ndistinguish between acoustic and photo-acoustic responses. An SVM classifier is\nused to learn the underlying model that differentiates between an acoustic- and\nlaser-induced (photo-acoustic) response in the MEMS microphone. The proposed\nframework is evaluated on a data set of 190 audios, consisting of 19 speakers.\nThe experimental results indicate that the proposed framework is able to\ncorrectly classify $98\\%$ of the acoustic- and laser-induced audio in a random\ndata partition setting and $100\\%$ of the audio in speaker-independent and\ntext-independent data partition settings.\n","authors":["Hashim Ali","Dhimant Khuttan","Rafi Ud Daula Refat","Hafiz Malik"],"pdf_url":"https://arxiv.org/pdf/2310.09404v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.09388v1","updated":"2023-10-13T20:17:44Z","published":"2023-10-13T20:17:44Z","title":"CORN: Co-Trained Full-Reference And No-Reference Audio Metrics","summary":"  Perceptual evaluation constitutes a crucial aspect of various\naudio-processing tasks. Full reference (FR) or similarity-based metrics rely on\nhigh-quality reference recordings, to which lower-quality or corrupted versions\nof the recording may be compared for evaluation. In contrast, no-reference (NR)\nmetrics evaluate a recording without relying on a reference. Both the FR and NR\napproaches exhibit advantages and drawbacks relative to each other. In this\npaper, we present a novel framework called CORN that amalgamates these dual\napproaches, concurrently training both FR and NR models together. After\ntraining, the models can be applied independently. We evaluate CORN by\npredicting several common objective metrics and across two different\narchitectures. The NR model trained using CORN has access to a reference\nrecording during training, and thus, as one would expect, it consistently\noutperforms baseline NR models trained independently. Perhaps even more\nremarkable is that the CORN FR model also outperforms its baseline counterpart,\neven though it relies on the same training data and the same model\narchitecture. Thus, a single training regime produces two independently useful\nmodels, each outperforming independently trained models.\n","authors":["Pranay Manocha","Donald Williamson","Adam Finkelstein"],"pdf_url":"https://arxiv.org/pdf/2310.09388v1.pdf","comment":null}]},"2023-10-16T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2310.05374v2","updated":"2023-10-16T01:21:03Z","published":"2023-10-09T03:10:49Z","title":"Improving End-to-End Speech Processing by Efficient Text Data\n  Utilization with Latent Synthesis","summary":"  Training a high performance end-to-end speech (E2E) processing model requires\nan enormous amount of labeled speech data, especially in the era of\ndata-centric artificial intelligence. However, labeled speech data are usually\nscarcer and more expensive for collection, compared to textual data. We propose\nLatent Synthesis (LaSyn), an efficient textual data utilization framework for\nE2E speech processing models. We train a latent synthesizer to convert textual\ndata into an intermediate latent representation of a pre-trained speech model.\nThese pseudo acoustic representations of textual data augment acoustic data for\nmodel training. We evaluate LaSyn on low-resource automatic speech recognition\n(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an\nE2E baseline trained on LibriSpeech train-clean-100, with relative word error\nrate reductions over 22.3% on different test sets. For SLU, LaSyn improves our\nE2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for\nslot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)\nand EM-Tree accuracies on STOP respectively. With fewer parameters, the results\nof LaSyn are competitive to published state-of-the-art works. The results\ndemonstrate the quality of the augmented training data. The source code will be\navailable to the community.\n","authors":["Jianqiao Lu","Wenyong Huang","Nianzu Zheng","Xingshan Zeng","Yu Ting Yeung","Xiao Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05374v2.pdf","comment":"15 pages, 8 figures, 8 tables, Accepted to EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2310.10604v1","updated":"2023-10-16T17:31:26Z","published":"2023-10-16T17:31:26Z","title":"Generation or Replication: Auscultating Audio Latent Diffusion Models","summary":"  The introduction of audio latent diffusion models possessing the ability to\ngenerate realistic sound clips on demand from a text description has the\npotential to revolutionize how we work with audio. In this work, we make an\ninitial attempt at understanding the inner workings of audio latent diffusion\nmodels by investigating how their audio outputs compare with the training data,\nsimilar to how a doctor auscultates a patient by listening to the sounds of\ntheir organs. Using text-to-audio latent diffusion models trained on the\nAudioCaps dataset, we systematically analyze memorization behavior as a\nfunction of training set size. We also evaluate different retrieval metrics for\nevidence of training data memorization, finding the similarity between mel\nspectrograms to be more robust in detecting matches than learned embedding\nvectors. In the process of analyzing memorization in audio latent diffusion\nmodels, we also discover a large amount of duplicated audio clips within the\nAudioCaps database.\n","authors":["Dimitrios Bralios","Gordon Wichern","Fran√ßois G. Germain","Zexu Pan","Sameer Khurana","Chiori Hori","Jonathan Le Roux"],"pdf_url":"https://arxiv.org/pdf/2310.10604v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2302.02088v3","updated":"2023-10-16T15:11:01Z","published":"2023-02-04T04:17:19Z","title":"AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene\n  Synthesis","summary":"  Can machines recording an audio-visual scene produce realistic, matching\naudio-visual experiences at novel positions and novel view directions? We\nanswer it by studying a new task -- real-world audio-visual scene synthesis --\nand a first-of-its-kind NeRF-based approach for multimodal learning.\nConcretely, given a video recording of an audio-visual scene, the task is to\nsynthesize new videos with spatial audios along arbitrary novel camera\ntrajectories in that scene. We propose an acoustic-aware audio generation\nmodule that integrates prior knowledge of audio propagation into NeRF, in which\nwe implicitly associate audio generation with the 3D geometry and material\nproperties of a visual environment. Furthermore, we present a coordinate\ntransformation module that expresses a view direction relative to the sound\nsource, enabling the model to learn sound source-centric acoustic fields. To\nfacilitate the study of this new task, we collect a high-quality Real-World\nAudio-Visual Scene (RWAVS) dataset. We demonstrate the advantages of our method\non this real-world dataset and the simulation-based SoundSpaces dataset.\n","authors":["Susan Liang","Chao Huang","Yapeng Tian","Anurag Kumar","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2302.02088v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.10300v1","updated":"2023-10-16T11:36:38Z","published":"2023-10-16T11:36:38Z","title":"BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework\n  for Music-Dance Retrieval","summary":"  Dance and music are closely related forms of expression, with mutual\nretrieval between dance videos and music being a fundamental task in various\nfields like education, art, and sports. However, existing methods often suffer\nfrom unnatural generation effects or fail to fully explore the correlation\nbetween music and dance. To overcome these challenges, we propose BeatDance, a\nnovel beat-based model-agnostic contrastive learning framework. BeatDance\nincorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat\nBlender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval\nperformance by utilizing the alignment between music beats and dance movements.\nWe also introduce the Music-Dance (MD) dataset, a large-scale collection of\nover 10,000 music-dance video pairs for training and testing. Experimental\nresults on the MD dataset demonstrate the superiority of our method over\nexisting baselines, achieving state-of-the-art performance. The code and\ndataset will be made public available upon acceptance.\n","authors":["Kaixing Yang","Xukun Zhou","Xulong Tang","Ran Diao","Hongyan Liu","Jun He","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2310.10300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10179v1","updated":"2023-10-16T08:40:11Z","published":"2023-10-16T08:40:11Z","title":"Advancing Audio Emotion and Intent Recognition with Large Pre-Trained\n  Models and Bayesian Inference","summary":"  Large pre-trained models are essential in paralinguistic systems,\ndemonstrating effectiveness in tasks like emotion recognition and stuttering\ndetection. In this paper, we employ large pre-trained models for the ACM\nMultimedia Computational Paralinguistics Challenge, addressing the Requests and\nEmotion Share tasks. We explore audio-only and hybrid solutions leveraging\naudio and text modalities. Our empirical results consistently show the\nsuperiority of the hybrid approaches over the audio-only models. Moreover, we\nintroduce a Bayesian layer as an alternative to the standard linear output\nlayer. The multimodal fusion approach achieves an 85.4% UAR on HC-Requests and\n60.2% on HC-Complaints. The ensemble model for the Emotion Share task yields\nthe best rho value of .614. The Bayesian wav2vec2 approach, explored in this\nstudy, allows us to easily build ensembles, at the cost of fine-tuning only one\nmodel. Moreover, we can have usable confidence values instead of the usual\noverconfident posterior probabilities.\n","authors":["Dejan Porjazovski","Yaroslav Getman","Tam√°s Gr√≥sz","Mikko Kurimo"],"pdf_url":"https://arxiv.org/pdf/2310.10179v1.pdf","comment":"Accepted at ACMM 2023"},{"id":"http://arxiv.org/abs/2310.10159v1","updated":"2023-10-16T08:00:16Z","published":"2023-10-16T08:00:16Z","title":"Joint Music and Language Attention Models for Zero-shot Music Tagging","summary":"  Music tagging is a task to predict the tags of music recordings. However,\nprevious music tagging research primarily focuses on close-set music tagging\ntasks which can not be generalized to new tags. In this work, we propose a\nzero-shot music tagging system modeled by a joint music and language attention\n(JMLA) model to address the open-set music tagging problem. The JMLA model\nconsists of an audio encoder modeled by a pretrained masked autoencoder and a\ndecoder modeled by a Falcon7B. We introduce preceiver resampler to convert\narbitrary length audio into fixed length embeddings. We introduce dense\nattention connections between encoder and decoder layers to improve the\ninformation flow between the encoder and decoder layers. We collect a\nlarge-scale music and description dataset from the internet. We propose to use\nChatGPT to convert the raw descriptions into formalized and diverse\ndescriptions to train the JMLA models. Our proposed JMLA system achieves a\nzero-shot audio tagging accuracy of $ 64.82\\% $ on the GTZAN dataset,\noutperforming previous zero-shot systems and achieves comparable results to\nprevious systems on the FMA and the MagnaTagATune datasets.\n","authors":["Xingjian Du","Zhesong Yu","Jiaju Lin","Bilei Zhu","Qiuqiang Kong"],"pdf_url":"https://arxiv.org/pdf/2310.10159v1.pdf","comment":"\\begin{keywords} Music tagging, joint music and language attention\n  models, Music Foundation Model. \\end{keywords}"},{"id":"http://arxiv.org/abs/2310.10106v1","updated":"2023-10-16T06:40:18Z","published":"2023-10-16T06:40:18Z","title":"End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder\n  and Input Feature Analysis","summary":"  We present an end-to-end multichannel speaker-attributed automatic speech\nrecognition (MC-SA-ASR) system that combines a Conformer-based encoder with\nmulti-frame crosschannel attention and a speaker-attributed Transformer-based\ndecoder. To the best of our knowledge, this is the first model that efficiently\nintegrates ASR and speaker identification modules in a multichannel setting. On\nsimulated mixtures of LibriSpeech data, our system reduces the word error rate\n(WER) by up to 12% and 16% relative compared to previously proposed\nsingle-channel and multichannel approaches, respectively. Furthermore, we\ninvestigate the impact of different input features, including multichannel\nmagnitude and phase information, on the ASR performance. Finally, our\nexperiments on the AMI corpus confirm the effectiveness of our system for\nreal-world multichannel meeting transcription.\n","authors":["Can Cui","Imran Ahamad Sheikh","Mostafa Sadeghi","Emmanuel Vincent"],"pdf_url":"https://arxiv.org/pdf/2310.10106v1.pdf","comment":"2023 IEEE Automatic Speech Recognition and Understanding Workshop\n  (ASRU 2023), Dec 2023, Taipei, Taiwan"},{"id":"http://arxiv.org/abs/2309.15701v2","updated":"2023-10-16T05:47:42Z","published":"2023-09-27T14:44:10Z","title":"HyPoradise: An Open Baseline for Generative Speech Recognition with\n  Large Language Models","summary":"  Advancements in deep neural networks have allowed automatic speech\nrecognition (ASR) systems to attain human parity on several publicly available\nclean speech datasets. However, even state-of-the-art ASR systems experience\nperformance degradation when confronted with adverse conditions, as a\nwell-trained acoustic model is sensitive to variations in the speech domain,\ne.g., background noise. Intuitively, humans address this issue by relying on\ntheir linguistic knowledge: the meaning of ambiguous spoken terms is usually\ninferred from contextual cues thereby reducing the dependency on the auditory\nsystem. Inspired by this observation, we introduce the first open-source\nbenchmark to utilize external large language models (LLMs) for ASR error\ncorrection, where N-best decoding hypotheses provide informative elements for\ntrue transcription prediction. This approach is a paradigm shift from the\ntraditional language model rescoring strategy that can only select one\ncandidate hypothesis as the output transcription. The proposed benchmark\ncontains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairs\nof N-best hypotheses and corresponding accurate transcriptions across prevalent\nspeech domains. Given this dataset, we examine three types of error correction\ntechniques based on LLMs with varying amounts of labeled\nhypotheses-transcription pairs, which gains a significant word error rate (WER)\nreduction. Experimental evidence demonstrates the proposed technique achieves a\nbreakthrough by surpassing the upper bound of traditional re-ranking based\nmethods. More surprisingly, LLM with reasonable prompt and its generative\ncapability can even correct those tokens that are missing in N-best list. We\nmake our results publicly accessible for reproducible pipelines with released\npre-trained models, thus providing a new evaluation paradigm for ASR error\ncorrection with LLMs.\n","authors":["Chen Chen","Yuchen Hu","Chao-Han Huck Yang","Sabato Macro Siniscalchi","Pin-Yu Chen","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2309.15701v2.pdf","comment":"Accepted to NeurIPS 2023, 24 pages. Datasets and Benchmarks Track.\n  Added the first Mandarin and code-switching (zh-cn and en-us) results from\n  the LLM-based generative ASR error correction to Table 8 on Page 21"},{"id":"http://arxiv.org/abs/2211.11917v2","updated":"2023-10-16T03:06:50Z","published":"2022-11-22T00:02:57Z","title":"Latent Iterative Refinement for Modular Source Separation","summary":"  Traditional source separation approaches train deep neural network models\nend-to-end with all the data available at once by minimizing the empirical risk\non the whole training set. On the inference side, after training the model, the\nuser fetches a static computation graph and runs the full model on some\nspecified observed mixture signal to get the estimated source signals.\nAdditionally, many of those models consist of several basic processing blocks\nwhich are applied sequentially. We argue that we can significantly increase\nresource efficiency during both training and inference stages by reformulating\na model's training and inference procedures as iterative mappings of latent\nsignal representations. First, we can apply the same processing block more than\nonce on its output to refine the input signal and consequently improve\nparameter efficiency. During training, we can follow a block-wise procedure\nwhich enables a reduction on memory requirements. Thus, one can train a very\ncomplicated network structure using significantly less computation compared to\nend-to-end training. During inference, we can dynamically adjust how many\nprocessing blocks and iterations of a specific block an input signal needs\nusing a gating module.\n","authors":["Dimitrios Bralios","Efthymios Tzinis","Gordon Wichern","Paris Smaragdis","Jonathan Le Roux"],"pdf_url":"https://arxiv.org/pdf/2211.11917v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10026v1","updated":"2023-10-16T03:02:29Z","published":"2023-10-16T03:02:29Z","title":"Real-time Speech Enhancement and Separation with a Unified Deep Neural\n  Network for Single/Dual Talker Scenarios","summary":"  This paper introduces a practical approach for leveraging a real-time deep\nlearning model to alternate between speech enhancement and joint speech\nenhancement and separation depending on whether the input mixture contains one\nor two active speakers. Scale-invariant signal-to-distortion ratio (SI-SDR) has\nshown to be a highly effective training measure in time-domain speech\nseparation. However, the SI-SDR metric is ill-defined for zero-energy target\nsignals, which is a problem when training a speech separation model using\nutterances with varying numbers of talkers. Unlike existing solutions that\nfocus on modifying the loss function to accommodate zero-energy target signals,\nthe proposed approach circumvents this problem by training the model to extract\nspeech on both its output channels regardless if the input is a single or\ndual-talker mixture. A lightweight speaker overlap detection (SOD) module is\nalso introduced to differentiate between single and dual-talker segments in\nreal-time. The proposed module takes advantage of the new formulation by\noperating directly on the separated masks, given by the separation model,\ninstead of the original mixture, thus effectively simplifying the detection\ntask. Experimental results show that the proposed training approach outperforms\nexisting solutions, and the SOD module exhibits high accuracy.\n","authors":["Kashyap Patel","Anton Kovalyov","Issa Panahi"],"pdf_url":"https://arxiv.org/pdf/2310.10026v1.pdf","comment":"6 Pages, Accepted at IEEE Asilomar"},{"id":"http://arxiv.org/abs/2310.06434v2","updated":"2023-10-16T21:32:56Z","published":"2023-10-10T09:04:33Z","title":"Whispering LLaMA: A Cross-Modal Generative Error Correction Framework\n  for Speech Recognition","summary":"  We introduce a new cross-modal fusion technique designed for generative error\ncorrection in automatic speech recognition (ASR). Our methodology leverages\nboth acoustic information and external linguistic representations to generate\naccurate speech transcription contexts. This marks a step towards a fresh\nparadigm in generative error correction within the realm of n-best hypotheses.\nUnlike the existing ranking-based rescoring methods, our approach adeptly uses\ndistinct initialization techniques and parameter-efficient algorithms to boost\nASR performance derived from pre-trained speech and text models. Through\nevaluation across diverse ASR datasets, we evaluate the stability and\nreproducibility of our fusion technique, demonstrating its improved word error\nrate relative (WERR) performance in comparison to n-best hypotheses by\nrelatively 37.66%. To encourage future research, we have made our code and\npre-trained models open source at\nhttps://github.com/Srijith-rkr/Whispering-LLaMA.\n","authors":["Srijith Radhakrishnan","Chao-Han Huck Yang","Sumeer Ahmad Khan","Rohit Kumar","Narsis A. Kiani","David Gomez-Cabrero","Jesper N. Tegner"],"pdf_url":"https://arxiv.org/pdf/2310.06434v2.pdf","comment":"Accepted to EMNLP 2023 as main paper. 10 pages. Revised math\n  notations. GitHub: https://github.com/Srijith-rkr/Whispering-LLaMA"},{"id":"http://arxiv.org/abs/2310.10772v1","updated":"2023-10-16T19:12:20Z","published":"2023-10-16T19:12:20Z","title":"Unsupervised Lead Sheet Generation via Semantic Compression","summary":"  Lead sheets have become commonplace in generative music research, being used\nas an initial compressed representation for downstream tasks like multitrack\nmusic generation and automatic arrangement. Despite this, researchers have\noften fallen back on deterministic reduction methods (such as the skyline\nalgorithm) to generate lead sheets when seeking paired lead sheets and full\nscores, with little attention being paid toward the quality of the lead sheets\nthemselves and how they accurately reflect their orchestrated counterparts. To\naddress these issues, we propose the problem of conditional lead sheet\ngeneration (i.e. generating a lead sheet given its full score version), and\nshow that this task can be formulated as an unsupervised music compression\ntask, where the lead sheet represents a compressed latent version of the score.\nWe introduce a novel model, called Lead-AE, that models the lead sheets as a\ndiscrete subselection of the original sequence, using a differentiable top-k\noperator to allow for controllable local sparsity constraints. Across both\nautomatic proxy tasks and direct human evaluations, we find that our method\nimproves upon the established deterministic baseline and produces coherent\nreductions of large multitrack scores.\n","authors":["Zachary Novack","Nikita Srivatsan","Taylor Berg-Kirkpatrick","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2310.10772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10704v1","updated":"2023-10-16T12:14:21Z","published":"2023-10-16T12:14:21Z","title":"Optimized Tokenization for Transcribed Error Correction","summary":"  The challenges facing speech recognition systems, such as variations in\npronunciations, adverse audio conditions, and the scarcity of labeled data,\nemphasize the necessity for a post-processing step that corrects recurring\nerrors. Previous research has shown the advantages of employing dedicated error\ncorrection models, yet training such models requires large amounts of labeled\ndata which is not easily obtained. To overcome this limitation, synthetic\ntranscribed-like data is often utilized, however, bridging the distribution gap\nbetween transcribed errors and synthetic noise is not trivial. In this paper,\nwe demonstrate that the performance of correction models can be significantly\nincreased by training solely using synthetic data. Specifically, we empirically\nshow that: (1) synthetic data generated using the error distribution derived\nfrom a set of transcribed data outperforms the common approach of applying\nrandom perturbations; (2) applying language-specific adjustments to the\nvocabulary of a BPE tokenizer strike a balance between adapting to unseen\ndistributions and retaining knowledge of transcribed errors. We showcase the\nbenefits of these key observations, and evaluate our approach using multiple\nlanguages, speech recognition systems and prominent speech recognition\ndatasets.\n","authors":["Tomer Wullach","Shlomo E. Chazan"],"pdf_url":"https://arxiv.org/pdf/2310.10704v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.05374v2","updated":"2023-10-16T01:21:03Z","published":"2023-10-09T03:10:49Z","title":"Improving End-to-End Speech Processing by Efficient Text Data\n  Utilization with Latent Synthesis","summary":"  Training a high performance end-to-end speech (E2E) processing model requires\nan enormous amount of labeled speech data, especially in the era of\ndata-centric artificial intelligence. However, labeled speech data are usually\nscarcer and more expensive for collection, compared to textual data. We propose\nLatent Synthesis (LaSyn), an efficient textual data utilization framework for\nE2E speech processing models. We train a latent synthesizer to convert textual\ndata into an intermediate latent representation of a pre-trained speech model.\nThese pseudo acoustic representations of textual data augment acoustic data for\nmodel training. We evaluate LaSyn on low-resource automatic speech recognition\n(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an\nE2E baseline trained on LibriSpeech train-clean-100, with relative word error\nrate reductions over 22.3% on different test sets. For SLU, LaSyn improves our\nE2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for\nslot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)\nand EM-Tree accuracies on STOP respectively. With fewer parameters, the results\nof LaSyn are competitive to published state-of-the-art works. The results\ndemonstrate the quality of the augmented training data. The source code will be\navailable to the community.\n","authors":["Jianqiao Lu","Wenyong Huang","Nianzu Zheng","Xingshan Zeng","Yu Ting Yeung","Xiao Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05374v2.pdf","comment":"15 pages, 8 figures, 8 tables, Accepted to EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2310.10604v1","updated":"2023-10-16T17:31:26Z","published":"2023-10-16T17:31:26Z","title":"Generation or Replication: Auscultating Audio Latent Diffusion Models","summary":"  The introduction of audio latent diffusion models possessing the ability to\ngenerate realistic sound clips on demand from a text description has the\npotential to revolutionize how we work with audio. In this work, we make an\ninitial attempt at understanding the inner workings of audio latent diffusion\nmodels by investigating how their audio outputs compare with the training data,\nsimilar to how a doctor auscultates a patient by listening to the sounds of\ntheir organs. Using text-to-audio latent diffusion models trained on the\nAudioCaps dataset, we systematically analyze memorization behavior as a\nfunction of training set size. We also evaluate different retrieval metrics for\nevidence of training data memorization, finding the similarity between mel\nspectrograms to be more robust in detecting matches than learned embedding\nvectors. In the process of analyzing memorization in audio latent diffusion\nmodels, we also discover a large amount of duplicated audio clips within the\nAudioCaps database.\n","authors":["Dimitrios Bralios","Gordon Wichern","Fran√ßois G. Germain","Zexu Pan","Sameer Khurana","Chiori Hori","Jonathan Le Roux"],"pdf_url":"https://arxiv.org/pdf/2310.10604v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2302.02088v3","updated":"2023-10-16T15:11:01Z","published":"2023-02-04T04:17:19Z","title":"AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene\n  Synthesis","summary":"  Can machines recording an audio-visual scene produce realistic, matching\naudio-visual experiences at novel positions and novel view directions? We\nanswer it by studying a new task -- real-world audio-visual scene synthesis --\nand a first-of-its-kind NeRF-based approach for multimodal learning.\nConcretely, given a video recording of an audio-visual scene, the task is to\nsynthesize new videos with spatial audios along arbitrary novel camera\ntrajectories in that scene. We propose an acoustic-aware audio generation\nmodule that integrates prior knowledge of audio propagation into NeRF, in which\nwe implicitly associate audio generation with the 3D geometry and material\nproperties of a visual environment. Furthermore, we present a coordinate\ntransformation module that expresses a view direction relative to the sound\nsource, enabling the model to learn sound source-centric acoustic fields. To\nfacilitate the study of this new task, we collect a high-quality Real-World\nAudio-Visual Scene (RWAVS) dataset. We demonstrate the advantages of our method\non this real-world dataset and the simulation-based SoundSpaces dataset.\n","authors":["Susan Liang","Chao Huang","Yapeng Tian","Anurag Kumar","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2302.02088v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.10300v1","updated":"2023-10-16T11:36:38Z","published":"2023-10-16T11:36:38Z","title":"BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework\n  for Music-Dance Retrieval","summary":"  Dance and music are closely related forms of expression, with mutual\nretrieval between dance videos and music being a fundamental task in various\nfields like education, art, and sports. However, existing methods often suffer\nfrom unnatural generation effects or fail to fully explore the correlation\nbetween music and dance. To overcome these challenges, we propose BeatDance, a\nnovel beat-based model-agnostic contrastive learning framework. BeatDance\nincorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat\nBlender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval\nperformance by utilizing the alignment between music beats and dance movements.\nWe also introduce the Music-Dance (MD) dataset, a large-scale collection of\nover 10,000 music-dance video pairs for training and testing. Experimental\nresults on the MD dataset demonstrate the superiority of our method over\nexisting baselines, achieving state-of-the-art performance. The code and\ndataset will be made public available upon acceptance.\n","authors":["Kaixing Yang","Xukun Zhou","Xulong Tang","Ran Diao","Hongyan Liu","Jun He","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2310.10300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10179v1","updated":"2023-10-16T08:40:11Z","published":"2023-10-16T08:40:11Z","title":"Advancing Audio Emotion and Intent Recognition with Large Pre-Trained\n  Models and Bayesian Inference","summary":"  Large pre-trained models are essential in paralinguistic systems,\ndemonstrating effectiveness in tasks like emotion recognition and stuttering\ndetection. In this paper, we employ large pre-trained models for the ACM\nMultimedia Computational Paralinguistics Challenge, addressing the Requests and\nEmotion Share tasks. We explore audio-only and hybrid solutions leveraging\naudio and text modalities. Our empirical results consistently show the\nsuperiority of the hybrid approaches over the audio-only models. Moreover, we\nintroduce a Bayesian layer as an alternative to the standard linear output\nlayer. The multimodal fusion approach achieves an 85.4% UAR on HC-Requests and\n60.2% on HC-Complaints. The ensemble model for the Emotion Share task yields\nthe best rho value of .614. The Bayesian wav2vec2 approach, explored in this\nstudy, allows us to easily build ensembles, at the cost of fine-tuning only one\nmodel. Moreover, we can have usable confidence values instead of the usual\noverconfident posterior probabilities.\n","authors":["Dejan Porjazovski","Yaroslav Getman","Tam√°s Gr√≥sz","Mikko Kurimo"],"pdf_url":"https://arxiv.org/pdf/2310.10179v1.pdf","comment":"Accepted at ACMM 2023"},{"id":"http://arxiv.org/abs/2310.10159v1","updated":"2023-10-16T08:00:16Z","published":"2023-10-16T08:00:16Z","title":"Joint Music and Language Attention Models for Zero-shot Music Tagging","summary":"  Music tagging is a task to predict the tags of music recordings. However,\nprevious music tagging research primarily focuses on close-set music tagging\ntasks which can not be generalized to new tags. In this work, we propose a\nzero-shot music tagging system modeled by a joint music and language attention\n(JMLA) model to address the open-set music tagging problem. The JMLA model\nconsists of an audio encoder modeled by a pretrained masked autoencoder and a\ndecoder modeled by a Falcon7B. We introduce preceiver resampler to convert\narbitrary length audio into fixed length embeddings. We introduce dense\nattention connections between encoder and decoder layers to improve the\ninformation flow between the encoder and decoder layers. We collect a\nlarge-scale music and description dataset from the internet. We propose to use\nChatGPT to convert the raw descriptions into formalized and diverse\ndescriptions to train the JMLA models. Our proposed JMLA system achieves a\nzero-shot audio tagging accuracy of $ 64.82\\% $ on the GTZAN dataset,\noutperforming previous zero-shot systems and achieves comparable results to\nprevious systems on the FMA and the MagnaTagATune datasets.\n","authors":["Xingjian Du","Zhesong Yu","Jiaju Lin","Bilei Zhu","Qiuqiang Kong"],"pdf_url":"https://arxiv.org/pdf/2310.10159v1.pdf","comment":"\\begin{keywords} Music tagging, joint music and language attention\n  models, Music Foundation Model. \\end{keywords}"},{"id":"http://arxiv.org/abs/2310.10106v1","updated":"2023-10-16T06:40:18Z","published":"2023-10-16T06:40:18Z","title":"End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder\n  and Input Feature Analysis","summary":"  We present an end-to-end multichannel speaker-attributed automatic speech\nrecognition (MC-SA-ASR) system that combines a Conformer-based encoder with\nmulti-frame crosschannel attention and a speaker-attributed Transformer-based\ndecoder. To the best of our knowledge, this is the first model that efficiently\nintegrates ASR and speaker identification modules in a multichannel setting. On\nsimulated mixtures of LibriSpeech data, our system reduces the word error rate\n(WER) by up to 12% and 16% relative compared to previously proposed\nsingle-channel and multichannel approaches, respectively. Furthermore, we\ninvestigate the impact of different input features, including multichannel\nmagnitude and phase information, on the ASR performance. Finally, our\nexperiments on the AMI corpus confirm the effectiveness of our system for\nreal-world multichannel meeting transcription.\n","authors":["Can Cui","Imran Ahamad Sheikh","Mostafa Sadeghi","Emmanuel Vincent"],"pdf_url":"https://arxiv.org/pdf/2310.10106v1.pdf","comment":"2023 IEEE Automatic Speech Recognition and Understanding Workshop\n  (ASRU 2023), Dec 2023, Taipei, Taiwan"},{"id":"http://arxiv.org/abs/2309.15701v2","updated":"2023-10-16T05:47:42Z","published":"2023-09-27T14:44:10Z","title":"HyPoradise: An Open Baseline for Generative Speech Recognition with\n  Large Language Models","summary":"  Advancements in deep neural networks have allowed automatic speech\nrecognition (ASR) systems to attain human parity on several publicly available\nclean speech datasets. However, even state-of-the-art ASR systems experience\nperformance degradation when confronted with adverse conditions, as a\nwell-trained acoustic model is sensitive to variations in the speech domain,\ne.g., background noise. Intuitively, humans address this issue by relying on\ntheir linguistic knowledge: the meaning of ambiguous spoken terms is usually\ninferred from contextual cues thereby reducing the dependency on the auditory\nsystem. Inspired by this observation, we introduce the first open-source\nbenchmark to utilize external large language models (LLMs) for ASR error\ncorrection, where N-best decoding hypotheses provide informative elements for\ntrue transcription prediction. This approach is a paradigm shift from the\ntraditional language model rescoring strategy that can only select one\ncandidate hypothesis as the output transcription. The proposed benchmark\ncontains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairs\nof N-best hypotheses and corresponding accurate transcriptions across prevalent\nspeech domains. Given this dataset, we examine three types of error correction\ntechniques based on LLMs with varying amounts of labeled\nhypotheses-transcription pairs, which gains a significant word error rate (WER)\nreduction. Experimental evidence demonstrates the proposed technique achieves a\nbreakthrough by surpassing the upper bound of traditional re-ranking based\nmethods. More surprisingly, LLM with reasonable prompt and its generative\ncapability can even correct those tokens that are missing in N-best list. We\nmake our results publicly accessible for reproducible pipelines with released\npre-trained models, thus providing a new evaluation paradigm for ASR error\ncorrection with LLMs.\n","authors":["Chen Chen","Yuchen Hu","Chao-Han Huck Yang","Sabato Macro Siniscalchi","Pin-Yu Chen","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2309.15701v2.pdf","comment":"Accepted to NeurIPS 2023, 24 pages. Datasets and Benchmarks Track.\n  Added the first Mandarin and code-switching (zh-cn and en-us) results from\n  the LLM-based generative ASR error correction to Table 8 on Page 21"},{"id":"http://arxiv.org/abs/2305.14203v2","updated":"2023-10-16T05:06:22Z","published":"2023-05-23T16:20:46Z","title":"Improving the Gap in Visual Speech Recognition Between Normal and Silent\n  Speech Based on Metric Learning","summary":"  This paper presents a novel metric learning approach to address the\nperformance gap between normal and silent speech in visual speech recognition\n(VSR). The difference in lip movements between the two poses a challenge for\nexisting VSR models, which exhibit degraded accuracy when applied to silent\nspeech. To solve this issue and tackle the scarcity of training data for silent\nspeech, we propose to leverage the shared literal content between normal and\nsilent speech and present a metric learning approach based on visemes.\nSpecifically, we aim to map the input of two speech types close to each other\nin a latent space if they have similar viseme representations. By minimizing\nthe Kullback-Leibler divergence of the predicted viseme probability\ndistributions between and within the two speech types, our model effectively\nlearns and predicts viseme identities. Our evaluation demonstrates that our\nmethod improves the accuracy of silent VSR, even when limited training data is\navailable.\n","authors":["Sara Kashiwagi","Keitaro Tanaka","Qi Feng","Shigeo Morishima"],"pdf_url":"https://arxiv.org/pdf/2305.14203v2.pdf","comment":"Accepted by INTERSPEECH 2023"},{"id":"http://arxiv.org/abs/2211.11917v2","updated":"2023-10-16T03:06:50Z","published":"2022-11-22T00:02:57Z","title":"Latent Iterative Refinement for Modular Source Separation","summary":"  Traditional source separation approaches train deep neural network models\nend-to-end with all the data available at once by minimizing the empirical risk\non the whole training set. On the inference side, after training the model, the\nuser fetches a static computation graph and runs the full model on some\nspecified observed mixture signal to get the estimated source signals.\nAdditionally, many of those models consist of several basic processing blocks\nwhich are applied sequentially. We argue that we can significantly increase\nresource efficiency during both training and inference stages by reformulating\na model's training and inference procedures as iterative mappings of latent\nsignal representations. First, we can apply the same processing block more than\nonce on its output to refine the input signal and consequently improve\nparameter efficiency. During training, we can follow a block-wise procedure\nwhich enables a reduction on memory requirements. Thus, one can train a very\ncomplicated network structure using significantly less computation compared to\nend-to-end training. During inference, we can dynamically adjust how many\nprocessing blocks and iterations of a specific block an input signal needs\nusing a gating module.\n","authors":["Dimitrios Bralios","Efthymios Tzinis","Gordon Wichern","Paris Smaragdis","Jonathan Le Roux"],"pdf_url":"https://arxiv.org/pdf/2211.11917v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10026v1","updated":"2023-10-16T03:02:29Z","published":"2023-10-16T03:02:29Z","title":"Real-time Speech Enhancement and Separation with a Unified Deep Neural\n  Network for Single/Dual Talker Scenarios","summary":"  This paper introduces a practical approach for leveraging a real-time deep\nlearning model to alternate between speech enhancement and joint speech\nenhancement and separation depending on whether the input mixture contains one\nor two active speakers. Scale-invariant signal-to-distortion ratio (SI-SDR) has\nshown to be a highly effective training measure in time-domain speech\nseparation. However, the SI-SDR metric is ill-defined for zero-energy target\nsignals, which is a problem when training a speech separation model using\nutterances with varying numbers of talkers. Unlike existing solutions that\nfocus on modifying the loss function to accommodate zero-energy target signals,\nthe proposed approach circumvents this problem by training the model to extract\nspeech on both its output channels regardless if the input is a single or\ndual-talker mixture. A lightweight speaker overlap detection (SOD) module is\nalso introduced to differentiate between single and dual-talker segments in\nreal-time. The proposed module takes advantage of the new formulation by\noperating directly on the separated masks, given by the separation model,\ninstead of the original mixture, thus effectively simplifying the detection\ntask. Experimental results show that the proposed training approach outperforms\nexisting solutions, and the SOD module exhibits high accuracy.\n","authors":["Kashyap Patel","Anton Kovalyov","Issa Panahi"],"pdf_url":"https://arxiv.org/pdf/2310.10026v1.pdf","comment":"6 Pages, Accepted at IEEE Asilomar"},{"id":"http://arxiv.org/abs/2310.06434v2","updated":"2023-10-16T21:32:56Z","published":"2023-10-10T09:04:33Z","title":"Whispering LLaMA: A Cross-Modal Generative Error Correction Framework\n  for Speech Recognition","summary":"  We introduce a new cross-modal fusion technique designed for generative error\ncorrection in automatic speech recognition (ASR). Our methodology leverages\nboth acoustic information and external linguistic representations to generate\naccurate speech transcription contexts. This marks a step towards a fresh\nparadigm in generative error correction within the realm of n-best hypotheses.\nUnlike the existing ranking-based rescoring methods, our approach adeptly uses\ndistinct initialization techniques and parameter-efficient algorithms to boost\nASR performance derived from pre-trained speech and text models. Through\nevaluation across diverse ASR datasets, we evaluate the stability and\nreproducibility of our fusion technique, demonstrating its improved word error\nrate relative (WERR) performance in comparison to n-best hypotheses by\nrelatively 37.66%. To encourage future research, we have made our code and\npre-trained models open source at\nhttps://github.com/Srijith-rkr/Whispering-LLaMA.\n","authors":["Srijith Radhakrishnan","Chao-Han Huck Yang","Sumeer Ahmad Khan","Rohit Kumar","Narsis A. Kiani","David Gomez-Cabrero","Jesper N. Tegner"],"pdf_url":"https://arxiv.org/pdf/2310.06434v2.pdf","comment":"Accepted to EMNLP 2023 as main paper. 10 pages. Revised math\n  notations. GitHub: https://github.com/Srijith-rkr/Whispering-LLaMA"},{"id":"http://arxiv.org/abs/2310.10803v1","updated":"2023-10-16T20:05:36Z","published":"2023-10-16T20:05:36Z","title":"SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT","summary":"  Data-driven unit discovery in self-supervised learning (SSL) of speech has\nembarked on a new era of spoken language processing. Yet, the discovered units\noften remain in phonetic space, limiting the utility of SSL representations.\nHere, we demonstrate that a syllabic organization emerges in learning\nsentence-level representation of speech. In particular, we adopt\n\"self-distillation\" objective to fine-tune the pretrained HuBERT with an\naggregator token that summarizes the entire sentence. Without any supervision,\nthe resulting model draws definite boundaries in speech, and the\nrepresentations across frames show salient syllabic structures. We demonstrate\nthat this emergent structure largely corresponds to the ground truth syllables.\nFurthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating\nsentence-level representation of speech. When compared to previous models, our\nmodel outperforms in both unsupervised syllable discovery and learning\nsentence-level representation. Together, we demonstrate that the\nself-distillation of HuBERT gives rise to syllabic organization without relying\non external labels or modalities, and potentially provides novel data-driven\nunits for spoken language modeling.\n","authors":["Cheol Jun Cho","Abdelrahman Mohamed","Shang-Wen Li","Alan W Black","Gopala K. Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2310.10803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10788v1","updated":"2023-10-16T19:50:01Z","published":"2023-10-16T19:50:01Z","title":"Self-Supervised Models of Speech Infer Universal Articulatory Kinematics","summary":"  Self-Supervised Learning (SSL) based models of speech have shown remarkable\nperformance on a range of downstream tasks. These state-of-the-art models have\nremained blackboxes, but many recent studies have begun \"probing\" models like\nHuBERT, to correlate their internal representations to different aspects of\nspeech. In this paper, we show \"inference of articulatory kinematics\" as\nfundamental property of SSL models, i.e., the ability of these models to\ntransform acoustics into the causal articulatory dynamics underlying the speech\nsignal. We also show that this abstraction is largely overlapping across the\nlanguage of the data used to train the model, with preference to the language\nwith similar phonological system. Furthermore, we show that with simple affine\ntransformations, Acoustic-to-Articulatory inversion (AAI) is transferrable\nacross speakers, even across genders, languages, and dialects, showing the\ngeneralizability of this property. Together, these results shed new light on\nthe internals of SSL models that are critical to their superior performance,\nand open up new avenues into language-agnostic universal models for speech\nengineering, that are interpretable and grounded in speech science.\n","authors":["Cheol Jun Cho","Abdelrahman Mohamed","Alan W Black","Gopala K. Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2310.10788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10772v1","updated":"2023-10-16T19:12:20Z","published":"2023-10-16T19:12:20Z","title":"Unsupervised Lead Sheet Generation via Semantic Compression","summary":"  Lead sheets have become commonplace in generative music research, being used\nas an initial compressed representation for downstream tasks like multitrack\nmusic generation and automatic arrangement. Despite this, researchers have\noften fallen back on deterministic reduction methods (such as the skyline\nalgorithm) to generate lead sheets when seeking paired lead sheets and full\nscores, with little attention being paid toward the quality of the lead sheets\nthemselves and how they accurately reflect their orchestrated counterparts. To\naddress these issues, we propose the problem of conditional lead sheet\ngeneration (i.e. generating a lead sheet given its full score version), and\nshow that this task can be formulated as an unsupervised music compression\ntask, where the lead sheet represents a compressed latent version of the score.\nWe introduce a novel model, called Lead-AE, that models the lead sheets as a\ndiscrete subselection of the original sequence, using a differentiable top-k\noperator to allow for controllable local sparsity constraints. Across both\nautomatic proxy tasks and direct human evaluations, we find that our method\nimproves upon the established deterministic baseline and produces coherent\nreductions of large multitrack scores.\n","authors":["Zachary Novack","Nikita Srivatsan","Taylor Berg-Kirkpatrick","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2310.10772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10704v1","updated":"2023-10-16T12:14:21Z","published":"2023-10-16T12:14:21Z","title":"Optimized Tokenization for Transcribed Error Correction","summary":"  The challenges facing speech recognition systems, such as variations in\npronunciations, adverse audio conditions, and the scarcity of labeled data,\nemphasize the necessity for a post-processing step that corrects recurring\nerrors. Previous research has shown the advantages of employing dedicated error\ncorrection models, yet training such models requires large amounts of labeled\ndata which is not easily obtained. To overcome this limitation, synthetic\ntranscribed-like data is often utilized, however, bridging the distribution gap\nbetween transcribed errors and synthetic noise is not trivial. In this paper,\nwe demonstrate that the performance of correction models can be significantly\nincreased by training solely using synthetic data. Specifically, we empirically\nshow that: (1) synthetic data generated using the error distribution derived\nfrom a set of transcribed data outperforms the common approach of applying\nrandom perturbations; (2) applying language-specific adjustments to the\nvocabulary of a BPE tokenizer strike a balance between adapting to unseen\ndistributions and retaining knowledge of transcribed errors. We showcase the\nbenefits of these key observations, and evaluate our approach using multiple\nlanguages, speech recognition systems and prominent speech recognition\ndatasets.\n","authors":["Tomer Wullach","Shlomo E. Chazan"],"pdf_url":"https://arxiv.org/pdf/2310.10704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.02331v3","updated":"2023-10-16T05:01:19Z","published":"2021-06-04T08:27:01Z","title":"Manifold-Aware Deep Clustering: Maximizing Angles between Embedding\n  Vectors Based on Regular Simplex","summary":"  This paper presents a new deep clustering (DC) method called manifold-aware\nDC (M-DC) that can enhance hyperspace utilization more effectively than the\noriginal DC. The original DC has a limitation in that a pair of two speakers\nhas to be embedded having an orthogonal relationship due to its use of the\none-hot vector-based loss function, while our method derives a unique loss\nfunction aimed at maximizing the target angle in the hyperspace based on the\nnature of a regular simplex. Our proposed loss imposes a higher penalty than\nthe original DC when the speaker is assigned incorrectly. The change from DC to\nM-DC can be easily achieved by rewriting just one term in the loss function of\nDC, without any other modifications to the network architecture or model\nparameters. As such, our method has high practicability because it does not\naffect the original inference part. The experimental results show that the\nproposed method improves the performances of the original DC and its expansion\nmethod.\n","authors":["Keitaro Tanaka","Ryosuke Sawata","Shusuke Takahashi"],"pdf_url":"https://arxiv.org/pdf/2106.02331v3.pdf","comment":"Accepted by Interspeech 2021"},{"id":"http://arxiv.org/abs/2310.13010v1","updated":"2023-10-16T21:07:12Z","published":"2023-10-16T21:07:12Z","title":"Detecting Speech Abnormalities with a Perceiver-based Sequence\n  Classifier that Leverages a Universal Speech Model","summary":"  We propose a Perceiver-based sequence classifier to detect abnormalities in\nspeech reflective of several neurological disorders. We combine this classifier\nwith a Universal Speech Model (USM) that is trained (unsupervised) on 12\nmillion hours of diverse audio recordings. Our model compresses long sequences\ninto a small set of class-specific latent representations and a factorized\nprojection is used to predict different attributes of the disordered input\nspeech. The benefit of our approach is that it allows us to model different\nregions of the input for different classes and is at the same time data\nefficient. We evaluated the proposed model extensively on a curated corpus from\nthe Mayo Clinic. Our model outperforms standard transformer (80.9%) and\nperceiver (81.8%) models and achieves an average accuracy of 83.1%. With\nlimited task-specific data, we find that pretraining is important and\nsurprisingly pretraining with the unrelated automatic speech recognition (ASR)\ntask is also beneficial. Encodings from the middle layers provide a mix of both\nacoustic and phonetic information and achieve best prediction results compared\nto just using the final layer encodings (83.1% vs. 79.6%). The results are\npromising and with further refinements may help clinicians detect speech\nabnormalities without needing access to highly specialized speech-language\npathologists.\n","authors":["Hagen Soltau","Izhak Shafran","Alex Ottenwess","Joseph R. JR Duffy","Rene L. Utianski","Leland R. Barnard","John L. Stricker","Daniela Wiepert","David T. Jones","Hugo Botha"],"pdf_url":"https://arxiv.org/pdf/2310.13010v1.pdf","comment":null}]},"2023-10-15T00:00:00Z":{"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.07284v3","updated":"2023-10-15T03:58:29Z","published":"2023-10-11T08:17:54Z","title":"Typing to Listen at the Cocktail Party: Text-Guided Target Speaker\n  Extraction","summary":"  Humans possess an extraordinary ability to selectively focus on the sound\nsource of interest amidst complex acoustic environments, commonly referred to\nas cocktail party scenarios. In an attempt to replicate this remarkable\nauditory attention capability in machines, target speaker extraction (TSE)\nmodels have been developed. These models leverage the pre-registered cues of\nthe target speaker to extract the sound source of interest. However, the\neffectiveness of these models is hindered in real-world scenarios due to the\nunreliable or even absence of pre-registered cues. To address this limitation,\nthis study investigates the integration of natural language description to\nenhance the feasibility, controllability, and performance of existing TSE\nmodels. Specifically, we propose a model named LLM-TSE, wherein a large\nlanguage model (LLM) extracts useful semantic cues from the user's typed text\ninput. These cues can serve as independent extraction cues, task selectors to\ncontrol the TSE process or complement the pre-registered cues. Our experimental\nresults demonstrate competitive performance when only text-based cues are\npresented, the effectiveness of using input text as a task selector, and a new\nstate-of-the-art when combining text-based cues with pre-registered cues. To\nour knowledge, this is the first study to successfully incorporate LLMs to\nguide target speaker extraction, which can be a cornerstone for cocktail party\nproblem research.\n","authors":["Xiang Hao","Jibin Wu","Jianwei Yu","Chenglin Xu","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07284v3.pdf","comment":"Under review, https://github.com/haoxiangsnr/llm-tse"},{"id":"http://arxiv.org/abs/2310.09853v1","updated":"2023-10-15T15:00:00Z","published":"2023-10-15T15:00:00Z","title":"MERTech: Instrument Playing Technique Detection Using Self-Supervised\n  Pretrained Model With Multi-Task Finetuning","summary":"  Instrument playing techniques (IPTs) constitute a pivotal component of\nmusical expression. However, the development of automatic IPT detection methods\nsuffers from limited labeled data and inherent class imbalance issues. In this\npaper, we propose to apply a self-supervised learning model pre-trained on\nlarge-scale unlabeled music data and finetune it on IPT detection tasks. This\napproach addresses data scarcity and class imbalance challenges. Recognizing\nthe significance of pitch in capturing the nuances of IPTs and the importance\nof onset in locating IPT events, we investigate multi-task finetuning with\npitch and onset detection as auxiliary tasks. Additionally, we apply a\npost-processing approach for event-level prediction, where an IPT activation\ninitiates an event only if the onset output confirms an onset in that frame.\nOur method outperforms prior approaches in both frame-level and event-level\nmetrics across multiple IPT benchmark datasets. Further experiments demonstrate\nthe efficacy of multi-task finetuning on each IPT class.\n","authors":["Dichucheng Li","Yinghao Ma","Weixing Wei","Qiuqiang Kong","Yulun Wu","Mingjin Che","Fan Xia","Emmanouil Benetos","Wei Li"],"pdf_url":"https://arxiv.org/pdf/2310.09853v1.pdf","comment":"submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.09843v1","updated":"2023-10-15T14:04:48Z","published":"2023-10-15T14:04:48Z","title":"CoCoFormer: A controllable feature-rich polyphonic music generation\n  method","summary":"  This paper explores the modeling method of polyphonic music sequence. Due to\nthe great potential of Transformer models in music generation, controllable\nmusic generation is receiving more attention. In the task of polyphonic music,\ncurrent controllable generation research focuses on controlling the generation\nof chords, but lacks precise adjustment for the controllable generation of\nchoral music textures. This paper proposed Condition Choir Transformer\n(CoCoFormer) which controls the output of the model by controlling the chord\nand rhythm inputs at a fine-grained level. In this paper, the self-supervised\nmethod improves the loss function and performs joint training through\nconditional control input and unconditional input training. In order to\nalleviate the lack of diversity on generated samples caused by the teacher\nforcing training, this paper added an adversarial training method. CoCoFormer\nenhances model performance with explicit and implicit inputs to chords and\nrhythms. In this paper, the experiments proves that CoCoFormer has reached the\ncurrent better level than current models. On the premise of specifying the\npolyphonic music texture, the same melody can also be generated in a variety of\nways.\n","authors":["Jiuyang Zhou","Tengfei Niu","Hong Zhu","Xingping Wang"],"pdf_url":"https://arxiv.org/pdf/2310.09843v1.pdf","comment":null}],"Sound":[{"id":"http://arxiv.org/abs/2310.09853v1","updated":"2023-10-15T15:00:00Z","published":"2023-10-15T15:00:00Z","title":"MERTech: Instrument Playing Technique Detection Using Self-Supervised\n  Pretrained Model With Multi-Task Finetuning","summary":"  Instrument playing techniques (IPTs) constitute a pivotal component of\nmusical expression. However, the development of automatic IPT detection methods\nsuffers from limited labeled data and inherent class imbalance issues. In this\npaper, we propose to apply a self-supervised learning model pre-trained on\nlarge-scale unlabeled music data and finetune it on IPT detection tasks. This\napproach addresses data scarcity and class imbalance challenges. Recognizing\nthe significance of pitch in capturing the nuances of IPTs and the importance\nof onset in locating IPT events, we investigate multi-task finetuning with\npitch and onset detection as auxiliary tasks. Additionally, we apply a\npost-processing approach for event-level prediction, where an IPT activation\ninitiates an event only if the onset output confirms an onset in that frame.\nOur method outperforms prior approaches in both frame-level and event-level\nmetrics across multiple IPT benchmark datasets. Further experiments demonstrate\nthe efficacy of multi-task finetuning on each IPT class.\n","authors":["Dichucheng Li","Yinghao Ma","Weixing Wei","Qiuqiang Kong","Yulun Wu","Mingjin Che","Fan Xia","Emmanouil Benetos","Wei Li"],"pdf_url":"https://arxiv.org/pdf/2310.09853v1.pdf","comment":"submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.09843v1","updated":"2023-10-15T14:04:48Z","published":"2023-10-15T14:04:48Z","title":"CoCoFormer: A controllable feature-rich polyphonic music generation\n  method","summary":"  This paper explores the modeling method of polyphonic music sequence. Due to\nthe great potential of Transformer models in music generation, controllable\nmusic generation is receiving more attention. In the task of polyphonic music,\ncurrent controllable generation research focuses on controlling the generation\nof chords, but lacks precise adjustment for the controllable generation of\nchoral music textures. This paper proposed Condition Choir Transformer\n(CoCoFormer) which controls the output of the model by controlling the chord\nand rhythm inputs at a fine-grained level. In this paper, the self-supervised\nmethod improves the loss function and performs joint training through\nconditional control input and unconditional input training. In order to\nalleviate the lack of diversity on generated samples caused by the teacher\nforcing training, this paper added an adversarial training method. CoCoFormer\nenhances model performance with explicit and implicit inputs to chords and\nrhythms. In this paper, the experiments proves that CoCoFormer has reached the\ncurrent better level than current models. On the premise of specifying the\npolyphonic music texture, the same melody can also be generated in a variety of\nways.\n","authors":["Jiuyang Zhou","Tengfei Niu","Hong Zhu","Xingping Wang"],"pdf_url":"https://arxiv.org/pdf/2310.09843v1.pdf","comment":null}]},"2023-10-17T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2310.10497v2","updated":"2023-10-17T13:52:41Z","published":"2023-10-16T15:19:05Z","title":"LocSelect: Target Speaker Localization with an Auditory Selective\n  Hearing Mechanism","summary":"  The prevailing noise-resistant and reverberation-resistant localization\nalgorithms primarily emphasize separating and providing directional output for\neach speaker in multi-speaker scenarios, without association with the identity\nof speakers. In this paper, we present a target speaker localization algorithm\nwith a selective hearing mechanism. Given a reference speech of the target\nspeaker, we first produce a speaker-dependent spectrogram mask to eliminate\ninterfering speakers' speech. Subsequently, a Long short-term memory (LSTM)\nnetwork is employed to extract the target speaker's location from the filtered\nspectrogram. Experiments validate the superiority of our proposed method over\nthe existing algorithms for different scale invariant signal-to-noise ratios\n(SNR) conditions. Specifically, at SNR = -10 dB, our proposed network LocSelect\nachieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.\n","authors":["Yu Chen","Xinyuan Qian","Zexu Pan","Kainan Chen","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2310.10497v2.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.09680v2","updated":"2023-10-17T05:44:03Z","published":"2023-10-14T23:16:05Z","title":"Improved Contextual Recognition In Automatic Speech Recognition Systems\n  By Semantic Lattice Rescoring","summary":"  Automatic Speech Recognition (ASR) has witnessed a profound research\ninterest. Recent breakthroughs have given ASR systems different prospects such\nas faithfully transcribing spoken language, which is a pivotal advancement in\nbuilding conversational agents. However, there is still an imminent challenge\nof accurately discerning context-dependent words and phrases. In this work, we\npropose a novel approach for enhancing contextual recognition within ASR\nsystems via semantic lattice processing leveraging the power of deep learning\nmodels in accurately delivering spot-on transcriptions across a wide variety of\nvocabularies and speaking styles. Our solution consists of using Hidden Markov\nModels and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks\n(DNN) models integrating both language and acoustic modeling for better\naccuracy. We infused our network with the use of a transformer-based model to\nproperly rescore the word lattice achieving remarkable capabilities with a\npalpable reduction in Word Error Rate (WER). We demonstrate the effectiveness\nof our proposed framework on the LibriSpeech dataset with empirical analyses.\n","authors":["Ankitha Sudarshan","Vinay Samuel","Parth Patwa","Ibtihel Amara","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2310.09680v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.01250,\n  arXiv:2301.06735 by other authors"},{"id":"http://arxiv.org/abs/2310.11379v1","updated":"2023-10-17T16:22:18Z","published":"2023-10-17T16:22:18Z","title":"Robust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles","summary":"  Voice-based interfaces rely on a wake-up word mechanism to initiate\ncommunication with devices. However, achieving a robust, energy-efficient, and\nfast detection remains a challenge. This paper addresses these real production\nneeds by enhancing data with temporal alignments and using detection based on\ntwo phases with multi-resolution. It employs two models: a lightweight\non-device model for real-time processing of the audio stream and a verification\nmodel on the server-side, which is an ensemble of heterogeneous architectures\nthat refine detection. This scheme allows the optimization of two operating\npoints. To protect privacy, audio features are sent to the cloud instead of raw\naudio. The study investigated different parametric configurations for feature\nextraction to select one for on-device detection and another for the\nverification model. Furthermore, thirteen different audio classifiers were\ncompared in terms of performance and inference time. The proposed ensemble\noutperforms our stronger classifier in every noise condition.\n","authors":["Fernando L√≥pez","Jordi Luque","Carlos Segura","Pablo G√≥mez"],"pdf_url":"https://arxiv.org/pdf/2310.11379v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.11364v1","updated":"2023-10-17T16:02:07Z","published":"2023-10-17T16:02:07Z","title":"High-Fidelity Noise Reduction with Differentiable Signal Processing","summary":"  Noise reduction techniques based on deep learning have demonstrated\nimpressive performance in enhancing the overall quality of recorded speech.\nWhile these approaches are highly performant, their application in audio\nengineering can be limited due to a number of factors. These include operation\nonly on speech without support for music, lack of real-time capability, lack of\ninterpretable control parameters, operation at lower sample rates, and a\ntendency to introduce artifacts. On the other hand, signal processing-based\nnoise reduction algorithms offer fine-grained control and operation on a broad\nrange of content, however, they often require manual operation to achieve the\nbest results. To address the limitations of both approaches, in this work we\nintroduce a method that leverages a signal processing-based denoiser that when\ncombined with a neural network controller, enables fully automatic and\nhigh-fidelity noise reduction on both speech and music signals. We evaluate our\nproposed method with objective metrics and a perceptual listening test. Our\nevaluation reveals that speech enhancement models can be extended to music,\nhowever training the model to remove only stationary noise is critical.\nFurthermore, our proposed approach achieves performance on par with the deep\nlearning models, while being significantly more efficient and introducing fewer\nartifacts in some cases. Listening examples are available online at\nhttps://tape.it/research/denoiser .\n","authors":["Christian J. Steinmetz","Thomas Walther","Joshua D. Reiss"],"pdf_url":"https://arxiv.org/pdf/2310.11364v1.pdf","comment":"Accepted for publication at the 155th Convention of the Audio\n  Engineering Society"},{"id":"http://arxiv.org/abs/2305.09652v2","updated":"2023-10-17T14:59:28Z","published":"2023-05-16T17:53:03Z","title":"The Interpreter Understands Your Meaning: End-to-end Spoken Language\n  Understanding Aided by Speech Translation","summary":"  End-to-end spoken language understanding (SLU) remains elusive even with\ncurrent large pretrained language models on text and speech, especially in\nmultilingual cases. Machine translation has been established as a powerful\npretraining objective on text as it enables the model to capture high-level\nsemantics of the input utterance and associations between different languages,\nwhich is desired for speech models that work on lower-level acoustic frames.\nMotivated particularly by the task of cross-lingual SLU, we demonstrate that\nthe task of speech translation (ST) is a good means of pretraining speech\nmodels for end-to-end SLU on both intra- and cross-lingual scenarios.\n  By introducing ST, our models reach higher performance over baselines on\nmonolingual and multilingual intent classification as well as spoken question\nanswering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the\neffectiveness of our methods, we also create new benchmark datasets from both\nsynthetic and real sources, for speech summarization and low-resource/zero-shot\ntransfer from English to French or Spanish. We further show the value of\npreserving knowledge for the ST pretraining task for better downstream\nperformance, possibly using Bayesian transfer regularizers.\n","authors":["Mutian He","Philip N. Garner"],"pdf_url":"https://arxiv.org/pdf/2305.09652v2.pdf","comment":"16 pages, 3 figures; accepted by Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2309.14107v2","updated":"2023-10-17T13:38:27Z","published":"2023-09-25T13:00:33Z","title":"Wav2vec-based Detection and Severity Level Classification of Dysarthria\n  from Speech","summary":"  Automatic detection and severity level classification of dysarthria directly\nfrom acoustic speech signals can be used as a tool in medical diagnosis. In\nthis work, the pre-trained wav2vec 2.0 model is studied as a feature extractor\nto build detection and severity level classification systems for dysarthric\nspeech. The experiments were carried out with the popularly used UA-speech\ndatabase. In the detection experiments, the results revealed that the best\nperformance was obtained using the embeddings from the first layer of the\nwav2vec model that yielded an absolute improvement of 1.23% in accuracy\ncompared to the best performing baseline feature (spectrogram). In the studied\nseverity level classification task, the results revealed that the embeddings\nfrom the final layer gave an absolute improvement of 10.62% in accuracy\ncompared to the best baseline features (mel-frequency cepstral coefficients).\n","authors":["Farhad Javanmardi","Saska Tirronen","Manila Kodali","Sudarsana Reddy Kadiri","Paavo Alku"],"pdf_url":"https://arxiv.org/pdf/2309.14107v2.pdf","comment":"copyright 2023 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2309.14080v2","updated":"2023-10-17T13:36:36Z","published":"2023-09-25T12:14:25Z","title":"Analysis and Detection of Pathological Voice using Glottal Source\n  Features","summary":"  Automatic detection of voice pathology enables objective assessment and\nearlier intervention for the diagnosis. This study provides a systematic\nanalysis of glottal source features and investigates their effectiveness in\nvoice pathology detection. Glottal source features are extracted using glottal\nflows estimated with the quasi-closed phase (QCP) glottal inverse filtering\nmethod, using approximate glottal source signals computed with the zero\nfrequency filtering (ZFF) method, and using acoustic voice signals directly. In\naddition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from\nthe glottal source waveforms computed by QCP and ZFF to effectively capture the\nvariations in glottal source spectra of pathological voice. Experiments were\ncarried out using two databases, the Hospital Universitario Principe de\nAsturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database.\nAnalysis of features revealed that the glottal source contains information that\ndiscriminates normal and pathological voice. Pathology detection experiments\nwere carried out using support vector machine (SVM). From the detection\nexperiments it was observed that the performance achieved with the studied\nglottal source features is comparable or better than that of conventional MFCCs\nand perceptual linear prediction (PLP) features. The best detection performance\nwas achieved when the glottal source features were combined with the\nconventional MFCCs and PLP features, which indicates the complementary nature\nof the features.\n","authors":["Sudarsana Reddy Kadiri","Paavo Alku"],"pdf_url":"https://arxiv.org/pdf/2309.14080v2.pdf","comment":"Copyright 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2309.08157v2","updated":"2023-10-17T12:28:42Z","published":"2023-09-15T04:48:10Z","title":"RVAE-EM: Generative speech dereverberation based on recurrent\n  variational auto-encoder and convolutive transfer function","summary":"  In indoor scenes, reverberation is a crucial factor in degrading the\nperceived quality and intelligibility of speech. In this work, we propose a\ngenerative dereverberation method. Our approach is based on a probabilistic\nmodel utilizing a recurrent variational auto-encoder (RVAE) network and the\nconvolutive transfer function (CTF) approximation. Different from most previous\napproaches, the output of our RVAE serves as the prior of the clean speech. And\nour target is the maximum a posteriori (MAP) estimation of clean speech, which\nis achieved iteratively through the expectation maximization (EM) algorithm.\nThe proposed method integrates the capabilities of network-based speech prior\nmodelling and CTF-based observation modelling. Experiments on single-channel\nspeech dereverberation show that the proposed generative method noticeably\noutperforms the advanced discriminative networks.\n","authors":["Pengyu Wang","Xiaofei Li"],"pdf_url":"https://arxiv.org/pdf/2309.08157v2.pdf","comment":"Submitted to ICASSP2024"},{"id":"http://arxiv.org/abs/2310.11165v1","updated":"2023-10-17T11:31:29Z","published":"2023-10-17T11:31:29Z","title":"Serenade: A Model for Human-in-the-loop Automatic Chord Estimation","summary":"  Computational harmony analysis is important for MIR tasks such as automatic\nsegmentation, corpus analysis and automatic chord label estimation. However,\nrecent research into the ambiguous nature of musical harmony, causing limited\ninter-rater agreement, has made apparent that there is a glass ceiling for\ncommon metrics such as accuracy. Commonly, these issues are addressed either in\nthe training data itself by creating majority-rule annotations or during the\ntraining phase by learning soft targets. We propose a novel alternative\napproach in which a human and an autoregressive model together co-create a\nharmonic annotation for an audio track. After automatically generating harmony\npredictions, a human sparsely annotates parts with low model confidence and the\nmodel then adjusts its predictions following human guidance. We evaluate our\nmodel on a dataset of popular music and we show that, with this\nhuman-in-the-loop approach, harmonic analysis performance improves over a\nmodel-only approach. The human contribution is amplified by the second,\nconstrained prediction of the model.\n","authors":["Hendrik Vincent Koops","Gianluca Micchi","Ilaria Manco","Elio Quinton"],"pdf_url":"https://arxiv.org/pdf/2310.11165v1.pdf","comment":"Accepted at MMRP23. 7 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2310.11160v1","updated":"2023-10-17T11:26:28Z","published":"2023-10-17T11:26:28Z","title":"Leveraging Content-based Features from Multiple Acoustic Models for\n  Singing Voice Conversion","summary":"  Singing voice conversion (SVC) is a technique to enable an arbitrary singer\nto sing an arbitrary song. To achieve that, it is important to obtain\nspeaker-agnostic representations from source audio, which is a challenging\ntask. A common solution is to extract content-based features (e.g., PPGs) from\na pretrained acoustic model. However, the choices for acoustic models are vast\nand varied. It is yet to be explored what characteristics of content features\nfrom different acoustic models are, and whether integrating multiple content\nfeatures can help each other. Motivated by that, this study investigates three\ndistinct content features, sourcing from WeNet, Whisper, and ContentVec,\nrespectively. We explore their complementary roles in intelligibility, prosody,\nand conversion similarity for SVC. By integrating the multiple content features\nwith a diffusion-based SVC model, our SVC system achieves superior conversion\nperformance on both objective and subjective evaluation in comparison to a\nsingle source of content features. Our demo page and code can be available\nhttps://www.zhangxueyao.com/data/MultipleContentsSVC/index.html.\n","authors":["Xueyao Zhang","Yicheng Gu","Haopeng Chen","Zihao Fang","Lexiao Zou","Liumeng Xue","Zhizheng Wu"],"pdf_url":"https://arxiv.org/pdf/2310.11160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11141v1","updated":"2023-10-17T10:44:05Z","published":"2023-10-17T10:44:05Z","title":"Long-form Simultaneous Speech Translation: Thesis Proposal","summary":"  Simultaneous speech translation (SST) aims to provide real-time translation\nof spoken language, even before the speaker finishes their sentence.\nTraditionally, SST has been addressed primarily by cascaded systems that\ndecompose the task into subtasks, including speech recognition, segmentation,\nand machine translation. However, the advent of deep learning has sparked\nsignificant interest in end-to-end (E2E) systems. Nevertheless, a major\nlimitation of most approaches to E2E SST reported in the current literature is\nthat they assume that the source speech is pre-segmented into sentences, which\nis a significant obstacle for practical, real-world applications. This thesis\nproposal addresses end-to-end simultaneous speech translation, particularly in\nthe long-form setting, i.e., without pre-segmentation. We present a survey of\nthe latest advancements in E2E SST, assess the primary obstacles in SST and its\nrelevance to long-form scenarios, and suggest approaches to tackle these\nchallenges.\n","authors":["Peter Pol√°k"],"pdf_url":"https://arxiv.org/pdf/2310.11141v1.pdf","comment":"IJCNLP-AACL SRW 2023 - camera-ready version"},{"id":"http://arxiv.org/abs/2310.11069v1","updated":"2023-10-17T08:33:02Z","published":"2023-10-17T08:33:02Z","title":"VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System","summary":"  Arabic is a complex language with many varieties and dialects spoken by over\n450 millions all around the world. Due to the linguistic diversity and\nvariations, it is challenging to build a robust and generalized ASR system for\nArabic. In this work, we address this gap by developing and demoing a system,\ndubbed VoxArabica, for dialect identification (DID) as well as automatic speech\nrecognition (ASR) of Arabic. We train a wide range of models such as HuBERT\n(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR\ntasks. Our DID models are trained to identify 17 different dialects in addition\nto MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.\nAdditionally, for the remaining dialects in ASR, we provide the option to\nchoose various models such as Whisper and MMS in a zero-shot setting. We\nintegrate these models into a single web interface with diverse features such\nas audio recording, file upload, model selection, and the option to raise flags\nfor incorrect outputs. Overall, we believe VoxArabica will be useful for a wide\nrange of audiences concerned with Arabic research. Our system is currently\nrunning at https://cdce-206-12-100-168.ngrok.io/.\n","authors":["Abdul Waheed","Bashar Talafha","Peter Suvellin","Abdelrahman Elmadney","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2310.11069v1.pdf","comment":"Accepted at ArabicNLP conference co-located with EMNLP'23"},{"id":"http://arxiv.org/abs/2305.19130v3","updated":"2023-10-17T08:01:34Z","published":"2023-05-30T15:41:47Z","title":"Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using\n  Spatial Transformer Networks","summary":"  Thanks to the latest deep learning algorithms, silent speech interfaces (SSI)\nare now able to synthesize intelligible speech from articulatory movement data\nunder certain conditions. However, the resulting models are rather\nspeaker-specific, making a quick switch between users troublesome. Even for the\nsame speaker, these models perform poorly cross-session, i.e. after dismounting\nand re-mounting the recording equipment. To aid quick speaker and session\nadaptation of ultrasound tongue imaging-based SSI models, we extend our deep\nnetworks with a spatial transformer network (STN) module, capable of performing\nan affine transformation on the input images. Although the STN part takes up\nonly about 10% of the network, our experiments show that adapting just the STN\nmodule might allow to reduce MSE by 88% on the average, compared to retraining\nthe whole network. The improvement is even larger (around 92%) when adapting\nthe network to different recording sessions from the same speaker.\n","authors":["L√°szl√≥ T√≥th","Amin Honarmandi Shandiz","G√°bor Gosztolya","Csap√≥ Tam√°s G√°bor"],"pdf_url":"https://arxiv.org/pdf/2305.19130v3.pdf","comment":"5 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.11035v1","updated":"2023-10-17T07:02:26Z","published":"2023-10-17T07:02:26Z","title":"Lyricist-Singer Entropy Affects Lyric-Lyricist Classification\n  Performance","summary":"  Although lyrics represent an essential component of music, few music\ninformation processing studies have been conducted on the characteristics of\nlyricists. Because these characteristics may be valuable for musical\napplications, such as recommendations, they warrant further study. We\nconsidered a potential method that extracts features representing the\ncharacteristics of lyricists from lyrics. Because these features must be\nidentified prior to extraction, we focused on lyricists with easily\nidentifiable features. We believe that it is desirable for singers to perform\nunique songs that share certain characteristics specific to the singer.\nAccordingly, we hypothesized that lyricists account for the unique\ncharacteristics of the singers they write lyrics for. In other words,\nlyric-lyricist classification performance or the ease of capturing the features\nof a lyricist from the lyrics may depend on the variety of singers. In this\nstudy, we observed a relationship between lyricist-singer entropy or the\nvariety of singers associated with a single lyricist and lyric-lyricist\nclassification performance. As an example, the lyricist-singer entropy is\nminimal when the lyricist writes lyrics for only one singer. In our\nexperiments, we grouped lyricists among five groups in terms of lyricist-singer\nentropy and assessed the lyric-lyricist classification performance within each\ngroup. Consequently, the best F1 score was obtained for the group with the\nlowest lyricist-singer entropy. Our results suggest that further analyses of\nthe features contributing to lyric-lyricist classification performance on the\nlowest lyricist-singer entropy group may improve the feature extraction task\nfor lyricists.\n","authors":["Mitsuki Morita","Masato Kikuchi","Tadachika Ozono"],"pdf_url":"https://arxiv.org/pdf/2310.11035v1.pdf","comment":"The 10th International Conference on Advanced Informatics: Concepts,\n  Theory and Applications (ICAICTA 2023)"},{"id":"http://arxiv.org/abs/2310.11003v1","updated":"2023-10-17T05:10:39Z","published":"2023-10-17T05:10:39Z","title":"Correction Focused Language Model Training for Speech Recognition","summary":"  Language models (LMs) have been commonly adopted to boost the performance of\nautomatic speech recognition (ASR) particularly in domain adaptation tasks.\nConventional way of LM training treats all the words in corpora equally,\nresulting in suboptimal improvements in ASR performance. In this work, we\nintroduce a novel correction focused LM training approach which aims to\nprioritize ASR fallible words. The word-level ASR fallibility score,\nrepresenting the likelihood of ASR mis-recognition, is defined and shaped as a\nprior word distribution to guide the LM training. To enable correction focused\ntraining with text-only corpora, large language models (LLMs) are employed as\nfallibility score predictors and text generators through multi-task\nfine-tuning. Experimental results for domain adaptation tasks demonstrate the\neffectiveness of our proposed method. Compared with conventional LMs,\ncorrection focused training achieves up to relatively 5.5% word error rate\n(WER) reduction in sufficient text scenarios. In insufficient text scenarios,\nLM training with LLM-generated text achieves up to relatively 13% WER\nreduction, while correction focused training further obtains up to relatively\n6% WER reduction.\n","authors":["Yingyi Ma","Zhe Liu","Ozlem Kalinli"],"pdf_url":"https://arxiv.org/pdf/2310.11003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09623v2","updated":"2023-10-17T05:08:47Z","published":"2023-09-18T09:52:54Z","title":"HumTrans: A Novel Open-Source Dataset for Humming Melody Transcription\n  and Beyond","summary":"  This paper introduces the HumTrans dataset, which is publicly available and\nprimarily designed for humming melody transcription. The dataset can also serve\nas a foundation for downstream tasks such as humming melody based music\ngeneration. It consists of 500 musical compositions of different genres and\nlanguages, with each composition divided into multiple segments. In total, the\ndataset comprises 1000 music segments. To collect this humming dataset, we\nemployed 10 college students, all of whom are either music majors or proficient\nin playing at least one musical instrument. Each of them hummed every segment\ntwice using the web recording interface provided by our designed website. The\nhumming recordings were sampled at a frequency of 44,100 Hz. During the humming\nsession, the main interface provides a musical score for students to reference,\nwith the melody audio playing simultaneously to aid in capturing both melody\nand rhythm. The dataset encompasses approximately 56.22 hours of audio, making\nit the largest known humming dataset to date. The dataset will be released on\nHugging Face, and we will provide a GitHub repository containing baseline\nresults and evaluation codes.\n","authors":["Shansong Liu","Xu Li","Dian Li","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2309.09623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10992v1","updated":"2023-10-17T04:30:37Z","published":"2023-10-17T04:30:37Z","title":"A High Fidelity and Low Complexity Neural Audio Coding","summary":"  Audio coding is an essential module in the real-time communication system.\nNeural audio codecs can compress audio samples with a low bitrate due to the\nstrong modeling and generative capabilities of deep neural networks. To address\nthe poor high-frequency expression and high computational cost and storage\nconsumption, we proposed an integrated framework that utilizes a neural network\nto model wide-band components and adopts traditional signal processing to\ncompress high-band components according to psychological hearing knowledge.\nInspired by auditory perception theory, a perception-based loss function is\ndesigned to improve harmonic modeling. Besides, generative adversarial network\n(GAN) compression is proposed for the first time for neural audio codecs. Our\nmethod is superior to prior advanced neural codecs across subjective and\nobjective metrics and allows real-time inference on desktop and mobile.\n","authors":["Wenzhe Liu","Wei Xiao","Meng Wang","Shan Yang","Yupeng Shi","Yuyong Kang","Dan Su","Shidong Shang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2310.10992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10922v1","updated":"2023-10-17T01:31:59Z","published":"2023-10-17T01:31:59Z","title":"Spatial HuBERT: Self-supervised Spatial Speech Representation Learning\n  for a Single Talker from Multi-channel Audio","summary":"  Self-supervised learning has been used to leverage unlabelled data, improving\naccuracy and generalisation of speech systems through the training of\nrepresentation models. While many recent works have sought to produce effective\nrepresentations across a variety of acoustic domains, languages, modalities and\neven simultaneous speakers, these studies have all been limited to\nsingle-channel audio recordings. This paper presents Spatial HuBERT, a\nself-supervised speech representation model that learns both acoustic and\nspatial information pertaining to a single speaker in a potentially noisy\nenvironment by using multi-channel audio inputs. Spatial HuBERT learns\nrepresentations that outperform state-of-the-art single-channel speech\nrepresentations on a variety of spatial downstream tasks, particularly in\nreverberant and noisy environments. We also demonstrate the utility of the\nrepresentations learned by Spatial HuBERT on a speech localisation downstream\ntask. Along with this paper, we publicly release a new dataset of 100 000\nsimulated first-order ambisonics room impulse responses.\n","authors":["Antoni Dimitriadis","Siqi Pan","Vidhyasaharan Sethu","Beena Ahmed"],"pdf_url":"https://arxiv.org/pdf/2310.10922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13013v1","updated":"2023-10-17T14:49:48Z","published":"2023-10-17T14:49:48Z","title":"Generative error correction for code-switching speech recognition using\n  large language models","summary":"  Code-switching (CS) speech refers to the phenomenon of mixing two or more\nlanguages within the same sentence. Despite the recent advances in automatic\nspeech recognition (ASR), CS-ASR is still a challenging task ought to the\ngrammatical structure complexity of the phenomenon and the data scarcity of\nspecific training corpus. In this work, we propose to leverage large language\nmodels (LLMs) and lists of hypotheses generated by an ASR to address the CS\nproblem. Specifically, we first employ multiple well-trained ASR models for\nN-best hypotheses generation, with the aim of increasing the diverse and\ninformative elements in the set of hypotheses. Next, we utilize the LLMs to\nlearn the hypotheses-to-transcription (H2T) mapping by adding a trainable\nlow-rank adapter. Such a generative error correction (GER) method directly\npredicts the accurate transcription according to its expert linguistic\nknowledge and N-best hypotheses, resulting in a paradigm shift from the\ntraditional language model rescoring or error correction techniques.\nExperimental evidence demonstrates that GER significantly enhances CS-ASR\naccuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show\nremarkable data efficiency for H2T learning, providing a potential solution to\nthe data scarcity problem of CS-ASR in low-resource languages.\n","authors":["Chen Chen","Yuchen Hu","Chao-Han Huck Yang","Hexin Liu","Sabato Marco Siniscalchi","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2310.13013v1.pdf","comment":"Submitted to ICASSP2024"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.10497v2","updated":"2023-10-17T13:52:41Z","published":"2023-10-16T15:19:05Z","title":"LocSelect: Target Speaker Localization with an Auditory Selective\n  Hearing Mechanism","summary":"  The prevailing noise-resistant and reverberation-resistant localization\nalgorithms primarily emphasize separating and providing directional output for\neach speaker in multi-speaker scenarios, without association with the identity\nof speakers. In this paper, we present a target speaker localization algorithm\nwith a selective hearing mechanism. Given a reference speech of the target\nspeaker, we first produce a speaker-dependent spectrogram mask to eliminate\ninterfering speakers' speech. Subsequently, a Long short-term memory (LSTM)\nnetwork is employed to extract the target speaker's location from the filtered\nspectrogram. Experiments validate the superiority of our proposed method over\nthe existing algorithms for different scale invariant signal-to-noise ratios\n(SNR) conditions. Specifically, at SNR = -10 dB, our proposed network LocSelect\nachieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.\n","authors":["Yu Chen","Xinyuan Qian","Zexu Pan","Kainan Chen","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2310.10497v2.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.09680v2","updated":"2023-10-17T05:44:03Z","published":"2023-10-14T23:16:05Z","title":"Improved Contextual Recognition In Automatic Speech Recognition Systems\n  By Semantic Lattice Rescoring","summary":"  Automatic Speech Recognition (ASR) has witnessed a profound research\ninterest. Recent breakthroughs have given ASR systems different prospects such\nas faithfully transcribing spoken language, which is a pivotal advancement in\nbuilding conversational agents. However, there is still an imminent challenge\nof accurately discerning context-dependent words and phrases. In this work, we\npropose a novel approach for enhancing contextual recognition within ASR\nsystems via semantic lattice processing leveraging the power of deep learning\nmodels in accurately delivering spot-on transcriptions across a wide variety of\nvocabularies and speaking styles. Our solution consists of using Hidden Markov\nModels and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks\n(DNN) models integrating both language and acoustic modeling for better\naccuracy. We infused our network with the use of a transformer-based model to\nproperly rescore the word lattice achieving remarkable capabilities with a\npalpable reduction in Word Error Rate (WER). We demonstrate the effectiveness\nof our proposed framework on the LibriSpeech dataset with empirical analyses.\n","authors":["Ankitha Sudarshan","Vinay Samuel","Parth Patwa","Ibtihel Amara","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2310.09680v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.01250,\n  arXiv:2301.06735 by other authors"},{"id":"http://arxiv.org/abs/2310.11379v1","updated":"2023-10-17T16:22:18Z","published":"2023-10-17T16:22:18Z","title":"Robust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles","summary":"  Voice-based interfaces rely on a wake-up word mechanism to initiate\ncommunication with devices. However, achieving a robust, energy-efficient, and\nfast detection remains a challenge. This paper addresses these real production\nneeds by enhancing data with temporal alignments and using detection based on\ntwo phases with multi-resolution. It employs two models: a lightweight\non-device model for real-time processing of the audio stream and a verification\nmodel on the server-side, which is an ensemble of heterogeneous architectures\nthat refine detection. This scheme allows the optimization of two operating\npoints. To protect privacy, audio features are sent to the cloud instead of raw\naudio. The study investigated different parametric configurations for feature\nextraction to select one for on-device detection and another for the\nverification model. Furthermore, thirteen different audio classifiers were\ncompared in terms of performance and inference time. The proposed ensemble\noutperforms our stronger classifier in every noise condition.\n","authors":["Fernando L√≥pez","Jordi Luque","Carlos Segura","Pablo G√≥mez"],"pdf_url":"https://arxiv.org/pdf/2310.11379v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.11364v1","updated":"2023-10-17T16:02:07Z","published":"2023-10-17T16:02:07Z","title":"High-Fidelity Noise Reduction with Differentiable Signal Processing","summary":"  Noise reduction techniques based on deep learning have demonstrated\nimpressive performance in enhancing the overall quality of recorded speech.\nWhile these approaches are highly performant, their application in audio\nengineering can be limited due to a number of factors. These include operation\nonly on speech without support for music, lack of real-time capability, lack of\ninterpretable control parameters, operation at lower sample rates, and a\ntendency to introduce artifacts. On the other hand, signal processing-based\nnoise reduction algorithms offer fine-grained control and operation on a broad\nrange of content, however, they often require manual operation to achieve the\nbest results. To address the limitations of both approaches, in this work we\nintroduce a method that leverages a signal processing-based denoiser that when\ncombined with a neural network controller, enables fully automatic and\nhigh-fidelity noise reduction on both speech and music signals. We evaluate our\nproposed method with objective metrics and a perceptual listening test. Our\nevaluation reveals that speech enhancement models can be extended to music,\nhowever training the model to remove only stationary noise is critical.\nFurthermore, our proposed approach achieves performance on par with the deep\nlearning models, while being significantly more efficient and introducing fewer\nartifacts in some cases. Listening examples are available online at\nhttps://tape.it/research/denoiser .\n","authors":["Christian J. Steinmetz","Thomas Walther","Joshua D. Reiss"],"pdf_url":"https://arxiv.org/pdf/2310.11364v1.pdf","comment":"Accepted for publication at the 155th Convention of the Audio\n  Engineering Society"},{"id":"http://arxiv.org/abs/2305.09652v2","updated":"2023-10-17T14:59:28Z","published":"2023-05-16T17:53:03Z","title":"The Interpreter Understands Your Meaning: End-to-end Spoken Language\n  Understanding Aided by Speech Translation","summary":"  End-to-end spoken language understanding (SLU) remains elusive even with\ncurrent large pretrained language models on text and speech, especially in\nmultilingual cases. Machine translation has been established as a powerful\npretraining objective on text as it enables the model to capture high-level\nsemantics of the input utterance and associations between different languages,\nwhich is desired for speech models that work on lower-level acoustic frames.\nMotivated particularly by the task of cross-lingual SLU, we demonstrate that\nthe task of speech translation (ST) is a good means of pretraining speech\nmodels for end-to-end SLU on both intra- and cross-lingual scenarios.\n  By introducing ST, our models reach higher performance over baselines on\nmonolingual and multilingual intent classification as well as spoken question\nanswering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the\neffectiveness of our methods, we also create new benchmark datasets from both\nsynthetic and real sources, for speech summarization and low-resource/zero-shot\ntransfer from English to French or Spanish. We further show the value of\npreserving knowledge for the ST pretraining task for better downstream\nperformance, possibly using Bayesian transfer regularizers.\n","authors":["Mutian He","Philip N. Garner"],"pdf_url":"https://arxiv.org/pdf/2305.09652v2.pdf","comment":"16 pages, 3 figures; accepted by Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2309.14107v2","updated":"2023-10-17T13:38:27Z","published":"2023-09-25T13:00:33Z","title":"Wav2vec-based Detection and Severity Level Classification of Dysarthria\n  from Speech","summary":"  Automatic detection and severity level classification of dysarthria directly\nfrom acoustic speech signals can be used as a tool in medical diagnosis. In\nthis work, the pre-trained wav2vec 2.0 model is studied as a feature extractor\nto build detection and severity level classification systems for dysarthric\nspeech. The experiments were carried out with the popularly used UA-speech\ndatabase. In the detection experiments, the results revealed that the best\nperformance was obtained using the embeddings from the first layer of the\nwav2vec model that yielded an absolute improvement of 1.23% in accuracy\ncompared to the best performing baseline feature (spectrogram). In the studied\nseverity level classification task, the results revealed that the embeddings\nfrom the final layer gave an absolute improvement of 10.62% in accuracy\ncompared to the best baseline features (mel-frequency cepstral coefficients).\n","authors":["Farhad Javanmardi","Saska Tirronen","Manila Kodali","Sudarsana Reddy Kadiri","Paavo Alku"],"pdf_url":"https://arxiv.org/pdf/2309.14107v2.pdf","comment":"copyright 2023 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2309.14080v2","updated":"2023-10-17T13:36:36Z","published":"2023-09-25T12:14:25Z","title":"Analysis and Detection of Pathological Voice using Glottal Source\n  Features","summary":"  Automatic detection of voice pathology enables objective assessment and\nearlier intervention for the diagnosis. This study provides a systematic\nanalysis of glottal source features and investigates their effectiveness in\nvoice pathology detection. Glottal source features are extracted using glottal\nflows estimated with the quasi-closed phase (QCP) glottal inverse filtering\nmethod, using approximate glottal source signals computed with the zero\nfrequency filtering (ZFF) method, and using acoustic voice signals directly. In\naddition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from\nthe glottal source waveforms computed by QCP and ZFF to effectively capture the\nvariations in glottal source spectra of pathological voice. Experiments were\ncarried out using two databases, the Hospital Universitario Principe de\nAsturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database.\nAnalysis of features revealed that the glottal source contains information that\ndiscriminates normal and pathological voice. Pathology detection experiments\nwere carried out using support vector machine (SVM). From the detection\nexperiments it was observed that the performance achieved with the studied\nglottal source features is comparable or better than that of conventional MFCCs\nand perceptual linear prediction (PLP) features. The best detection performance\nwas achieved when the glottal source features were combined with the\nconventional MFCCs and PLP features, which indicates the complementary nature\nof the features.\n","authors":["Sudarsana Reddy Kadiri","Paavo Alku"],"pdf_url":"https://arxiv.org/pdf/2309.14080v2.pdf","comment":"Copyright 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2310.11230v1","updated":"2023-10-17T13:01:10Z","published":"2023-10-17T13:01:10Z","title":"Zipformer: A faster and better encoder for automatic speech recognition","summary":"  The Conformer has become the most popular encoder model for automatic speech\nrecognition (ASR). It adds convolution modules to a transformer to learn both\nlocal and global dependencies. In this work we describe a faster, more\nmemory-efficient, and better-performing transformer, called Zipformer. Modeling\nchanges include: 1) a U-Net-like encoder structure where middle stacks operate\nat lower frame rates; 2) reorganized block structure with more modules, within\nwhich we re-use attention weights for efficiency; 3) a modified form of\nLayerNorm called BiasNorm allows us to retain some length information; 4) new\nactivation functions SwooshR and SwooshL work better than Swish. We also\npropose a new optimizer, called ScaledAdam, which scales the update by each\ntensor's current scale to keep the relative change about the same, and also\nexplictly learns the parameter scale. It achieves faster convergence and better\nperformance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and\nWenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer\nover other state-of-the-art ASR models. Our code is publicly available at\nhttps://github.com/k2-fsa/icefall.\n","authors":["Zengwei Yao","Liyong Guo","Xiaoyu Yang","Wei Kang","Fangjun Kuang","Yifan Yang","Zengrui Jin","Long Lin","Daniel Povey"],"pdf_url":"https://arxiv.org/pdf/2310.11230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08157v2","updated":"2023-10-17T12:28:42Z","published":"2023-09-15T04:48:10Z","title":"RVAE-EM: Generative speech dereverberation based on recurrent\n  variational auto-encoder and convolutive transfer function","summary":"  In indoor scenes, reverberation is a crucial factor in degrading the\nperceived quality and intelligibility of speech. In this work, we propose a\ngenerative dereverberation method. Our approach is based on a probabilistic\nmodel utilizing a recurrent variational auto-encoder (RVAE) network and the\nconvolutive transfer function (CTF) approximation. Different from most previous\napproaches, the output of our RVAE serves as the prior of the clean speech. And\nour target is the maximum a posteriori (MAP) estimation of clean speech, which\nis achieved iteratively through the expectation maximization (EM) algorithm.\nThe proposed method integrates the capabilities of network-based speech prior\nmodelling and CTF-based observation modelling. Experiments on single-channel\nspeech dereverberation show that the proposed generative method noticeably\noutperforms the advanced discriminative networks.\n","authors":["Pengyu Wang","Xiaofei Li"],"pdf_url":"https://arxiv.org/pdf/2309.08157v2.pdf","comment":"Submitted to ICASSP2024"},{"id":"http://arxiv.org/abs/2310.11165v1","updated":"2023-10-17T11:31:29Z","published":"2023-10-17T11:31:29Z","title":"Serenade: A Model for Human-in-the-loop Automatic Chord Estimation","summary":"  Computational harmony analysis is important for MIR tasks such as automatic\nsegmentation, corpus analysis and automatic chord label estimation. However,\nrecent research into the ambiguous nature of musical harmony, causing limited\ninter-rater agreement, has made apparent that there is a glass ceiling for\ncommon metrics such as accuracy. Commonly, these issues are addressed either in\nthe training data itself by creating majority-rule annotations or during the\ntraining phase by learning soft targets. We propose a novel alternative\napproach in which a human and an autoregressive model together co-create a\nharmonic annotation for an audio track. After automatically generating harmony\npredictions, a human sparsely annotates parts with low model confidence and the\nmodel then adjusts its predictions following human guidance. We evaluate our\nmodel on a dataset of popular music and we show that, with this\nhuman-in-the-loop approach, harmonic analysis performance improves over a\nmodel-only approach. The human contribution is amplified by the second,\nconstrained prediction of the model.\n","authors":["Hendrik Vincent Koops","Gianluca Micchi","Ilaria Manco","Elio Quinton"],"pdf_url":"https://arxiv.org/pdf/2310.11165v1.pdf","comment":"Accepted at MMRP23. 7 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2310.11160v1","updated":"2023-10-17T11:26:28Z","published":"2023-10-17T11:26:28Z","title":"Leveraging Content-based Features from Multiple Acoustic Models for\n  Singing Voice Conversion","summary":"  Singing voice conversion (SVC) is a technique to enable an arbitrary singer\nto sing an arbitrary song. To achieve that, it is important to obtain\nspeaker-agnostic representations from source audio, which is a challenging\ntask. A common solution is to extract content-based features (e.g., PPGs) from\na pretrained acoustic model. However, the choices for acoustic models are vast\nand varied. It is yet to be explored what characteristics of content features\nfrom different acoustic models are, and whether integrating multiple content\nfeatures can help each other. Motivated by that, this study investigates three\ndistinct content features, sourcing from WeNet, Whisper, and ContentVec,\nrespectively. We explore their complementary roles in intelligibility, prosody,\nand conversion similarity for SVC. By integrating the multiple content features\nwith a diffusion-based SVC model, our SVC system achieves superior conversion\nperformance on both objective and subjective evaluation in comparison to a\nsingle source of content features. Our demo page and code can be available\nhttps://www.zhangxueyao.com/data/MultipleContentsSVC/index.html.\n","authors":["Xueyao Zhang","Yicheng Gu","Haopeng Chen","Zihao Fang","Lexiao Zou","Liumeng Xue","Zhizheng Wu"],"pdf_url":"https://arxiv.org/pdf/2310.11160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11141v1","updated":"2023-10-17T10:44:05Z","published":"2023-10-17T10:44:05Z","title":"Long-form Simultaneous Speech Translation: Thesis Proposal","summary":"  Simultaneous speech translation (SST) aims to provide real-time translation\nof spoken language, even before the speaker finishes their sentence.\nTraditionally, SST has been addressed primarily by cascaded systems that\ndecompose the task into subtasks, including speech recognition, segmentation,\nand machine translation. However, the advent of deep learning has sparked\nsignificant interest in end-to-end (E2E) systems. Nevertheless, a major\nlimitation of most approaches to E2E SST reported in the current literature is\nthat they assume that the source speech is pre-segmented into sentences, which\nis a significant obstacle for practical, real-world applications. This thesis\nproposal addresses end-to-end simultaneous speech translation, particularly in\nthe long-form setting, i.e., without pre-segmentation. We present a survey of\nthe latest advancements in E2E SST, assess the primary obstacles in SST and its\nrelevance to long-form scenarios, and suggest approaches to tackle these\nchallenges.\n","authors":["Peter Pol√°k"],"pdf_url":"https://arxiv.org/pdf/2310.11141v1.pdf","comment":"IJCNLP-AACL SRW 2023 - camera-ready version"},{"id":"http://arxiv.org/abs/2310.11069v1","updated":"2023-10-17T08:33:02Z","published":"2023-10-17T08:33:02Z","title":"VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System","summary":"  Arabic is a complex language with many varieties and dialects spoken by over\n450 millions all around the world. Due to the linguistic diversity and\nvariations, it is challenging to build a robust and generalized ASR system for\nArabic. In this work, we address this gap by developing and demoing a system,\ndubbed VoxArabica, for dialect identification (DID) as well as automatic speech\nrecognition (ASR) of Arabic. We train a wide range of models such as HuBERT\n(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR\ntasks. Our DID models are trained to identify 17 different dialects in addition\nto MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.\nAdditionally, for the remaining dialects in ASR, we provide the option to\nchoose various models such as Whisper and MMS in a zero-shot setting. We\nintegrate these models into a single web interface with diverse features such\nas audio recording, file upload, model selection, and the option to raise flags\nfor incorrect outputs. Overall, we believe VoxArabica will be useful for a wide\nrange of audiences concerned with Arabic research. Our system is currently\nrunning at https://cdce-206-12-100-168.ngrok.io/.\n","authors":["Abdul Waheed","Bashar Talafha","Peter Suvellin","Abdelrahman Elmadney","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2310.11069v1.pdf","comment":"Accepted at ArabicNLP conference co-located with EMNLP'23"},{"id":"http://arxiv.org/abs/2305.19130v3","updated":"2023-10-17T08:01:34Z","published":"2023-05-30T15:41:47Z","title":"Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using\n  Spatial Transformer Networks","summary":"  Thanks to the latest deep learning algorithms, silent speech interfaces (SSI)\nare now able to synthesize intelligible speech from articulatory movement data\nunder certain conditions. However, the resulting models are rather\nspeaker-specific, making a quick switch between users troublesome. Even for the\nsame speaker, these models perform poorly cross-session, i.e. after dismounting\nand re-mounting the recording equipment. To aid quick speaker and session\nadaptation of ultrasound tongue imaging-based SSI models, we extend our deep\nnetworks with a spatial transformer network (STN) module, capable of performing\nan affine transformation on the input images. Although the STN part takes up\nonly about 10% of the network, our experiments show that adapting just the STN\nmodule might allow to reduce MSE by 88% on the average, compared to retraining\nthe whole network. The improvement is even larger (around 92%) when adapting\nthe network to different recording sessions from the same speaker.\n","authors":["L√°szl√≥ T√≥th","Amin Honarmandi Shandiz","G√°bor Gosztolya","Csap√≥ Tam√°s G√°bor"],"pdf_url":"https://arxiv.org/pdf/2305.19130v3.pdf","comment":"5 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.11035v1","updated":"2023-10-17T07:02:26Z","published":"2023-10-17T07:02:26Z","title":"Lyricist-Singer Entropy Affects Lyric-Lyricist Classification\n  Performance","summary":"  Although lyrics represent an essential component of music, few music\ninformation processing studies have been conducted on the characteristics of\nlyricists. Because these characteristics may be valuable for musical\napplications, such as recommendations, they warrant further study. We\nconsidered a potential method that extracts features representing the\ncharacteristics of lyricists from lyrics. Because these features must be\nidentified prior to extraction, we focused on lyricists with easily\nidentifiable features. We believe that it is desirable for singers to perform\nunique songs that share certain characteristics specific to the singer.\nAccordingly, we hypothesized that lyricists account for the unique\ncharacteristics of the singers they write lyrics for. In other words,\nlyric-lyricist classification performance or the ease of capturing the features\nof a lyricist from the lyrics may depend on the variety of singers. In this\nstudy, we observed a relationship between lyricist-singer entropy or the\nvariety of singers associated with a single lyricist and lyric-lyricist\nclassification performance. As an example, the lyricist-singer entropy is\nminimal when the lyricist writes lyrics for only one singer. In our\nexperiments, we grouped lyricists among five groups in terms of lyricist-singer\nentropy and assessed the lyric-lyricist classification performance within each\ngroup. Consequently, the best F1 score was obtained for the group with the\nlowest lyricist-singer entropy. Our results suggest that further analyses of\nthe features contributing to lyric-lyricist classification performance on the\nlowest lyricist-singer entropy group may improve the feature extraction task\nfor lyricists.\n","authors":["Mitsuki Morita","Masato Kikuchi","Tadachika Ozono"],"pdf_url":"https://arxiv.org/pdf/2310.11035v1.pdf","comment":"The 10th International Conference on Advanced Informatics: Concepts,\n  Theory and Applications (ICAICTA 2023)"},{"id":"http://arxiv.org/abs/2310.11010v1","updated":"2023-10-17T05:44:10Z","published":"2023-10-17T05:44:10Z","title":"Iterative Shallow Fusion of Backward Language Model for End-to-End\n  Speech Recognition","summary":"  We propose a new shallow fusion (SF) method to exploit an external backward\nlanguage model (BLM) for end-to-end automatic speech recognition (ASR). The BLM\nhas complementary characteristics with a forward language model (FLM), and the\neffectiveness of their combination has been confirmed by rescoring ASR\nhypotheses as post-processing. In the proposed SF, we iteratively apply the BLM\nto partial ASR hypotheses in the backward direction (i.e., from the possible\nnext token to the start symbol) during decoding, substituting the newly\ncalculated BLM scores for the scores calculated at the last iteration. To\nenhance the effectiveness of this iterative SF (ISF), we train a partial\nsentence-aware BLM (PBLM) using reversed text data including partial sentences,\nconsidering the framework of ISF. In experiments using an attention-based\nencoder-decoder ASR system, we confirmed that ISF using the PBLM shows\ncomparable performance with SF using the FLM. By performing ISF, early pruning\nof prospective hypotheses can be prevented during decoding, and we can obtain a\nperformance improvement compared to applying the PBLM as post-processing.\nFinally, we confirmed that, by combining SF and ISF, further performance\nimprovement can be obtained thanks to the complementarity of the FLM and PBLM.\n","authors":["Atsunori Ogawa","Takafumi Moriya","Naoyuki Kamo","Naohiro Tawara","Marc Delcroix"],"pdf_url":"https://arxiv.org/pdf/2310.11010v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2310.11004v1","updated":"2023-10-17T05:13:46Z","published":"2023-10-17T05:13:46Z","title":"Advanced accent/dialect identification and accentedness assessment with\n  multi-embedding models and automatic speech recognition","summary":"  Accurately classifying accents and assessing accentedness in non-native\nspeakers are both challenging tasks due to the complexity and diversity of\naccent and dialect variations. In this study, embeddings from advanced\npre-trained language identification (LID) and speaker identification (SID)\nmodels are leveraged to improve the accuracy of accent classification and\nnon-native accentedness assessment. Findings demonstrate that employing\npre-trained LID and SID models effectively encodes accent/dialect information\nin speech. Furthermore, the LID and SID encoded accent information complement\nan end-to-end accent identification (AID) model trained from scratch. By\nincorporating all three embeddings, the proposed multi-embedding AID system\nachieves superior accuracy in accent identification. Next, we investigate\nleveraging automatic speech recognition (ASR) and accent identification models\nto explore accentedness estimation. The ASR model is an end-to-end\nconnectionist temporal classification (CTC) model trained exclusively with\nen-US utterances. The ASR error rate and en-US output of the AID model are\nleveraged as objective accentedness scores. Evaluation results demonstrate a\nstrong correlation between the scores estimated by the two models.\nAdditionally, a robust correlation between the objective accentedness scores\nand subjective scores based on human perception is demonstrated, providing\nevidence for the reliability and validity of utilizing AID-based and ASR-based\nsystems for accentedness assessment in non-native speech.\n","authors":["Shahram Ghorbani","John H. L. Hansen"],"pdf_url":"https://arxiv.org/pdf/2310.11004v1.pdf","comment":"Submitted to The Journal of the Acoustical Society of America"},{"id":"http://arxiv.org/abs/2310.11003v1","updated":"2023-10-17T05:10:39Z","published":"2023-10-17T05:10:39Z","title":"Correction Focused Language Model Training for Speech Recognition","summary":"  Language models (LMs) have been commonly adopted to boost the performance of\nautomatic speech recognition (ASR) particularly in domain adaptation tasks.\nConventional way of LM training treats all the words in corpora equally,\nresulting in suboptimal improvements in ASR performance. In this work, we\nintroduce a novel correction focused LM training approach which aims to\nprioritize ASR fallible words. The word-level ASR fallibility score,\nrepresenting the likelihood of ASR mis-recognition, is defined and shaped as a\nprior word distribution to guide the LM training. To enable correction focused\ntraining with text-only corpora, large language models (LLMs) are employed as\nfallibility score predictors and text generators through multi-task\nfine-tuning. Experimental results for domain adaptation tasks demonstrate the\neffectiveness of our proposed method. Compared with conventional LMs,\ncorrection focused training achieves up to relatively 5.5% word error rate\n(WER) reduction in sufficient text scenarios. In insufficient text scenarios,\nLM training with LLM-generated text achieves up to relatively 13% WER\nreduction, while correction focused training further obtains up to relatively\n6% WER reduction.\n","authors":["Yingyi Ma","Zhe Liu","Ozlem Kalinli"],"pdf_url":"https://arxiv.org/pdf/2310.11003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09623v2","updated":"2023-10-17T05:08:47Z","published":"2023-09-18T09:52:54Z","title":"HumTrans: A Novel Open-Source Dataset for Humming Melody Transcription\n  and Beyond","summary":"  This paper introduces the HumTrans dataset, which is publicly available and\nprimarily designed for humming melody transcription. The dataset can also serve\nas a foundation for downstream tasks such as humming melody based music\ngeneration. It consists of 500 musical compositions of different genres and\nlanguages, with each composition divided into multiple segments. In total, the\ndataset comprises 1000 music segments. To collect this humming dataset, we\nemployed 10 college students, all of whom are either music majors or proficient\nin playing at least one musical instrument. Each of them hummed every segment\ntwice using the web recording interface provided by our designed website. The\nhumming recordings were sampled at a frequency of 44,100 Hz. During the humming\nsession, the main interface provides a musical score for students to reference,\nwith the melody audio playing simultaneously to aid in capturing both melody\nand rhythm. The dataset encompasses approximately 56.22 hours of audio, making\nit the largest known humming dataset to date. The dataset will be released on\nHugging Face, and we will provide a GitHub repository containing baseline\nresults and evaluation codes.\n","authors":["Shansong Liu","Xu Li","Dian Li","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2309.09623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10992v1","updated":"2023-10-17T04:30:37Z","published":"2023-10-17T04:30:37Z","title":"A High Fidelity and Low Complexity Neural Audio Coding","summary":"  Audio coding is an essential module in the real-time communication system.\nNeural audio codecs can compress audio samples with a low bitrate due to the\nstrong modeling and generative capabilities of deep neural networks. To address\nthe poor high-frequency expression and high computational cost and storage\nconsumption, we proposed an integrated framework that utilizes a neural network\nto model wide-band components and adopts traditional signal processing to\ncompress high-band components according to psychological hearing knowledge.\nInspired by auditory perception theory, a perception-based loss function is\ndesigned to improve harmonic modeling. Besides, generative adversarial network\n(GAN) compression is proposed for the first time for neural audio codecs. Our\nmethod is superior to prior advanced neural codecs across subjective and\nobjective metrics and allows real-time inference on desktop and mobile.\n","authors":["Wenzhe Liu","Wei Xiao","Meng Wang","Shan Yang","Yupeng Shi","Yuyong Kang","Dan Su","Shidong Shang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2310.10992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10922v1","updated":"2023-10-17T01:31:59Z","published":"2023-10-17T01:31:59Z","title":"Spatial HuBERT: Self-supervised Spatial Speech Representation Learning\n  for a Single Talker from Multi-channel Audio","summary":"  Self-supervised learning has been used to leverage unlabelled data, improving\naccuracy and generalisation of speech systems through the training of\nrepresentation models. While many recent works have sought to produce effective\nrepresentations across a variety of acoustic domains, languages, modalities and\neven simultaneous speakers, these studies have all been limited to\nsingle-channel audio recordings. This paper presents Spatial HuBERT, a\nself-supervised speech representation model that learns both acoustic and\nspatial information pertaining to a single speaker in a potentially noisy\nenvironment by using multi-channel audio inputs. Spatial HuBERT learns\nrepresentations that outperform state-of-the-art single-channel speech\nrepresentations on a variety of spatial downstream tasks, particularly in\nreverberant and noisy environments. We also demonstrate the utility of the\nrepresentations learned by Spatial HuBERT on a speech localisation downstream\ntask. Along with this paper, we publicly release a new dataset of 100 000\nsimulated first-order ambisonics room impulse responses.\n","authors":["Antoni Dimitriadis","Siqi Pan","Vidhyasaharan Sethu","Beena Ahmed"],"pdf_url":"https://arxiv.org/pdf/2310.10922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11541v1","updated":"2023-10-17T19:27:23Z","published":"2023-10-17T19:27:23Z","title":"MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and\n  Phonetic Domains for Speech Representation Learning","summary":"  In this paper, we present a methodology for linguistic feature extraction,\nfocusing particularly on automatically syllabifying words in multiple\nlanguages, with a design to be compatible with a forced-alignment tool, the\nMontreal Forced Aligner (MFA). In both the textual and phonetic domains, our\nmethod focuses on the extraction of phonetic transcriptions from text, stress\nmarks, and a unified automatic syllabification (in text and phonetic domains).\nThe system was built with open-source components and resources. Through an\nablation study, we demonstrate the efficacy of our approach in automatically\nsyllabifying words from several languages (English, French and Spanish).\nAdditionally, we apply the technique to the transcriptions of the CMU ARCTIC\ndataset, generating valuable annotations available\nonline\\footnote{\\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for\nspeech representation learning, speech unit discovery, and disentanglement of\nspeech factors in several speech-related fields.\n","authors":["No√© Tits"],"pdf_url":"https://arxiv.org/pdf/2310.11541v1.pdf","comment":"Accepted for publication at EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.11532v1","updated":"2023-10-17T19:02:40Z","published":"2023-10-17T19:02:40Z","title":"Multi-stage Large Language Model Correction for Speech Recognition","summary":"  In this paper, we investigate the usage of large language models (LLMs) to\nimprove the performance of competitive speech recognition systems. Different\nfrom traditional language models that focus on one single data domain, the rise\nof LLMs brings us the opportunity to push the limit of state-of-the-art ASR\nperformance, and at the same time to achieve higher robustness and generalize\neffectively across multiple domains. Motivated by this, we propose a novel\nmulti-stage approach to combine traditional language model re-scoring and LLM\nprompting. Specifically, the proposed method has two stages: the first stage\nuses a language model to re-score an N-best list of ASR hypotheses and run a\nconfidence check; The second stage uses prompts to a LLM to perform ASR error\ncorrection on less confident results from the first stage. Our experimental\nresults demonstrate the effectiveness of the proposed method by showing a 10% ~\n20% relative improvement in WER over a competitive ASR system -- across\nmultiple test domains.\n","authors":["Jie Pu","Thai-Son Nguyen","Sebastian St√ºker"],"pdf_url":"https://arxiv.org/pdf/2310.11532v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.11486v1","updated":"2023-10-17T16:12:18Z","published":"2023-10-17T16:12:18Z","title":"End-to-End real time tracking of children's reading with pointer network","summary":"  In this work, we explore how a real time reading tracker can be built\nefficiently for children's voices. While previously proposed reading trackers\nfocused on ASR-based cascaded approaches, we propose a fully end-to-end model\nmaking it less prone to lags in voice tracking. We employ a pointer network\nthat directly learns to predict positions in the ground truth text conditioned\non the streaming speech. To train this pointer network, we generate ground\ntruth training signals by using forced alignment between the read speech and\nthe text being read on the training set. Exploring different forced alignment\nmodels, we find a neural attention based model is at least as close in\nalignment accuracy to the Montreal Forced Aligner, but surprisingly is a better\ntraining signal for the pointer network. Our results are reported on one adult\nspeech data (TIMIT) and two children's speech datasets (CMU Kids and Reading\nRaces). Our best model can accurately track adult speech with 87.8% accuracy\nand the much harder and disfluent children's speech with 77.1% accuracy on CMU\nKids data and a 65.3% accuracy on the Reading Races dataset.\n","authors":["Vishal Sunder","Beulah Karrolla","Eric Fosler-Lussier"],"pdf_url":"https://arxiv.org/pdf/2310.11486v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.13015v1","updated":"2023-10-17T21:21:40Z","published":"2023-10-17T21:21:40Z","title":"Audio-AdapterFusion: A Task-ID-free Approach for Efficient and\n  Non-Destructive Multi-task Speech Recognition","summary":"  Adapters are an efficient, composable alternative to full fine-tuning of\npre-trained models and help scale the deployment of large ASR models to many\ntasks. In practice, a task ID is commonly prepended to the input during\ninference to route to single-task adapters for the specified task. However, one\nmajor limitation of this approach is that the task ID may not be known during\ninference, rendering it unsuitable for most multi-task settings. To address\nthis, we propose three novel task-ID-free methods to combine single-task\nadapters in multi-task ASR and investigate two learning algorithms for\ntraining. We evaluate our methods on 10 test sets from 4 diverse ASR tasks and\nshow that our methods are non-destructive and parameter-efficient. While only\nupdating 17% of the model parameters, our methods can achieve an 8% mean WER\nimprovement relative to full fine-tuning and are on-par with task-ID adapter\nrouting.\n","authors":["Hillary Ngai","Rohan Agrawal","Neeraj Gaur","Ronny Huang","Parisa Haghani","Pedro Moreno Mengibar"],"pdf_url":"https://arxiv.org/pdf/2310.13015v1.pdf","comment":"2023 IEEE Automatic Speech Recognition and Understanding Workshop\n  (ASRU) Proceedings"},{"id":"http://arxiv.org/abs/2310.13013v1","updated":"2023-10-17T14:49:48Z","published":"2023-10-17T14:49:48Z","title":"Generative error correction for code-switching speech recognition using\n  large language models","summary":"  Code-switching (CS) speech refers to the phenomenon of mixing two or more\nlanguages within the same sentence. Despite the recent advances in automatic\nspeech recognition (ASR), CS-ASR is still a challenging task ought to the\ngrammatical structure complexity of the phenomenon and the data scarcity of\nspecific training corpus. In this work, we propose to leverage large language\nmodels (LLMs) and lists of hypotheses generated by an ASR to address the CS\nproblem. Specifically, we first employ multiple well-trained ASR models for\nN-best hypotheses generation, with the aim of increasing the diverse and\ninformative elements in the set of hypotheses. Next, we utilize the LLMs to\nlearn the hypotheses-to-transcription (H2T) mapping by adding a trainable\nlow-rank adapter. Such a generative error correction (GER) method directly\npredicts the accurate transcription according to its expert linguistic\nknowledge and N-best hypotheses, resulting in a paradigm shift from the\ntraditional language model rescoring or error correction techniques.\nExperimental evidence demonstrates that GER significantly enhances CS-ASR\naccuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show\nremarkable data efficiency for H2T learning, providing a potential solution to\nthe data scarcity problem of CS-ASR in low-resource languages.\n","authors":["Chen Chen","Yuchen Hu","Chao-Han Huck Yang","Hexin Liu","Sabato Marco Siniscalchi","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2310.13013v1.pdf","comment":"Submitted to ICASSP2024"}]},"2023-10-14T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2310.09653v1","updated":"2023-10-14T19:51:17Z","published":"2023-10-14T19:51:17Z","title":"SelfVC: Voice Conversion With Iterative Refinement using Self\n  Transformations","summary":"  We propose SelfVC, a training strategy to iteratively improve a voice\nconversion model with self-synthesized examples. Previous efforts on voice\nconversion focus on explicitly disentangling speech representations to\nseparately encode speaker characteristics and linguistic content. However,\ndisentangling speech representations to capture such attributes using\ntask-specific loss terms can lead to information loss by discarding finer\nnuances of the original signal. In this work, instead of explicitly\ndisentangling attributes with loss terms, we present a framework to train a\ncontrollable voice conversion model on entangled speech representations derived\nfrom self-supervised learning and speaker verification models. First, we\ndevelop techniques to derive prosodic information from the audio signal and SSL\nrepresentations to train predictive submodules in the synthesis model. Next, we\npropose a training strategy to iteratively improve the synthesis model for\nvoice conversion, by creating a challenging training objective using\nself-synthesized examples. In this training approach, the current state of the\nsynthesis model is used to generate voice-converted variations of an utterance,\nwhich serve as inputs for the reconstruction task, ensuring a continuous and\npurposeful refinement of the model. We demonstrate that incorporating such\nself-synthesized examples during training improves the speaker similarity of\ngenerated speech as compared to a baseline voice conversion model trained\nsolely on heuristically perturbed inputs. SelfVC is trained without any text\nand is applicable to a range of tasks such as zero-shot voice conversion,\ncross-lingual voice conversion, and controllable speech synthesis with pitch\nand pace modifications. SelfVC achieves state-of-the-art results in zero-shot\nvoice conversion on metrics evaluating naturalness, speaker similarity, and\nintelligibility of synthesized audio.\n","authors":["Paarth Neekhara","Shehzeen Hussain","Rafael Valle","Boris Ginsburg","Rishabh Ranjan","Shlomo Dubnov","Farinaz Koushanfar","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2310.09653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13941v2","updated":"2023-10-14T14:24:23Z","published":"2023-08-26T18:58:10Z","title":"A small vocabulary database of ultrasound image sequences of vocal tract\n  dynamics","summary":"  This paper presents a new database consisting of concurrent articulatory and\nacoustic speech data. The articulatory data correspond to ultrasound videos of\nthe vocal tract dynamics, which allow the visualization of the tongue upper\ncontour during the speech production process. Acoustic data is composed of 30\nshort sentences that were acquired by a directional cardioid microphone. This\ndatabase includes data from 17 young subjects (8 male and 9 female) from the\nSantander region in Colombia, who reported not having any speech pathology.\n","authors":["Margareth Castillo","Felipe Rubio","Dagoberto Porras","Sonia H. Contreras-Ortiz","Alexander Sep√∫lveda"],"pdf_url":"https://arxiv.org/pdf/2308.13941v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14838v2","updated":"2023-10-14T08:47:43Z","published":"2023-05-24T07:42:15Z","title":"ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text\n  Translation","summary":"  Joint speech-language training is challenging due to the large demand for\ntraining data and GPU consumption, as well as the modality gap between speech\nand language. We present ComSL, a speech-language model built atop a composite\narchitecture of public pretrained speech-only and language-only models and\noptimized data-efficiently for spoken language tasks. Particularly, we propose\nto incorporate cross-modality learning into transfer learning and conduct them\nsimultaneously for downstream tasks in a multi-task learning manner. Our\napproach has demonstrated effectiveness in end-to-end speech-to-text\ntranslation tasks, achieving a new state-of-the-art average BLEU score of 31.5\non the multilingual speech to English text translation task for 21 languages,\nas measured on the public CoVoST2 evaluation set.\n","authors":["Chenyang Le","Yao Qian","Long Zhou","Shujie Liu","Yanmin Qian","Michael Zeng","Xuedong Huang"],"pdf_url":"https://arxiv.org/pdf/2305.14838v2.pdf","comment":"NeurIPS 2023, Poster"},{"id":"http://arxiv.org/abs/2310.09522v1","updated":"2023-10-14T07:25:52Z","published":"2023-10-14T07:25:52Z","title":"Dynamic Prediction of Full-Ocean Depth SSP by Hierarchical LSTM: An\n  Experimental Result","summary":"  SSP distribution is an important parameter for underwater positioning,\nnavigation and timing (PNT) because it affects the propagation mode of\nunderwater acoustic signals. To accurate predict future sound speed\ndistribution, we propose a hierarchical long short--term memory (H--LSTM)\nneural network for future sound speed prediction, which explore the\ndistribution pattern of sound velocity in the time dimension. To verify the\nfeasibility and effectiveness, we conducted both simulations and real\nexperiments. The ocean experiment was held in the South China Sea in April,\n2023. Results show that the accuracy of the proposed method outperforms the\nstate--of--the--art methods.\n","authors":["Jiajun Lu","Wei Huang","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.09522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13282v2","updated":"2023-10-14T06:27:13Z","published":"2022-11-23T19:51:16Z","title":"Voice-preserving Zero-shot Multiple Accent Conversion","summary":"  Most people who have tried to learn a foreign language would have experienced\ndifficulties understanding or speaking with a native speaker's accent. For\nnative speakers, understanding or speaking a new accent is likewise a difficult\ntask. An accent conversion system that changes a speaker's accent but preserves\nthat speaker's voice identity, such as timbre and pitch, has the potential for\na range of applications, such as communication, language learning, and\nentertainment. Existing accent conversion models tend to change the speaker\nidentity and accent at the same time. Here, we use adversarial learning to\ndisentangle accent dependent features while retaining other acoustic\ncharacteristics. What sets our work apart from existing accent conversion\nmodels is the capability to convert an unseen speaker's utterance to multiple\naccents while preserving its original voice identity. Subjective evaluations\nshow that our model generates audio that sound closer to the target accent and\nlike the original speaker.\n","authors":["Mumin Jin","Prashant Serai","Jilong Wu","Andros Tjandra","Vimal Manohar","Qing He"],"pdf_url":"https://arxiv.org/pdf/2211.13282v2.pdf","comment":"Accepted to IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2310.09505v1","updated":"2023-10-14T06:22:08Z","published":"2023-10-14T06:22:08Z","title":"Advancing Test-Time Adaptation for Acoustic Foundation Models in\n  Open-World Shifts","summary":"  Test-Time Adaptation (TTA) is a critical paradigm for tackling distribution\nshifts during inference, especially in visual recognition tasks. However, while\nacoustic models face similar challenges due to distribution shifts in test-time\nspeech, TTA techniques specifically designed for acoustic modeling in the\ncontext of open-world data shifts remain scarce. This gap is further\nexacerbated when considering the unique characteristics of acoustic foundation\nmodels: 1) they are primarily built on transformer architectures with layer\nnormalization and 2) they deal with test-time speech data of varying lengths in\na non-stationary manner. These aspects make the direct application of\nvision-focused TTA methods, which are mostly reliant on batch normalization and\nassume independent samples, infeasible. In this paper, we delve into TTA for\npre-trained acoustic models facing open-world data shifts. We find that noisy,\nhigh-entropy speech frames, often non-silent, carry key semantic content.\nTraditional TTA methods might inadvertently filter out this information using\npotentially flawed heuristics. In response, we introduce a heuristic-free,\nlearning-based adaptation enriched by confidence enhancement. Noting that\nspeech signals' short-term consistency, we also apply consistency\nregularization during test-time optimization. Our experiments on synthetic and\nreal-world datasets affirm our method's superiority over existing baselines.\n","authors":["Hongfu Liu","Hengguan Huang","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.09505v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.09653v1","updated":"2023-10-14T19:51:17Z","published":"2023-10-14T19:51:17Z","title":"SelfVC: Voice Conversion With Iterative Refinement using Self\n  Transformations","summary":"  We propose SelfVC, a training strategy to iteratively improve a voice\nconversion model with self-synthesized examples. Previous efforts on voice\nconversion focus on explicitly disentangling speech representations to\nseparately encode speaker characteristics and linguistic content. However,\ndisentangling speech representations to capture such attributes using\ntask-specific loss terms can lead to information loss by discarding finer\nnuances of the original signal. In this work, instead of explicitly\ndisentangling attributes with loss terms, we present a framework to train a\ncontrollable voice conversion model on entangled speech representations derived\nfrom self-supervised learning and speaker verification models. First, we\ndevelop techniques to derive prosodic information from the audio signal and SSL\nrepresentations to train predictive submodules in the synthesis model. Next, we\npropose a training strategy to iteratively improve the synthesis model for\nvoice conversion, by creating a challenging training objective using\nself-synthesized examples. In this training approach, the current state of the\nsynthesis model is used to generate voice-converted variations of an utterance,\nwhich serve as inputs for the reconstruction task, ensuring a continuous and\npurposeful refinement of the model. We demonstrate that incorporating such\nself-synthesized examples during training improves the speaker similarity of\ngenerated speech as compared to a baseline voice conversion model trained\nsolely on heuristically perturbed inputs. SelfVC is trained without any text\nand is applicable to a range of tasks such as zero-shot voice conversion,\ncross-lingual voice conversion, and controllable speech synthesis with pitch\nand pace modifications. SelfVC achieves state-of-the-art results in zero-shot\nvoice conversion on metrics evaluating naturalness, speaker similarity, and\nintelligibility of synthesized audio.\n","authors":["Paarth Neekhara","Shehzeen Hussain","Rafael Valle","Boris Ginsburg","Rishabh Ranjan","Shlomo Dubnov","Farinaz Koushanfar","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2310.09653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13941v2","updated":"2023-10-14T14:24:23Z","published":"2023-08-26T18:58:10Z","title":"A small vocabulary database of ultrasound image sequences of vocal tract\n  dynamics","summary":"  This paper presents a new database consisting of concurrent articulatory and\nacoustic speech data. The articulatory data correspond to ultrasound videos of\nthe vocal tract dynamics, which allow the visualization of the tongue upper\ncontour during the speech production process. Acoustic data is composed of 30\nshort sentences that were acquired by a directional cardioid microphone. This\ndatabase includes data from 17 young subjects (8 male and 9 female) from the\nSantander region in Colombia, who reported not having any speech pathology.\n","authors":["Margareth Castillo","Felipe Rubio","Dagoberto Porras","Sonia H. Contreras-Ortiz","Alexander Sep√∫lveda"],"pdf_url":"https://arxiv.org/pdf/2308.13941v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14838v2","updated":"2023-10-14T08:47:43Z","published":"2023-05-24T07:42:15Z","title":"ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text\n  Translation","summary":"  Joint speech-language training is challenging due to the large demand for\ntraining data and GPU consumption, as well as the modality gap between speech\nand language. We present ComSL, a speech-language model built atop a composite\narchitecture of public pretrained speech-only and language-only models and\noptimized data-efficiently for spoken language tasks. Particularly, we propose\nto incorporate cross-modality learning into transfer learning and conduct them\nsimultaneously for downstream tasks in a multi-task learning manner. Our\napproach has demonstrated effectiveness in end-to-end speech-to-text\ntranslation tasks, achieving a new state-of-the-art average BLEU score of 31.5\non the multilingual speech to English text translation task for 21 languages,\nas measured on the public CoVoST2 evaluation set.\n","authors":["Chenyang Le","Yao Qian","Long Zhou","Shujie Liu","Yanmin Qian","Michael Zeng","Xuedong Huang"],"pdf_url":"https://arxiv.org/pdf/2305.14838v2.pdf","comment":"NeurIPS 2023, Poster"},{"id":"http://arxiv.org/abs/2310.09522v1","updated":"2023-10-14T07:25:52Z","published":"2023-10-14T07:25:52Z","title":"Dynamic Prediction of Full-Ocean Depth SSP by Hierarchical LSTM: An\n  Experimental Result","summary":"  SSP distribution is an important parameter for underwater positioning,\nnavigation and timing (PNT) because it affects the propagation mode of\nunderwater acoustic signals. To accurate predict future sound speed\ndistribution, we propose a hierarchical long short--term memory (H--LSTM)\nneural network for future sound speed prediction, which explore the\ndistribution pattern of sound velocity in the time dimension. To verify the\nfeasibility and effectiveness, we conducted both simulations and real\nexperiments. The ocean experiment was held in the South China Sea in April,\n2023. Results show that the accuracy of the proposed method outperforms the\nstate--of--the--art methods.\n","authors":["Jiajun Lu","Wei Huang","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.09522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13282v2","updated":"2023-10-14T06:27:13Z","published":"2022-11-23T19:51:16Z","title":"Voice-preserving Zero-shot Multiple Accent Conversion","summary":"  Most people who have tried to learn a foreign language would have experienced\ndifficulties understanding or speaking with a native speaker's accent. For\nnative speakers, understanding or speaking a new accent is likewise a difficult\ntask. An accent conversion system that changes a speaker's accent but preserves\nthat speaker's voice identity, such as timbre and pitch, has the potential for\na range of applications, such as communication, language learning, and\nentertainment. Existing accent conversion models tend to change the speaker\nidentity and accent at the same time. Here, we use adversarial learning to\ndisentangle accent dependent features while retaining other acoustic\ncharacteristics. What sets our work apart from existing accent conversion\nmodels is the capability to convert an unseen speaker's utterance to multiple\naccents while preserving its original voice identity. Subjective evaluations\nshow that our model generates audio that sound closer to the target accent and\nlike the original speaker.\n","authors":["Mumin Jin","Prashant Serai","Jilong Wu","Andros Tjandra","Vimal Manohar","Qing He"],"pdf_url":"https://arxiv.org/pdf/2211.13282v2.pdf","comment":"Accepted to IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2310.09505v1","updated":"2023-10-14T06:22:08Z","published":"2023-10-14T06:22:08Z","title":"Advancing Test-Time Adaptation for Acoustic Foundation Models in\n  Open-World Shifts","summary":"  Test-Time Adaptation (TTA) is a critical paradigm for tackling distribution\nshifts during inference, especially in visual recognition tasks. However, while\nacoustic models face similar challenges due to distribution shifts in test-time\nspeech, TTA techniques specifically designed for acoustic modeling in the\ncontext of open-world data shifts remain scarce. This gap is further\nexacerbated when considering the unique characteristics of acoustic foundation\nmodels: 1) they are primarily built on transformer architectures with layer\nnormalization and 2) they deal with test-time speech data of varying lengths in\na non-stationary manner. These aspects make the direct application of\nvision-focused TTA methods, which are mostly reliant on batch normalization and\nassume independent samples, infeasible. In this paper, we delve into TTA for\npre-trained acoustic models facing open-world data shifts. We find that noisy,\nhigh-entropy speech frames, often non-silent, carry key semantic content.\nTraditional TTA methods might inadvertently filter out this information using\npotentially flawed heuristics. In response, we introduce a heuristic-free,\nlearning-based adaptation enriched by confidence enhancement. Noting that\nspeech signals' short-term consistency, we also apply consistency\nregularization during test-time optimization. Our experiments on synthetic and\nreal-world datasets affirm our method's superiority over existing baselines.\n","authors":["Hongfu Liu","Hengguan Huang","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.09505v1.pdf","comment":null}]},"2023-10-18T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2304.11029v4","updated":"2023-10-18T17:16:28Z","published":"2023-04-21T15:23:00Z","title":"CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic\n  Music Information Retrieval","summary":"  We introduce CLaMP: Contrastive Language-Music Pre-training, which learns\ncross-modal representations between natural language and symbolic music using a\nmusic encoder and a text encoder trained jointly with a contrastive loss. To\npre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs.\nIt employed text dropout as a data augmentation technique and bar patching to\nefficiently represent music data which reduces sequence length to less than\n10\\%. In addition, we developed a masked music model pre-training objective to\nenhance the music encoder's comprehension of musical context and structure.\nCLaMP integrates textual information to enable semantic search and zero-shot\nclassification for symbolic music, surpassing the capabilities of previous\nmodels. To support the evaluation of semantic search and music classification,\nwe publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in\nABC notation, each accompanied by a title, artist, genre, and description. In\ncomparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP\ndemonstrated comparable or superior performance on score-oriented datasets. Our\nmodels and code are available at\nhttps://github.com/microsoft/muzic/tree/main/clamp.\n","authors":["Shangda Wu","Dingyao Yu","Xu Tan","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2304.11029v4.pdf","comment":"11 pages, 5 figures, 5 tables, accepted by ISMIR 2023"},{"id":"http://arxiv.org/abs/2310.11967v1","updated":"2023-10-18T13:45:47Z","published":"2023-10-18T13:45:47Z","title":"Take the aTrain. Introducing an Interface for the Accessible\n  Transcription of Interviews","summary":"  aTrain is an open-source and offline tool for transcribing audio data in\nmultiple languages with CPU and NVIDIA GPU support. It is specifically designed\nfor researchers using qualitative data generated from various forms of speech\ninteractions with research participants. aTrain requires no programming skills,\nruns on most computers, does not require an internet connection, and was\nverified not to upload data to any server. aTrain combines OpenAI's Whisper\nmodel with speaker recognition to provide output that integrates with the\npopular qualitative data analysis software tools MAXQDA and ATLAS.ti. It has an\neasy-to-use graphical interface and is provided as a Windows-App through the\nMicrosoft Store allowing for simple installation by researchers. The source\ncode is freely available on GitHub. Having developed aTrain with a focus on\nspeed on local computers, we show that the transcription time on current mobile\nCPUs is around 2 to 3 times the duration of the audio file using the\nhighest-accuracy transcription models. If an entry-level graphics card is\navailable, the transcription speed increases to 20% of the audio duration.\n","authors":["Armin Haberl","J√ºrgen Flei√ü","Dominik Kowald","Stefan Thalmann"],"pdf_url":"https://arxiv.org/pdf/2310.11967v1.pdf","comment":"Install via Microsoft store:\n  apps.microsoft.com/store/detail/atrain/9N15Q44SZNS2. Github:\n  github.com/BANDAS-Center/aTrain"},{"id":"http://arxiv.org/abs/2310.11921v1","updated":"2023-10-18T12:26:43Z","published":"2023-10-18T12:26:43Z","title":"BUT CHiME-7 system description","summary":"  This paper describes the joint effort of Brno University of Technology (BUT),\nAGH University of Krakow and University of Buenos Aires on the development of\nAutomatic Speech Recognition systems for the CHiME-7 Challenge. We train and\nevaluate various end-to-end models with several toolkits. We heavily relied on\nGuided Source Separation (GSS) to convert multi-channel audio to single\nchannel. The ASR is leveraging speech representations from models pre-trained\nby self-supervised learning, and we do a fusion of several ASR systems. In\naddition, we modified external data from the LibriSpeech corpus to become a\nclose domain and added it to the training. Our efforts were focused on the\nfar-field acoustic robustness sub-track of Task 1 - Distant Automatic Speech\nRecognition (DASR), our systems use oracle segmentation.\n","authors":["Martin Karafi√°t","Karel Vesel√Ω","Igor Sz√∂ke","Ladislav Mo≈°ner","Karel Bene≈°","Marcin Witkowski","Germ√°n Barchi","Leonardo Pepino"],"pdf_url":"https://arxiv.org/pdf/2310.11921v1.pdf","comment":"6 pages, Chime-7 challenge 2023"},{"id":"http://arxiv.org/abs/2307.07280v2","updated":"2023-10-18T10:36:36Z","published":"2023-07-14T11:20:22Z","title":"Replay to Remember: Continual Layer-Specific Fine-tuning for German\n  Speech Recognition","summary":"  While Automatic Speech Recognition (ASR) models have shown significant\nadvances with the introduction of unsupervised or self-supervised training\ntechniques, these improvements are still only limited to a subsection of\nlanguages and speakers. Transfer learning enables the adaptation of large-scale\nmultilingual models to not only low-resource languages but also to more\nspecific speaker groups. However, fine-tuning on data from new domains is\nusually accompanied by a decrease in performance on the original domain.\nTherefore, in our experiments, we examine how well the performance of\nlarge-scale ASR models can be approximated for smaller domains, with our own\ndataset of German Senior Voice Commands (SVC-de), and how much of the general\nspeech recognition performance can be preserved by selectively freezing parts\nof the model during training. To further increase the robustness of the ASR\nmodel to vocabulary and speakers outside of the fine-tuned domain, we apply\nExperience Replay for continual learning. By adding only a fraction of data\nfrom the original domain, we are able to reach Word-Error-Rates (WERs) below\n5\\% on the new domain, while stabilizing performance for general speech\nrecognition at acceptable WERs.\n","authors":["Theresa Pekarek Rosin","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2307.07280v2.pdf","comment":"13 pages, 7 figures, accepted and presented at ICANN 2023"},{"id":"http://arxiv.org/abs/2310.11830v1","updated":"2023-10-18T09:31:56Z","published":"2023-10-18T09:31:56Z","title":"CLARA: Multilingual Contrastive Learning for Audio Representation\n  Acquisition","summary":"  This paper proposes a novel framework for multilingual speech and sound\nrepresentation learning using contrastive learning. The lack of sizeable\nlabelled datasets hinders speech-processing research across languages. Recent\nadvances in contrastive learning provide self-supervised techniques to learn\nfrom unlabelled data. Motivated by reducing data dependence and improving\ngeneralisation across diverse languages and conditions, we develop a\nmultilingual contrastive framework. This framework enables models to acquire\nshared representations across languages, facilitating cross-lingual transfer\nwith limited target language data.\n  Additionally, capturing emotional cues within speech is challenging due to\nsubjective perceptual assessments. By learning expressive representations from\ndiverse, multilingual data in a self-supervised manner, our approach aims to\ndevelop speech representations that encode emotive dimensions.\n  Our method trains encoders on a large corpus of multi-lingual audio data.\nData augmentation techniques are employed to expand the dataset. The\ncontrastive learning approach trains the model to maximise agreement between\npositive pairs and minimise agreement between negative pairs. Extensive\nexperiments demonstrate state-of-the-art performance of the proposed model on\nemotion recognition, audio classification, and retrieval benchmarks under\nzero-shot and few-shot conditions. This provides an effective approach for\nacquiring shared and generalised speech representations across languages and\nacoustic conditions while encoding latent emotional dimensions.\n","authors":["Kari A Noriy","Xiaosong Yang","Marcin Budka","Jian Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.11830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05374v2","updated":"2023-10-18T08:44:31Z","published":"2023-05-22T08:23:51Z","title":"Towards Ultrasound Tongue Image prediction from EEG during speech\n  production","summary":"  Previous initial research has already been carried out to propose\nspeech-based BCI using brain signals (e.g. non-invasive EEG and invasive sEEG /\nECoG), but there is a lack of combined methods that investigate non-invasive\nbrain, articulation, and speech signals together and analyze the cognitive\nprocesses in the brain, the kinematics of the articulatory movement and the\nresulting speech signal. In this paper, we describe our multimodal\n(electroencephalography, ultrasound tongue imaging, and speech) analysis and\nsynthesis experiments, as a feasibility study. We extend the analysis of brain\nsignals recorded during speech production with ultrasound-based articulation\ndata. From the brain signal measured with EEG, we predict ultrasound images of\nthe tongue with a fully connected deep neural network. The results show that\nthere is a weak but noticeable relationship between EEG and ultrasound tongue\nimages, i.e. the network can differentiate articulated speech and neutral\ntongue position.\n","authors":["Tam√°s G√°bor Csap√≥","Frigyes Viktor Arthur","P√©ter Nagy","√Åd√°m Boncz"],"pdf_url":"https://arxiv.org/pdf/2306.05374v2.pdf","comment":"accepted at Interspeech 2023"},{"id":"http://arxiv.org/abs/2310.11781v1","updated":"2023-10-18T08:20:54Z","published":"2023-10-18T08:20:54Z","title":"Blind estimation of audio effects using an auto-encoder approach and\n  differentiable signal processing","summary":"  Blind Estimation of Audio Effects (BE-AFX) aims at estimating the Audio\nEffects (AFXs) applied to an original, unprocessed audio sample solely based on\nthe processed audio sample. To train such a system traditional approaches\noptimize a loss between ground truth and estimated AFX parameters. This\ninvolves knowing the exact implementation of the AFXs used for the process. In\nthis work, we propose an alternative solution that eliminates the requirement\nfor knowing this implementation. Instead, we introduce an auto-encoder\napproach, which optimizes an audio quality metric. We explore, suggest, and\ncompare various implementations of commonly used mastering AFXs, using\ndifferential signal processing or neural approximations. Our findings\ndemonstrate that our auto-encoder approach yields superior estimates of the\naudio quality produced by a chain of AFXs, compared to the traditional\nparameter-based approach, even if the latter provides a more accurate parameter\nestimation.\n","authors":["C√¥me Peladeau","Geoffroy Peeters"],"pdf_url":"https://arxiv.org/pdf/2310.11781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11739v1","updated":"2023-10-18T06:45:49Z","published":"2023-10-18T06:45:49Z","title":"Unintended Memorization in Large ASR Models, and How to Mitigate It","summary":"  It is well-known that neural networks can unintentionally memorize their\ntraining examples, causing privacy concerns. However, auditing memorization in\nlarge non-auto-regressive automatic speech recognition (ASR) models has been\nchallenging due to the high compute cost of existing methods such as hardness\ncalibration. In this work, we design a simple auditing method to measure\nmemorization in large ASR models without the extra compute overhead.\nConcretely, we speed up randomly-generated utterances to create a mapping\nbetween vocal and text information that is difficult to learn from typical\ntraining examples. Hence, accurate predictions only for sped-up training\nexamples can serve as clear evidence for memorization, and the corresponding\naccuracy can be used to measure memorization. Using the proposed method, we\nshowcase memorization in the state-of-the-art ASR models. To mitigate\nmemorization, we tried gradient clipping during training to bound the influence\nof any individual example on the final model. We empirically show that clipping\neach example's gradient can mitigate memorization for sped-up training examples\nwith up to 16 repetitions in the training set. Furthermore, we show that in\nlarge-scale distributed training, clipping the average gradient on each compute\ncore maintains neutral model quality and compute cost while providing strong\nprivacy protection.\n","authors":["Lun Wang","Om Thakkar","Rajiv Mathews"],"pdf_url":"https://arxiv.org/pdf/2310.11739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11728v1","updated":"2023-10-18T05:56:53Z","published":"2023-10-18T05:56:53Z","title":"EchoScan: Scanning Complex Indoor Geometries via Acoustic Echoes","summary":"  Accurate estimation of indoor space geometries is vital for constructing\nprecise digital twins, whose broad industrial applications include navigation\nin unfamiliar environments and efficient evacuation planning, particularly in\nlow-light conditions. This study introduces EchoScan, a deep neural network\nmodel that utilizes acoustic echoes to perform room geometry inference.\nConventional sound-based techniques rely on estimating geometry-related room\nparameters such as wall position and room size, thereby limiting the diversity\nof inferable room geometries. Contrarily, EchoScan overcomes this limitation by\ndirectly inferring room floorplans and heights, thereby enabling it to handle\nrooms with arbitrary shapes, including curved walls. The key innovation of\nEchoScan is its ability to analyze the complex relationship between low- and\nhigh-order reflections in room impulse responses (RIRs) using a\nmulti-aggregation module. The analysis of high-order reflections also enables\nit to infer complex room shapes when echoes are unobservable from the position\nof an audio device. Herein, EchoScan was trained and evaluated using RIRs\nsynthesized from complex environments, including the Manhattan and Atlanta\nlayouts, employing a practical audio device configuration compatible with\ncommercial, off-the-shelf devices. Compared with vision-based methods, EchoScan\ndemonstrated outstanding geometry estimation performance in rooms with various\nshapes.\n","authors":["Inmo Yeon","Iljoo Jeong","Seungchul Lee","Jung-Woo Choi"],"pdf_url":"https://arxiv.org/pdf/2310.11728v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.11713v1","updated":"2023-10-18T05:03:57Z","published":"2023-10-18T05:03:57Z","title":"Separating Invisible Sounds Toward Universal Audiovisual Scene-Aware\n  Sound Separation","summary":"  The audio-visual sound separation field assumes visible sources in videos,\nbut this excludes invisible sounds beyond the camera's view. Current methods\nstruggle with such sounds lacking visible cues. This paper introduces a novel\n\"Audio-Visual Scene-Aware Separation\" (AVSA-Sep) framework. It includes a\nsemantic parser for visible and invisible sounds and a separator for\nscene-informed separation. AVSA-Sep successfully separates both sound types,\nwith joint training and cross-modal alignment enhancing effectiveness.\n","authors":["Yiyang Su","Ali Vosoughi","Shijian Deng","Yapeng Tian","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.11713v1.pdf","comment":"Accepted at ICCV 2023 - AV4D, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.11708v1","updated":"2023-10-18T04:53:01Z","published":"2023-10-18T04:53:01Z","title":"Experimental Results of Underwater Sound Speed Profile Inversion by\n  Few-shot Multi-task Learning","summary":"  Underwater Sound Speed Profile (SSP) distribution has great influence on the\npropagation mode of acoustic signal, thus the fast and accurate estimation of\nSSP is of great importance in building underwater observation systems. The\nstate-of-the-art SSP inversion methods include frameworks of matched field\nprocessing (MFP), compressive sensing (CS), and feedforeward neural networks\n(FNN), among which the FNN shows better real-time performance while maintain\nthe same level of accuracy. However, the training of FNN needs quite a lot\nhistorical SSP samples, which is diffcult to be satisfied in many ocean areas.\nThis situation is called few-shot learning. To tackle this issue, we propose a\nmulti-task learning (MTL) model with partial parameter sharing among different\ntraning tasks. By MTL, common features could be extracted, thus accelerating\nthe learning process on given tasks, and reducing the demand for reference\nsamples, so as to enhance the generalization ability in few-shot learning. To\nverify the feasibility and effectiveness of MTL, a deep-ocean experiment was\nheld in April 2023 at the South China Sea. Results shows that MTL outperforms\nthe state-of-the-art methods in terms of accuracy for SSP inversion, while\ninherits the real-time advantage of FNN during the inversion stage.\n","authors":["Wei Huang","Fan Gao","Junting Wang","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.11708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12378v1","updated":"2023-10-18T23:10:46Z","published":"2023-10-18T23:10:46Z","title":"The CHiME-7 Challenge: System Description and Performance of NeMo Team's\n  DASR System","summary":"  We present the NVIDIA NeMo team's multi-channel speech recognition system for\nthe 7th CHiME Challenge Distant Automatic Speech Recognition (DASR) Task,\nfocusing on the development of a multi-channel, multi-speaker speech\nrecognition system tailored to transcribe speech from distributed microphones\nand microphone arrays. The system predominantly comprises of the following\nintegral modules: the Speaker Diarization Module, Multi-channel Audio Front-End\nProcessing Module, and the ASR Module. These components collectively establish\na cascading system, meticulously processing multi-channel and multi-speaker\naudio input. Moreover, this paper highlights the comprehensive optimization\nprocess that significantly enhanced our system's performance. Our team's\nsubmission is largely based on NeMo toolkits and will be publicly available.\n","authors":["Tae Jin Park","He Huang","Ante Jukic","Kunal Dhawan","Krishna C. Puvvada","Nithin Koluguri","Nikolay Karpov","Aleksandr Laptev","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2310.12378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12371v1","updated":"2023-10-18T22:46:20Z","published":"2023-10-18T22:46:20Z","title":"Property-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling\n  Technique for Synthetic Data Generation","summary":"  We introduce a sophisticated multi-speaker speech data simulator,\nspecifically engineered to generate multi-speaker speech recordings. A notable\nfeature of this simulator is its capacity to modulate the distribution of\nsilence and overlap via the adjustment of statistical parameters. This\ncapability offers a tailored training environment for developing neural models\nsuited for speaker diarization and voice activity detection. The acquisition of\nsubstantial datasets for speaker diarization often presents a significant\nchallenge, particularly in multi-speaker scenarios. Furthermore, the precise\ntime stamp annotation of speech data is a critical factor for training both\nspeaker diarization and voice activity detection. Our proposed multi-speaker\nsimulator tackles these problems by generating large-scale audio mixtures that\nmaintain statistical properties closely aligned with the input parameters. We\ndemonstrate that the proposed multi-speaker simulator generates audio mixtures\nwith statistical properties that closely align with the input parameters\nderived from real-world statistics. Additionally, we present the effectiveness\nof speaker diarization and voice activity detection models, which have been\ntrained exclusively on the generated simulated datasets.\n","authors":["Tae Jin Park","He Huang","Coleman Hooper","Nithin Koluguri","Kunal Dhawan","Ante Jukic","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2310.12371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09730v2","updated":"2023-10-18T19:23:27Z","published":"2022-12-19T18:53:04Z","title":"Speaking Style Conversion in the Waveform Domain Using Discrete\n  Self-Supervised Units","summary":"  We introduce DISSC, a novel, lightweight method that converts the rhythm,\npitch contour and timbre of a recording to a target speaker in a textless\nmanner. Unlike DISSC, most voice conversion (VC) methods focus primarily on\ntimbre, and ignore people's unique speaking style (prosody). The proposed\napproach uses a pretrained, self-supervised model for encoding speech to\ndiscrete units, which makes it simple, effective, and fast to train. All\nconversion modules are only trained on reconstruction like tasks, thus suitable\nfor any-to-many VC with no paired data. We introduce a suite of quantitative\nand qualitative evaluation metrics for this setup, and empirically demonstrate\nthat DISSC significantly outperforms the evaluated baselines. Code and samples\nare available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.\n","authors":["Gallil Maimon","Yossi Adi"],"pdf_url":"https://arxiv.org/pdf/2212.09730v2.pdf","comment":"Accepted at EMNLP 2023"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2304.11029v4","updated":"2023-10-18T17:16:28Z","published":"2023-04-21T15:23:00Z","title":"CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic\n  Music Information Retrieval","summary":"  We introduce CLaMP: Contrastive Language-Music Pre-training, which learns\ncross-modal representations between natural language and symbolic music using a\nmusic encoder and a text encoder trained jointly with a contrastive loss. To\npre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs.\nIt employed text dropout as a data augmentation technique and bar patching to\nefficiently represent music data which reduces sequence length to less than\n10\\%. In addition, we developed a masked music model pre-training objective to\nenhance the music encoder's comprehension of musical context and structure.\nCLaMP integrates textual information to enable semantic search and zero-shot\nclassification for symbolic music, surpassing the capabilities of previous\nmodels. To support the evaluation of semantic search and music classification,\nwe publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in\nABC notation, each accompanied by a title, artist, genre, and description. In\ncomparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP\ndemonstrated comparable or superior performance on score-oriented datasets. Our\nmodels and code are available at\nhttps://github.com/microsoft/muzic/tree/main/clamp.\n","authors":["Shangda Wu","Dingyao Yu","Xu Tan","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2304.11029v4.pdf","comment":"11 pages, 5 figures, 5 tables, accepted by ISMIR 2023"},{"id":"http://arxiv.org/abs/2310.12111v1","updated":"2023-10-18T17:07:05Z","published":"2023-10-18T17:07:05Z","title":"DASA: Difficulty-Aware Semantic Augmentation for Speaker Verification","summary":"  Data augmentation is vital to the generalization ability and robustness of\ndeep neural networks (DNNs) models. Existing augmentation methods for speaker\nverification manipulate the raw signal, which are time-consuming and the\naugmented samples lack diversity. In this paper, we present a novel\ndifficulty-aware semantic augmentation (DASA) approach for speaker\nverification, which can generate diversified training samples in speaker\nembedding space with negligible extra computing cost. Firstly, we augment\ntraining samples by perturbing speaker embeddings along semantic directions,\nwhich are obtained from speaker-wise covariance matrices. Secondly, accurate\ncovariance matrices are estimated from robust speaker embeddings during\ntraining, so we introduce difficultyaware additive margin softmax\n(DAAM-Softmax) to obtain optimal speaker embeddings. Finally, we assume the\nnumber of augmented samples goes to infinity and derive a closed-form upper\nbound of the expected loss with DASA, which achieves compatibility and\nefficiency. Extensive experiments demonstrate the proposed approach can achieve\na remarkable performance improvement. The best result achieves a 14.6% relative\nreduction in EER metric on CN-Celeb evaluation set.\n","authors":["Yuanyuan Wang","Yang Zhang","Zhiyong Wu","Zhihan Yang","Tao Wei","Kun Zou","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2310.12111v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2310.12014v1","updated":"2023-10-18T14:44:38Z","published":"2023-10-18T14:44:38Z","title":"Enhancing Spoofing Speech Detection Using Rhythm Information","summary":"  Spoofing speech detection is a hot and in-demand research field. However,\ncurrent spoofing speech detection systems is lack of convincing evidence. In\nthis paper, to increase the reliability of detection systems, the flaws of\nrhythm information inherent in the TTS-generated speech are analyzed. TTS\nmodels take text as input and utilize acoustic models to predict rhythm\ninformation, which introduces artifacts in the rhythm information. By filtering\nout vocal tract response, the remaining glottal flow with rhythm information\nretains detection ability for TTS-generated speech. Based on these analyses, a\nrhythm perturbation module is proposed to enhance the copy-synthesis data\naugmentation method. Fake utterances generated by the proposed method force the\ndetecting model to pay attention to the artifacts in rhythm information and\neffectively improve the ability to detect TTS-generated speech of the\nanti-spoofing countermeasures.\n","authors":["Jingze Lu","Yuxiang Zhang","Wenchao Wang","Zengqiang Shang","Pengyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.12014v1.pdf","comment":"5 pages, 2 figures, submitted to icassp2024"},{"id":"http://arxiv.org/abs/2310.11967v1","updated":"2023-10-18T13:45:47Z","published":"2023-10-18T13:45:47Z","title":"Take the aTrain. Introducing an Interface for the Accessible\n  Transcription of Interviews","summary":"  aTrain is an open-source and offline tool for transcribing audio data in\nmultiple languages with CPU and NVIDIA GPU support. It is specifically designed\nfor researchers using qualitative data generated from various forms of speech\ninteractions with research participants. aTrain requires no programming skills,\nruns on most computers, does not require an internet connection, and was\nverified not to upload data to any server. aTrain combines OpenAI's Whisper\nmodel with speaker recognition to provide output that integrates with the\npopular qualitative data analysis software tools MAXQDA and ATLAS.ti. It has an\neasy-to-use graphical interface and is provided as a Windows-App through the\nMicrosoft Store allowing for simple installation by researchers. The source\ncode is freely available on GitHub. Having developed aTrain with a focus on\nspeed on local computers, we show that the transcription time on current mobile\nCPUs is around 2 to 3 times the duration of the audio file using the\nhighest-accuracy transcription models. If an entry-level graphics card is\navailable, the transcription speed increases to 20% of the audio duration.\n","authors":["Armin Haberl","J√ºrgen Flei√ü","Dominik Kowald","Stefan Thalmann"],"pdf_url":"https://arxiv.org/pdf/2310.11967v1.pdf","comment":"Install via Microsoft store:\n  apps.microsoft.com/store/detail/atrain/9N15Q44SZNS2. Github:\n  github.com/BANDAS-Center/aTrain"},{"id":"http://arxiv.org/abs/2310.11954v1","updated":"2023-10-18T13:31:10Z","published":"2023-10-18T13:31:10Z","title":"MusicAgent: An AI Agent for Music Understanding and Generation with\n  Large Language Models","summary":"  AI-empowered music processing is a diverse field that encompasses dozens of\ntasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension\ntasks (e.g., music classification). For developers and amateurs, it is very\ndifficult to grasp all of these task to satisfy their requirements in music\nprocessing, especially considering the huge differences in the representations\nof music data and the model applicability across platforms among various tasks.\nConsequently, it is necessary to build a system to organize and integrate these\ntasks, and thus help practitioners to automatically analyze their demand and\ncall suitable tools as solutions to fulfill their requirements. Inspired by the\nrecent success of large language models (LLMs) in task automation, we develop a\nsystem, named MusicAgent, which integrates numerous music-related tools and an\nautonomous workflow to address user requirements. More specifically, we build\n1) toolset that collects tools from diverse sources, including Hugging Face,\nGitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g.,\nChatGPT) to organize these tools and automatically decompose user requests into\nmultiple sub-tasks and invoke corresponding music tools. The primary goal of\nthis system is to free users from the intricacies of AI-music tools, enabling\nthem to concentrate on the creative aspect. By granting users the freedom to\neffortlessly combine tools, the system offers a seamless and enriching music\nexperience.\n","authors":["Dingyao Yu","Kaitao Song","Peiling Lu","Tianyu He","Xu Tan","Wei Ye","Shikun Zhang","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.11954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11921v1","updated":"2023-10-18T12:26:43Z","published":"2023-10-18T12:26:43Z","title":"BUT CHiME-7 system description","summary":"  This paper describes the joint effort of Brno University of Technology (BUT),\nAGH University of Krakow and University of Buenos Aires on the development of\nAutomatic Speech Recognition systems for the CHiME-7 Challenge. We train and\nevaluate various end-to-end models with several toolkits. We heavily relied on\nGuided Source Separation (GSS) to convert multi-channel audio to single\nchannel. The ASR is leveraging speech representations from models pre-trained\nby self-supervised learning, and we do a fusion of several ASR systems. In\naddition, we modified external data from the LibriSpeech corpus to become a\nclose domain and added it to the training. Our efforts were focused on the\nfar-field acoustic robustness sub-track of Task 1 - Distant Automatic Speech\nRecognition (DASR), our systems use oracle segmentation.\n","authors":["Martin Karafi√°t","Karel Vesel√Ω","Igor Sz√∂ke","Ladislav Mo≈°ner","Karel Bene≈°","Marcin Witkowski","Germ√°n Barchi","Leonardo Pepino"],"pdf_url":"https://arxiv.org/pdf/2310.11921v1.pdf","comment":"6 pages, Chime-7 challenge 2023"},{"id":"http://arxiv.org/abs/2307.07280v2","updated":"2023-10-18T10:36:36Z","published":"2023-07-14T11:20:22Z","title":"Replay to Remember: Continual Layer-Specific Fine-tuning for German\n  Speech Recognition","summary":"  While Automatic Speech Recognition (ASR) models have shown significant\nadvances with the introduction of unsupervised or self-supervised training\ntechniques, these improvements are still only limited to a subsection of\nlanguages and speakers. Transfer learning enables the adaptation of large-scale\nmultilingual models to not only low-resource languages but also to more\nspecific speaker groups. However, fine-tuning on data from new domains is\nusually accompanied by a decrease in performance on the original domain.\nTherefore, in our experiments, we examine how well the performance of\nlarge-scale ASR models can be approximated for smaller domains, with our own\ndataset of German Senior Voice Commands (SVC-de), and how much of the general\nspeech recognition performance can be preserved by selectively freezing parts\nof the model during training. To further increase the robustness of the ASR\nmodel to vocabulary and speakers outside of the fine-tuned domain, we apply\nExperience Replay for continual learning. By adding only a fraction of data\nfrom the original domain, we are able to reach Word-Error-Rates (WERs) below\n5\\% on the new domain, while stabilizing performance for general speech\nrecognition at acceptable WERs.\n","authors":["Theresa Pekarek Rosin","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2307.07280v2.pdf","comment":"13 pages, 7 figures, accepted and presented at ICANN 2023"},{"id":"http://arxiv.org/abs/2310.11830v1","updated":"2023-10-18T09:31:56Z","published":"2023-10-18T09:31:56Z","title":"CLARA: Multilingual Contrastive Learning for Audio Representation\n  Acquisition","summary":"  This paper proposes a novel framework for multilingual speech and sound\nrepresentation learning using contrastive learning. The lack of sizeable\nlabelled datasets hinders speech-processing research across languages. Recent\nadvances in contrastive learning provide self-supervised techniques to learn\nfrom unlabelled data. Motivated by reducing data dependence and improving\ngeneralisation across diverse languages and conditions, we develop a\nmultilingual contrastive framework. This framework enables models to acquire\nshared representations across languages, facilitating cross-lingual transfer\nwith limited target language data.\n  Additionally, capturing emotional cues within speech is challenging due to\nsubjective perceptual assessments. By learning expressive representations from\ndiverse, multilingual data in a self-supervised manner, our approach aims to\ndevelop speech representations that encode emotive dimensions.\n  Our method trains encoders on a large corpus of multi-lingual audio data.\nData augmentation techniques are employed to expand the dataset. The\ncontrastive learning approach trains the model to maximise agreement between\npositive pairs and minimise agreement between negative pairs. Extensive\nexperiments demonstrate state-of-the-art performance of the proposed model on\nemotion recognition, audio classification, and retrieval benchmarks under\nzero-shot and few-shot conditions. This provides an effective approach for\nacquiring shared and generalised speech representations across languages and\nacoustic conditions while encoding latent emotional dimensions.\n","authors":["Kari A Noriy","Xiaosong Yang","Marcin Budka","Jian Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.11830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05374v2","updated":"2023-10-18T08:44:31Z","published":"2023-05-22T08:23:51Z","title":"Towards Ultrasound Tongue Image prediction from EEG during speech\n  production","summary":"  Previous initial research has already been carried out to propose\nspeech-based BCI using brain signals (e.g. non-invasive EEG and invasive sEEG /\nECoG), but there is a lack of combined methods that investigate non-invasive\nbrain, articulation, and speech signals together and analyze the cognitive\nprocesses in the brain, the kinematics of the articulatory movement and the\nresulting speech signal. In this paper, we describe our multimodal\n(electroencephalography, ultrasound tongue imaging, and speech) analysis and\nsynthesis experiments, as a feasibility study. We extend the analysis of brain\nsignals recorded during speech production with ultrasound-based articulation\ndata. From the brain signal measured with EEG, we predict ultrasound images of\nthe tongue with a fully connected deep neural network. The results show that\nthere is a weak but noticeable relationship between EEG and ultrasound tongue\nimages, i.e. the network can differentiate articulated speech and neutral\ntongue position.\n","authors":["Tam√°s G√°bor Csap√≥","Frigyes Viktor Arthur","P√©ter Nagy","√Åd√°m Boncz"],"pdf_url":"https://arxiv.org/pdf/2306.05374v2.pdf","comment":"accepted at Interspeech 2023"},{"id":"http://arxiv.org/abs/2310.11781v1","updated":"2023-10-18T08:20:54Z","published":"2023-10-18T08:20:54Z","title":"Blind estimation of audio effects using an auto-encoder approach and\n  differentiable signal processing","summary":"  Blind Estimation of Audio Effects (BE-AFX) aims at estimating the Audio\nEffects (AFXs) applied to an original, unprocessed audio sample solely based on\nthe processed audio sample. To train such a system traditional approaches\noptimize a loss between ground truth and estimated AFX parameters. This\ninvolves knowing the exact implementation of the AFXs used for the process. In\nthis work, we propose an alternative solution that eliminates the requirement\nfor knowing this implementation. Instead, we introduce an auto-encoder\napproach, which optimizes an audio quality metric. We explore, suggest, and\ncompare various implementations of commonly used mastering AFXs, using\ndifferential signal processing or neural approximations. Our findings\ndemonstrate that our auto-encoder approach yields superior estimates of the\naudio quality produced by a chain of AFXs, compared to the traditional\nparameter-based approach, even if the latter provides a more accurate parameter\nestimation.\n","authors":["C√¥me Peladeau","Geoffroy Peeters"],"pdf_url":"https://arxiv.org/pdf/2310.11781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11739v1","updated":"2023-10-18T06:45:49Z","published":"2023-10-18T06:45:49Z","title":"Unintended Memorization in Large ASR Models, and How to Mitigate It","summary":"  It is well-known that neural networks can unintentionally memorize their\ntraining examples, causing privacy concerns. However, auditing memorization in\nlarge non-auto-regressive automatic speech recognition (ASR) models has been\nchallenging due to the high compute cost of existing methods such as hardness\ncalibration. In this work, we design a simple auditing method to measure\nmemorization in large ASR models without the extra compute overhead.\nConcretely, we speed up randomly-generated utterances to create a mapping\nbetween vocal and text information that is difficult to learn from typical\ntraining examples. Hence, accurate predictions only for sped-up training\nexamples can serve as clear evidence for memorization, and the corresponding\naccuracy can be used to measure memorization. Using the proposed method, we\nshowcase memorization in the state-of-the-art ASR models. To mitigate\nmemorization, we tried gradient clipping during training to bound the influence\nof any individual example on the final model. We empirically show that clipping\neach example's gradient can mitigate memorization for sped-up training examples\nwith up to 16 repetitions in the training set. Furthermore, we show that in\nlarge-scale distributed training, clipping the average gradient on each compute\ncore maintains neutral model quality and compute cost while providing strong\nprivacy protection.\n","authors":["Lun Wang","Om Thakkar","Rajiv Mathews"],"pdf_url":"https://arxiv.org/pdf/2310.11739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11728v1","updated":"2023-10-18T05:56:53Z","published":"2023-10-18T05:56:53Z","title":"EchoScan: Scanning Complex Indoor Geometries via Acoustic Echoes","summary":"  Accurate estimation of indoor space geometries is vital for constructing\nprecise digital twins, whose broad industrial applications include navigation\nin unfamiliar environments and efficient evacuation planning, particularly in\nlow-light conditions. This study introduces EchoScan, a deep neural network\nmodel that utilizes acoustic echoes to perform room geometry inference.\nConventional sound-based techniques rely on estimating geometry-related room\nparameters such as wall position and room size, thereby limiting the diversity\nof inferable room geometries. Contrarily, EchoScan overcomes this limitation by\ndirectly inferring room floorplans and heights, thereby enabling it to handle\nrooms with arbitrary shapes, including curved walls. The key innovation of\nEchoScan is its ability to analyze the complex relationship between low- and\nhigh-order reflections in room impulse responses (RIRs) using a\nmulti-aggregation module. The analysis of high-order reflections also enables\nit to infer complex room shapes when echoes are unobservable from the position\nof an audio device. Herein, EchoScan was trained and evaluated using RIRs\nsynthesized from complex environments, including the Manhattan and Atlanta\nlayouts, employing a practical audio device configuration compatible with\ncommercial, off-the-shelf devices. Compared with vision-based methods, EchoScan\ndemonstrated outstanding geometry estimation performance in rooms with various\nshapes.\n","authors":["Inmo Yeon","Iljoo Jeong","Seungchul Lee","Jung-Woo Choi"],"pdf_url":"https://arxiv.org/pdf/2310.11728v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.11713v1","updated":"2023-10-18T05:03:57Z","published":"2023-10-18T05:03:57Z","title":"Separating Invisible Sounds Toward Universal Audiovisual Scene-Aware\n  Sound Separation","summary":"  The audio-visual sound separation field assumes visible sources in videos,\nbut this excludes invisible sounds beyond the camera's view. Current methods\nstruggle with such sounds lacking visible cues. This paper introduces a novel\n\"Audio-Visual Scene-Aware Separation\" (AVSA-Sep) framework. It includes a\nsemantic parser for visible and invisible sounds and a separator for\nscene-informed separation. AVSA-Sep successfully separates both sound types,\nwith joint training and cross-modal alignment enhancing effectiveness.\n","authors":["Yiyang Su","Ali Vosoughi","Shijian Deng","Yapeng Tian","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.11713v1.pdf","comment":"Accepted at ICCV 2023 - AV4D, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.11708v1","updated":"2023-10-18T04:53:01Z","published":"2023-10-18T04:53:01Z","title":"Experimental Results of Underwater Sound Speed Profile Inversion by\n  Few-shot Multi-task Learning","summary":"  Underwater Sound Speed Profile (SSP) distribution has great influence on the\npropagation mode of acoustic signal, thus the fast and accurate estimation of\nSSP is of great importance in building underwater observation systems. The\nstate-of-the-art SSP inversion methods include frameworks of matched field\nprocessing (MFP), compressive sensing (CS), and feedforeward neural networks\n(FNN), among which the FNN shows better real-time performance while maintain\nthe same level of accuracy. However, the training of FNN needs quite a lot\nhistorical SSP samples, which is diffcult to be satisfied in many ocean areas.\nThis situation is called few-shot learning. To tackle this issue, we propose a\nmulti-task learning (MTL) model with partial parameter sharing among different\ntraning tasks. By MTL, common features could be extracted, thus accelerating\nthe learning process on given tasks, and reducing the demand for reference\nsamples, so as to enhance the generalization ability in few-shot learning. To\nverify the feasibility and effectiveness of MTL, a deep-ocean experiment was\nheld in April 2023 at the South China Sea. Results shows that MTL outperforms\nthe state-of-the-art methods in terms of accuracy for SSP inversion, while\ninherits the real-time advantage of FNN during the inversion stage.\n","authors":["Wei Huang","Fan Gao","Junting Wang","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.11708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12378v1","updated":"2023-10-18T23:10:46Z","published":"2023-10-18T23:10:46Z","title":"The CHiME-7 Challenge: System Description and Performance of NeMo Team's\n  DASR System","summary":"  We present the NVIDIA NeMo team's multi-channel speech recognition system for\nthe 7th CHiME Challenge Distant Automatic Speech Recognition (DASR) Task,\nfocusing on the development of a multi-channel, multi-speaker speech\nrecognition system tailored to transcribe speech from distributed microphones\nand microphone arrays. The system predominantly comprises of the following\nintegral modules: the Speaker Diarization Module, Multi-channel Audio Front-End\nProcessing Module, and the ASR Module. These components collectively establish\na cascading system, meticulously processing multi-channel and multi-speaker\naudio input. Moreover, this paper highlights the comprehensive optimization\nprocess that significantly enhanced our system's performance. Our team's\nsubmission is largely based on NeMo toolkits and will be publicly available.\n","authors":["Tae Jin Park","He Huang","Ante Jukic","Kunal Dhawan","Krishna C. Puvvada","Nithin Koluguri","Nikolay Karpov","Aleksandr Laptev","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2310.12378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12371v1","updated":"2023-10-18T22:46:20Z","published":"2023-10-18T22:46:20Z","title":"Property-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling\n  Technique for Synthetic Data Generation","summary":"  We introduce a sophisticated multi-speaker speech data simulator,\nspecifically engineered to generate multi-speaker speech recordings. A notable\nfeature of this simulator is its capacity to modulate the distribution of\nsilence and overlap via the adjustment of statistical parameters. This\ncapability offers a tailored training environment for developing neural models\nsuited for speaker diarization and voice activity detection. The acquisition of\nsubstantial datasets for speaker diarization often presents a significant\nchallenge, particularly in multi-speaker scenarios. Furthermore, the precise\ntime stamp annotation of speech data is a critical factor for training both\nspeaker diarization and voice activity detection. Our proposed multi-speaker\nsimulator tackles these problems by generating large-scale audio mixtures that\nmaintain statistical properties closely aligned with the input parameters. We\ndemonstrate that the proposed multi-speaker simulator generates audio mixtures\nwith statistical properties that closely align with the input parameters\nderived from real-world statistics. Additionally, we present the effectiveness\nof speaker diarization and voice activity detection models, which have been\ntrained exclusively on the generated simulated datasets.\n","authors":["Tae Jin Park","He Huang","Coleman Hooper","Nithin Koluguri","Kunal Dhawan","Ante Jukic","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2310.12371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09730v2","updated":"2023-10-18T19:23:27Z","published":"2022-12-19T18:53:04Z","title":"Speaking Style Conversion in the Waveform Domain Using Discrete\n  Self-Supervised Units","summary":"  We introduce DISSC, a novel, lightweight method that converts the rhythm,\npitch contour and timbre of a recording to a target speaker in a textless\nmanner. Unlike DISSC, most voice conversion (VC) methods focus primarily on\ntimbre, and ignore people's unique speaking style (prosody). The proposed\napproach uses a pretrained, self-supervised model for encoding speech to\ndiscrete units, which makes it simple, effective, and fast to train. All\nconversion modules are only trained on reconstruction like tasks, thus suitable\nfor any-to-many VC with no paired data. We introduce a suite of quantitative\nand qualitative evaluation metrics for this setup, and empirically demonstrate\nthat DISSC significantly outperforms the evaluated baselines. Code and samples\nare available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.\n","authors":["Gallil Maimon","Yossi Adi"],"pdf_url":"https://arxiv.org/pdf/2212.09730v2.pdf","comment":"Accepted at EMNLP 2023"}]},"2023-10-19T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2310.11804v2","updated":"2023-10-19T02:14:56Z","published":"2023-10-18T08:52:10Z","title":"Physics-informed Neural Network for Acoustic Resonance Analysis","summary":"  This study proposes the physics-informed neural network (PINN) framework to\nsolve the wave equation for acoustic resonance analysis. ResoNet, the\nanalytical model proposed in this study, minimizes the loss function for\nperiodic solutions, in addition to conventional PINN loss functions, thereby\neffectively using the function approximation capability of neural networks,\nwhile performing resonance analysis. Additionally, it can be easily applied to\ninverse problems. Herein, the resonance in a one-dimensional acoustic tube was\nanalyzed. The effectiveness of the proposed method was validated through the\nforward and inverse analyses of the wave equation with energy-loss terms. In\nthe forward analysis, the applicability of PINN to the resonance problem was\nevaluated by comparison with the finite-difference method. The inverse\nanalysis, which included the identification of the energy loss term in the wave\nequation and design optimization of the acoustic tube, was performed with good\naccuracy.\n","authors":["Kazuya Yokota","Takahiko Kurahashi","Masajiro Abe"],"pdf_url":"https://arxiv.org/pdf/2310.11804v2.pdf","comment":"11 pages, 14 figures. The following article has been submitted to the\n  Journal of the Acoustical Society of America. After it is published, it will\n  be found at https://pubs.aip.org/asa/jasa . v2: Corrected a typo in Eq. (22)"},{"id":"http://arxiv.org/abs/2310.12869v1","updated":"2023-10-19T16:18:10Z","published":"2023-10-19T16:18:10Z","title":"Uncertainty Quantification of Bandgaps in Acoustic Metamaterials with\n  Stochastic Geometric Defects and Material Properties","summary":"  This paper studies the utility of techniques within uncertainty\nquantification, namely spectral projection and polynomial chaos expansion, in\nreducing sampling needs for characterizing acoustic metamaterial dispersion\nband responses given stochastic material properties and geometric defects. A\nnovel method of encoding geometric defects in an interpretable, resolution\nindependent is showcased in the formation of input space probability\ndistributions. Orders of magnitude sampling reductions down to $\\sim10^0$ and\n$\\sim10^1$ are achieved in the 1D and 7D input space scenarios respectively\nwhile maintaining accurate output space probability distributions through\ncombining Monte Carlo, quadrature rule, and sparse grid sampling with surrogate\nmodel fitting.\n","authors":["Han Zhang","Rayehe Karimi Mahabadi","Cynthia Rudin","Johann Guilleminot","L. Catherine Brinson"],"pdf_url":"https://arxiv.org/pdf/2310.12869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12858v1","updated":"2023-10-19T16:09:44Z","published":"2023-10-19T16:09:44Z","title":"Audio Editing with Non-Rigid Text Prompts","summary":"  In this paper, we explore audio-editing with non-rigid text edits. We show\nthat the proposed editing pipeline is able to create audio edits that remain\nfaithful to the input audio. We explore text prompts that perform addition,\nstyle transfer, and in-painting. We quantitatively and qualitatively show that\nthe edits are able to obtain results which outperform Audio-LDM, a recently\nreleased text-prompted audio generation model. Qualitative inspection of the\nresults points out that the edits given by our approach remain more faithful to\nthe input audio in terms of keeping the original onsets and offsets of the\naudio events.\n","authors":["Francesco Paissan","Zhepei Wang","Mirco Ravanelli","Paris Smaragdis","Cem Subakan"],"pdf_url":"https://arxiv.org/pdf/2310.12858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12851v1","updated":"2023-10-19T16:02:53Z","published":"2023-10-19T16:02:53Z","title":"EmoDiarize: Speaker Diarization and Emotion Identification from Speech\n  Signals using Convolutional Neural Networks","summary":"  In the era of advanced artificial intelligence and human-computer\ninteraction, identifying emotions in spoken language is paramount. This\nresearch explores the integration of deep learning techniques in speech emotion\nrecognition, offering a comprehensive solution to the challenges associated\nwith speaker diarization and emotion identification. It introduces a framework\nthat combines a pre-existing speaker diarization pipeline and an emotion\nidentification model built on a Convolutional Neural Network (CNN) to achieve\nhigher precision. The proposed model was trained on data from five speech\nemotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out\nof which the latter is a speech emotion dataset created specifically for this\nresearch. The features extracted from each sample include Mel Frequency\nCepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS),\nand various data augmentation algorithms like pitch, noise, stretch, and shift.\nThis feature extraction approach aims to enhance prediction accuracy while\nreducing computational complexity. The proposed model yields an unweighted\naccuracy of 63%, demonstrating remarkable efficiency in accurately identifying\nemotional states within speech signals.\n","authors":["Hanan Hamza","Fiza Gafoor","Fathima Sithara","Gayathri Anil","V. S. Anoop"],"pdf_url":"https://arxiv.org/pdf/2310.12851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12765v1","updated":"2023-10-19T14:10:09Z","published":"2023-10-19T14:10:09Z","title":"Energy-Based Models For Speech Synthesis","summary":"  Recently there has been a lot of interest in non-autoregressive (non-AR)\nmodels for speech synthesis, such as FastSpeech 2 and diffusion models. Unlike\nAR models, these models do not have autoregressive dependencies among outputs\nwhich makes inference efficient. This paper expands the range of available\nnon-AR models with another member called energy-based models (EBMs). The paper\ndescribes how noise contrastive estimation, which relies on the comparison\nbetween positive and negative samples, can be used to train EBMs. It proposes a\nnumber of strategies for generating effective negative samples, including using\nhigh-performing AR models. It also describes how sampling from EBMs can be\nperformed using Langevin Markov Chain Monte-Carlo (MCMC). The use of Langevin\nMCMC enables to draw connections between EBMs and currently popular diffusion\nmodels. Experiments on LJSpeech dataset show that the proposed approach offers\nimprovements over Tacotron 2.\n","authors":["Wanli Sun","Zehai Tu","Anton Ragni"],"pdf_url":"https://arxiv.org/pdf/2310.12765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15687v2","updated":"2023-10-19T13:23:28Z","published":"2023-06-23T16:23:24Z","title":"Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale","summary":"  Large-scale generative models such as GPT and DALL-E have revolutionized the\nresearch community. These models not only generate high fidelity outputs, but\nare also generalists which can solve tasks not explicitly taught. In contrast,\nspeech generative models are still primitive in terms of scale and task\ngeneralization. In this paper, we present Voicebox, the most versatile\ntext-guided generative model for speech at scale. Voicebox is a\nnon-autoregressive flow-matching model trained to infill speech, given audio\ncontext and text, trained on over 50K hours of speech that are not filtered or\nenhanced. Similar to GPT, Voicebox can perform many different tasks through\nin-context learning, but is more flexible as it can also condition on future\ncontext. Voicebox can be used for mono or cross-lingual zero-shot\ntext-to-speech synthesis, noise removal, content editing, style conversion, and\ndiverse sample generation. In particular, Voicebox outperforms the\nstate-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs\n1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to\n20 times faster. Audio samples can be found in\n\\url{https://voicebox.metademolab.com}.\n","authors":["Matthew Le","Apoorv Vyas","Bowen Shi","Brian Karrer","Leda Sari","Rashel Moritz","Mary Williamson","Vimal Manohar","Yossi Adi","Jay Mahadeokar","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2306.15687v2.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2210.13623v3","updated":"2023-10-19T13:15:48Z","published":"2022-10-24T21:49:12Z","title":"Reinforcement Learning and Bandits for Speech and Language Processing:\n  Tutorial, Review and Outlook","summary":"  In recent years, reinforcement learning and bandits have transformed a wide\nrange of real-world applications including healthcare, finance, recommendation\nsystems, robotics, and last but not least, the speech and natural language\nprocessing. While most speech and language applications of reinforcement\nlearning algorithms are centered around improving the training of deep neural\nnetworks with its flexible optimization properties, there are still many\ngrounds to explore to utilize the benefits of reinforcement learning, such as\nits reward-driven adaptability, state representations, temporal structures and\ngeneralizability. In this survey, we present an overview of recent advancements\nof reinforcement learning and bandits, and discuss how they can be effectively\nemployed to solve speech and natural language processing problems with models\nthat are adaptive, interactive and scalable.\n","authors":["Baihan Lin"],"pdf_url":"https://arxiv.org/pdf/2210.13623v3.pdf","comment":"To appear in Expert Systems with Applications. Accompanying\n  INTERSPEECH 2022 Tutorial on the same topic. Including latest advancements in\n  large language models (LLMs)"},{"id":"http://arxiv.org/abs/2206.02147v3","updated":"2023-10-19T06:22:47Z","published":"2022-06-05T10:50:34Z","title":"Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for\n  Text-to-Speech","summary":"  Polyphone disambiguation aims to capture accurate pronunciation knowledge\nfrom natural text sequences for reliable Text-to-speech (TTS) systems. However,\nprevious approaches require substantial annotated training data and additional\nefforts from language experts, making it difficult to extend high-quality\nneural TTS systems to out-of-domain daily conversations and countless languages\nworldwide. This paper tackles the polyphone disambiguation problem from a\nconcise and novel perspective: we propose Dict-TTS, a semantic-aware generative\ntext-to-speech model with an online website dictionary (the existing prior\ninformation in the natural language). Specifically, we design a\nsemantics-to-pronunciation attention (S2PA) module to match the semantic\npatterns between the input text sequence and the prior semantics in the\ndictionary and obtain the corresponding pronunciations; The S2PA module can be\neasily trained with the end-to-end TTS model without any annotated phoneme\nlabels. Experimental results in three languages show that our model outperforms\nseveral strong baseline models in terms of pronunciation accuracy and improves\nthe prosody modeling of TTS systems. Further extensive analyses demonstrate\nthat each design in Dict-TTS is effective. The code is available at\n\\url{https://github.com/Zain-Jiang/Dict-TTS}.\n","authors":["Ziyue Jiang","Zhe Su","Zhou Zhao","Qian Yang","Yi Ren","Jinglin Liu","Zhenhui Ye"],"pdf_url":"https://arxiv.org/pdf/2206.02147v3.pdf","comment":"v3: fix the introduction for the concurrent similar work of Neural\n  Lexicon Reader (arXiv:2110.09698)"},{"id":"http://arxiv.org/abs/2305.14381v2","updated":"2023-10-19T02:55:13Z","published":"2023-05-22T09:44:39Z","title":"Connecting Multi-modal Contrastive Representations","summary":"  Multi-modal Contrastive Representation learning aims to encode different\nmodalities into a semantically aligned shared space. This paradigm shows\nremarkable generalization ability on numerous downstream tasks across various\nmodalities. However, the reliance on massive high-quality data pairs limits its\nfurther development on more modalities. This paper proposes a novel\ntraining-efficient method for learning MCR without paired data called\nConnecting Multi-modal Contrastive Representations (C-MCR). Specifically, given\ntwo existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project\nthem to a new space and use the data from the overlapping modality B to\naligning the two MCRs in the new space. Meanwhile, since the modality pairs (A,\nB) and (B, C) are already aligned within each MCR, the connection learned by\noverlapping modality can also be transferred to non-overlapping modality pair\n(A, C). To unleash the potential of C-MCR, we further introduce a\nsemantic-enhanced inter- and intra-MCR connection method. We first enhance the\nsemantic consistency and completion of embeddings across different modalities\nfor more robust alignment. Then we utilize the inter-MCR alignment to establish\nthe connection, and employ the intra-MCR alignment to better maintain the\nconnection for inputs from non-overlapping modalities. To demonstrate the\neffectiveness of C-MCR, we connect CLIP and CLAP via texts to derive\naudio-visual representations, and integrate CLIP and ULIP via images for\n3D-language representations. Remarkably, without using any paired data, C-MCR\nfor audio-visual achieves state-of-the-art performance on audio-image\nretrieval, audio-visual source localization, and counterfactual audio-image\nrecognition tasks. Furthermore, C-MCR for 3D-language also attains advanced\nzero-shot 3D point cloud classification accuracy on ModelNet40.\n","authors":["Zehan Wang","Yang Zhao","Xize Cheng","Haifeng Huang","Jiageng Liu","Li Tang","Linjun Li","Yongqi Wang","Aoxiong Yin","Ziang Zhang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2305.14381v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.12404v1","updated":"2023-10-19T01:20:12Z","published":"2023-10-19T01:20:12Z","title":"Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative\n  Editing","summary":"  Creating music is iterative, requiring varied methods at each stage. However,\nexisting AI music systems fall short in orchestrating multiple subsystems for\ndiverse needs. To address this gap, we introduce Loop Copilot, a novel system\nthat enables users to generate and iteratively refine music through an\ninteractive, multi-round dialogue interface. The system uses a large language\nmodel to interpret user intentions and select appropriate AI models for task\nexecution. Each backend model is specialized for a specific task, and their\noutputs are aggregated to meet the user's requirements. To ensure musical\ncoherence, essential attributes are maintained in a centralized table. We\nevaluate the effectiveness of the proposed system through semi-structured\ninterviews and questionnaires, highlighting its utility not only in\nfacilitating music creation but also its potential for broader applications.\n","authors":["Yixiao Zhang","Akira Maezawa","Gus Xia","Kazuhiko Yamamoto","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2310.12404v1.pdf","comment":"Source code and demo video are available at\n  \\url{https://sites.google.com/view/loop-copilot}"},{"id":"http://arxiv.org/abs/2309.11895v3","updated":"2023-10-19T21:32:01Z","published":"2023-09-21T08:59:13Z","title":"Audio Contrastive based Fine-tuning","summary":"  Audio classification plays a crucial role in speech and sound processing\ntasks with a wide range of applications. There still remains a challenge of\nstriking the right balance between fitting the model to the training data\n(avoiding overfitting) and enabling it to generalise well to a new domain.\nLeveraging the transferability of contrastive learning, we introduce Audio\nContrastive-based Fine-tuning (AudioConFit), an efficient approach\ncharacterised by robust generalisability. Empirical experiments on a variety of\naudio classification tasks demonstrate the effectiveness and robustness of our\napproach, which achieves state-of-the-art results in various settings.\n","authors":["Yang Wang","Qibin Liang","Chenghao Xiao","Yizhi Li","Noura Al Moubayed","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2309.11895v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2211.02809v3","updated":"2023-10-19T20:35:13Z","published":"2022-11-05T04:03:55Z","title":"LAMASSU: Streaming Language-Agnostic Multilingual Speech Recognition and\n  Translation Using Neural Transducers","summary":"  Automatic speech recognition (ASR) and speech translation (ST) can both use\nneural transducers as the model structure. It is thus possible to use a single\ntransducer model to perform both tasks. In real-world applications, such joint\nASR and ST models may need to be streaming and do not require source language\nidentification (i.e. language-agnostic). In this paper, we propose LAMASSU, a\nstreaming language-agnostic multilingual speech recognition and translation\nmodel using neural transducers. Based on the transducer model structure, we\npropose four methods, a unified joint and prediction network for multilingual\noutput, a clustered multilingual encoder, target language identification for\nencoder, and connectionist temporal classification regularization. Experimental\nresults show that LAMASSU not only drastically reduces the model size but also\nreaches the performances of monolingual ASR and bilingual ST models.\n","authors":["Peidong Wang","Eric Sun","Jian Xue","Yu Wu","Long Zhou","Yashesh Gaur","Shujie Liu","Jinyu Li"],"pdf_url":"https://arxiv.org/pdf/2211.02809v3.pdf","comment":"INTERSPEECH 2023"},{"id":"http://arxiv.org/abs/2310.13103v1","updated":"2023-10-19T19:01:26Z","published":"2023-10-19T19:01:26Z","title":"AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting\n  Multiple Experts for Video Deepfake Detection","summary":"  Forged content shared widely on social media platforms is a major social\nproblem that requires increased regulation and poses new challenges to the\nresearch community. The recent proliferation of hyper-realistic deepfake videos\nhas drawn attention to the threat of audio and visual forgeries. Most previous\nwork on detecting AI-generated fake videos only utilizes visual modality or\naudio modality. While there are some methods in the literature that exploit\naudio and visual modalities to detect forged videos, they have not been\ncomprehensively evaluated on multi-modal datasets of deepfake videos involving\nacoustic and visual manipulations. Moreover, these existing methods are mostly\nbased on CNN and suffer from low detection accuracy. Inspired by the recent\nsuccess of Transformer in various fields, to address the challenges posed by\ndeepfake technology, in this paper, we propose an Audio-Visual\nTransformer-based Ensemble Network (AVTENet) framework that considers both\nacoustic manipulation and visual manipulation to achieve effective video\nforgery detection. Specifically, the proposed model integrates several purely\ntransformer-based variants that capture video, audio, and audio-visual salient\ncues to reach a consensus in prediction. For evaluation, we use the recently\nreleased benchmark multi-modal audio-video FakeAVCeleb dataset. For a detailed\nanalysis, we evaluate AVTENet, its variants, and several existing methods on\nmultiple test sets of the FakeAVCeleb dataset. Experimental results show that\nour best model outperforms all existing methods and achieves state-of-the-art\nperformance on Testset-I and Testset-II of the FakeAVCeleb dataset.\n","authors":["Ammarah Hashmi","Sahibzada Adil Shahzad","Chia-Wen Lin","Yu Tsao","Hsin-Min Wang"],"pdf_url":"https://arxiv.org/pdf/2310.13103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13025v1","updated":"2023-10-19T06:51:43Z","published":"2023-10-19T06:51:43Z","title":"Powerset multi-class cross entropy loss for neural speaker diarization","summary":"  Since its introduction in 2019, the whole end-to-end neural diarization\n(EEND) line of work has been addressing speaker diarization as a frame-wise\nmulti-label classification problem with permutation-invariant training. Despite\nEEND showing great promise, a few recent works took a step back and studied the\npossible combination of (local) supervised EEND diarization with (global)\nunsupervised clustering. Yet, these hybrid contributions did not question the\noriginal multi-label formulation. We propose to switch from multi-label (where\nany two speakers can be active at the same time) to powerset multi-class\nclassification (where dedicated classes are assigned to pairs of overlapping\nspeakers). Through extensive experiments on 9 different benchmarks, we show\nthat this formulation leads to significantly better performance (mostly on\noverlapping speech) and robustness to domain mismatch, while eliminating the\ndetection threshold hyperparameter, critical for the multi-label formulation.\n","authors":["Alexis Plaquet","Herv√© Bredin"],"pdf_url":"https://arxiv.org/pdf/2310.13025v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.11804v2","updated":"2023-10-19T02:14:56Z","published":"2023-10-18T08:52:10Z","title":"Physics-informed Neural Network for Acoustic Resonance Analysis","summary":"  This study proposes the physics-informed neural network (PINN) framework to\nsolve the wave equation for acoustic resonance analysis. ResoNet, the\nanalytical model proposed in this study, minimizes the loss function for\nperiodic solutions, in addition to conventional PINN loss functions, thereby\neffectively using the function approximation capability of neural networks,\nwhile performing resonance analysis. Additionally, it can be easily applied to\ninverse problems. Herein, the resonance in a one-dimensional acoustic tube was\nanalyzed. The effectiveness of the proposed method was validated through the\nforward and inverse analyses of the wave equation with energy-loss terms. In\nthe forward analysis, the applicability of PINN to the resonance problem was\nevaluated by comparison with the finite-difference method. The inverse\nanalysis, which included the identification of the energy loss term in the wave\nequation and design optimization of the acoustic tube, was performed with good\naccuracy.\n","authors":["Kazuya Yokota","Takahiko Kurahashi","Masajiro Abe"],"pdf_url":"https://arxiv.org/pdf/2310.11804v2.pdf","comment":"11 pages, 14 figures. The following article has been submitted to the\n  Journal of the Acoustical Society of America. After it is published, it will\n  be found at https://pubs.aip.org/asa/jasa . v2: Corrected a typo in Eq. (22)"},{"id":"http://arxiv.org/abs/2310.12869v1","updated":"2023-10-19T16:18:10Z","published":"2023-10-19T16:18:10Z","title":"Uncertainty Quantification of Bandgaps in Acoustic Metamaterials with\n  Stochastic Geometric Defects and Material Properties","summary":"  This paper studies the utility of techniques within uncertainty\nquantification, namely spectral projection and polynomial chaos expansion, in\nreducing sampling needs for characterizing acoustic metamaterial dispersion\nband responses given stochastic material properties and geometric defects. A\nnovel method of encoding geometric defects in an interpretable, resolution\nindependent is showcased in the formation of input space probability\ndistributions. Orders of magnitude sampling reductions down to $\\sim10^0$ and\n$\\sim10^1$ are achieved in the 1D and 7D input space scenarios respectively\nwhile maintaining accurate output space probability distributions through\ncombining Monte Carlo, quadrature rule, and sparse grid sampling with surrogate\nmodel fitting.\n","authors":["Han Zhang","Rayehe Karimi Mahabadi","Cynthia Rudin","Johann Guilleminot","L. Catherine Brinson"],"pdf_url":"https://arxiv.org/pdf/2310.12869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12858v1","updated":"2023-10-19T16:09:44Z","published":"2023-10-19T16:09:44Z","title":"Audio Editing with Non-Rigid Text Prompts","summary":"  In this paper, we explore audio-editing with non-rigid text edits. We show\nthat the proposed editing pipeline is able to create audio edits that remain\nfaithful to the input audio. We explore text prompts that perform addition,\nstyle transfer, and in-painting. We quantitatively and qualitatively show that\nthe edits are able to obtain results which outperform Audio-LDM, a recently\nreleased text-prompted audio generation model. Qualitative inspection of the\nresults points out that the edits given by our approach remain more faithful to\nthe input audio in terms of keeping the original onsets and offsets of the\naudio events.\n","authors":["Francesco Paissan","Zhepei Wang","Mirco Ravanelli","Paris Smaragdis","Cem Subakan"],"pdf_url":"https://arxiv.org/pdf/2310.12858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12851v1","updated":"2023-10-19T16:02:53Z","published":"2023-10-19T16:02:53Z","title":"EmoDiarize: Speaker Diarization and Emotion Identification from Speech\n  Signals using Convolutional Neural Networks","summary":"  In the era of advanced artificial intelligence and human-computer\ninteraction, identifying emotions in spoken language is paramount. This\nresearch explores the integration of deep learning techniques in speech emotion\nrecognition, offering a comprehensive solution to the challenges associated\nwith speaker diarization and emotion identification. It introduces a framework\nthat combines a pre-existing speaker diarization pipeline and an emotion\nidentification model built on a Convolutional Neural Network (CNN) to achieve\nhigher precision. The proposed model was trained on data from five speech\nemotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out\nof which the latter is a speech emotion dataset created specifically for this\nresearch. The features extracted from each sample include Mel Frequency\nCepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS),\nand various data augmentation algorithms like pitch, noise, stretch, and shift.\nThis feature extraction approach aims to enhance prediction accuracy while\nreducing computational complexity. The proposed model yields an unweighted\naccuracy of 63%, demonstrating remarkable efficiency in accurately identifying\nemotional states within speech signals.\n","authors":["Hanan Hamza","Fiza Gafoor","Fathima Sithara","Gayathri Anil","V. S. Anoop"],"pdf_url":"https://arxiv.org/pdf/2310.12851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12837v1","updated":"2023-10-19T15:40:42Z","published":"2023-10-19T15:40:42Z","title":"Deep Beamforming for Speech Enhancement and Speaker Localization with an\n  Array Response-Aware Loss Function","summary":"  Recent research advances in deep neural network (DNN)-based beamformers have\nshown great promise for speech enhancement under adverse acoustic conditions.\nDifferent network architectures and input features have been explored in\nestimating beamforming weights. In this paper, we propose a deep beamformer\nbased on an efficient convolutional recurrent network (CNN) trained with a\nnovel ARray RespOnse-aWare (ARROW) loss function. The ARROW loss exploits the\narray responses of the target and interferer by using the ground truth relative\ntransfer functions (RTFs). The DNN-based beamforming system, trained with ARROW\nloss through supervised learning, is able to perform speech enhancement and\nspeaker localization jointly. Experimental results have shown that the proposed\ndeep beamformer, trained with the linearly weighted scale-invariant\nsource-to-noise ratio (SI-SNR) and ARROW loss functions, achieves superior\nperformance in speech enhancement and speaker localization compared to two\nbaselines.\n","authors":["Hsinyu Chang","Yicheng Hsu","Mingsian R. Bai"],"pdf_url":"https://arxiv.org/pdf/2310.12837v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2310.12765v1","updated":"2023-10-19T14:10:09Z","published":"2023-10-19T14:10:09Z","title":"Energy-Based Models For Speech Synthesis","summary":"  Recently there has been a lot of interest in non-autoregressive (non-AR)\nmodels for speech synthesis, such as FastSpeech 2 and diffusion models. Unlike\nAR models, these models do not have autoregressive dependencies among outputs\nwhich makes inference efficient. This paper expands the range of available\nnon-AR models with another member called energy-based models (EBMs). The paper\ndescribes how noise contrastive estimation, which relies on the comparison\nbetween positive and negative samples, can be used to train EBMs. It proposes a\nnumber of strategies for generating effective negative samples, including using\nhigh-performing AR models. It also describes how sampling from EBMs can be\nperformed using Langevin Markov Chain Monte-Carlo (MCMC). The use of Langevin\nMCMC enables to draw connections between EBMs and currently popular diffusion\nmodels. Experiments on LJSpeech dataset show that the proposed approach offers\nimprovements over Tacotron 2.\n","authors":["Wanli Sun","Zehai Tu","Anton Ragni"],"pdf_url":"https://arxiv.org/pdf/2310.12765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15687v2","updated":"2023-10-19T13:23:28Z","published":"2023-06-23T16:23:24Z","title":"Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale","summary":"  Large-scale generative models such as GPT and DALL-E have revolutionized the\nresearch community. These models not only generate high fidelity outputs, but\nare also generalists which can solve tasks not explicitly taught. In contrast,\nspeech generative models are still primitive in terms of scale and task\ngeneralization. In this paper, we present Voicebox, the most versatile\ntext-guided generative model for speech at scale. Voicebox is a\nnon-autoregressive flow-matching model trained to infill speech, given audio\ncontext and text, trained on over 50K hours of speech that are not filtered or\nenhanced. Similar to GPT, Voicebox can perform many different tasks through\nin-context learning, but is more flexible as it can also condition on future\ncontext. Voicebox can be used for mono or cross-lingual zero-shot\ntext-to-speech synthesis, noise removal, content editing, style conversion, and\ndiverse sample generation. In particular, Voicebox outperforms the\nstate-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs\n1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to\n20 times faster. Audio samples can be found in\n\\url{https://voicebox.metademolab.com}.\n","authors":["Matthew Le","Apoorv Vyas","Bowen Shi","Brian Karrer","Leda Sari","Rashel Moritz","Mary Williamson","Vimal Manohar","Yossi Adi","Jay Mahadeokar","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2306.15687v2.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2210.13623v3","updated":"2023-10-19T13:15:48Z","published":"2022-10-24T21:49:12Z","title":"Reinforcement Learning and Bandits for Speech and Language Processing:\n  Tutorial, Review and Outlook","summary":"  In recent years, reinforcement learning and bandits have transformed a wide\nrange of real-world applications including healthcare, finance, recommendation\nsystems, robotics, and last but not least, the speech and natural language\nprocessing. While most speech and language applications of reinforcement\nlearning algorithms are centered around improving the training of deep neural\nnetworks with its flexible optimization properties, there are still many\ngrounds to explore to utilize the benefits of reinforcement learning, such as\nits reward-driven adaptability, state representations, temporal structures and\ngeneralizability. In this survey, we present an overview of recent advancements\nof reinforcement learning and bandits, and discuss how they can be effectively\nemployed to solve speech and natural language processing problems with models\nthat are adaptive, interactive and scalable.\n","authors":["Baihan Lin"],"pdf_url":"https://arxiv.org/pdf/2210.13623v3.pdf","comment":"To appear in Expert Systems with Applications. Accompanying\n  INTERSPEECH 2022 Tutorial on the same topic. Including latest advancements in\n  large language models (LLMs)"},{"id":"http://arxiv.org/abs/2310.12599v1","updated":"2023-10-19T09:10:53Z","published":"2023-10-19T09:10:53Z","title":"On Feature Importance and Interpretability of Speaker Representations","summary":"  Unsupervised speech disentanglement aims at separating fast varying from\nslowly varying components of a speech signal. In this contribution, we take a\ncloser look at the embedding vector representing the slowly varying signal\ncomponents, commonly named the speaker embedding vector. We ask, which\nproperties of a speaker's voice are captured and investigate to which extent do\nindividual embedding vector components sign responsible for them, using the\nconcept of Shapley values. Our findings show that certain speaker-specific\nacoustic-phonetic properties can be fairly well predicted from the speaker\nembedding, while the investigated more abstract voice quality features cannot.\n","authors":["Frederik Rautenberg","Michael Kuhlmann","Jana Wiechmann","Fritz Seebauer","Petra Wagner","Reinhold Haeb-Umbach"],"pdf_url":"https://arxiv.org/pdf/2310.12599v1.pdf","comment":"Presented at the ITG conference on Speech Communication 2023"},{"id":"http://arxiv.org/abs/2206.02147v3","updated":"2023-10-19T06:22:47Z","published":"2022-06-05T10:50:34Z","title":"Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for\n  Text-to-Speech","summary":"  Polyphone disambiguation aims to capture accurate pronunciation knowledge\nfrom natural text sequences for reliable Text-to-speech (TTS) systems. However,\nprevious approaches require substantial annotated training data and additional\nefforts from language experts, making it difficult to extend high-quality\nneural TTS systems to out-of-domain daily conversations and countless languages\nworldwide. This paper tackles the polyphone disambiguation problem from a\nconcise and novel perspective: we propose Dict-TTS, a semantic-aware generative\ntext-to-speech model with an online website dictionary (the existing prior\ninformation in the natural language). Specifically, we design a\nsemantics-to-pronunciation attention (S2PA) module to match the semantic\npatterns between the input text sequence and the prior semantics in the\ndictionary and obtain the corresponding pronunciations; The S2PA module can be\neasily trained with the end-to-end TTS model without any annotated phoneme\nlabels. Experimental results in three languages show that our model outperforms\nseveral strong baseline models in terms of pronunciation accuracy and improves\nthe prosody modeling of TTS systems. Further extensive analyses demonstrate\nthat each design in Dict-TTS is effective. The code is available at\n\\url{https://github.com/Zain-Jiang/Dict-TTS}.\n","authors":["Ziyue Jiang","Zhe Su","Zhou Zhao","Qian Yang","Yi Ren","Jinglin Liu","Zhenhui Ye"],"pdf_url":"https://arxiv.org/pdf/2206.02147v3.pdf","comment":"v3: fix the introduction for the concurrent similar work of Neural\n  Lexicon Reader (arXiv:2110.09698)"},{"id":"http://arxiv.org/abs/2310.12477v1","updated":"2023-10-19T05:31:45Z","published":"2023-10-19T05:31:45Z","title":"An Exploration of In-Context Learning for Speech Language Model","summary":"  Ever since the development of GPT-3 in the natural language processing (NLP)\nfield, in-context learning (ICL) has played an important role in utilizing\nlarge language models (LLMs). By presenting the LM utterance-label\ndemonstrations at the input, the LM can accomplish few-shot learning without\nrelying on gradient descent or requiring explicit modification of its\nparameters. This enables the LM to learn and adapt in a black-box manner.\nDespite the success of ICL in NLP, little work is exploring the possibility of\nICL in speech processing. This study proposes the first exploration of ICL with\na speech LM without text supervision. We first show that the current speech LM\ndoes not have the ICL capability. With the proposed warmup training, the speech\nLM can, therefore, perform ICL on unseen tasks. In this work, we verify the\nfeasibility of ICL for speech LM on speech classification tasks.\n","authors":["Ming-Hao Hsu","Kai-Wei Chang","Shang-Wen Li","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2310.12477v1.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2305.14381v2","updated":"2023-10-19T02:55:13Z","published":"2023-05-22T09:44:39Z","title":"Connecting Multi-modal Contrastive Representations","summary":"  Multi-modal Contrastive Representation learning aims to encode different\nmodalities into a semantically aligned shared space. This paradigm shows\nremarkable generalization ability on numerous downstream tasks across various\nmodalities. However, the reliance on massive high-quality data pairs limits its\nfurther development on more modalities. This paper proposes a novel\ntraining-efficient method for learning MCR without paired data called\nConnecting Multi-modal Contrastive Representations (C-MCR). Specifically, given\ntwo existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project\nthem to a new space and use the data from the overlapping modality B to\naligning the two MCRs in the new space. Meanwhile, since the modality pairs (A,\nB) and (B, C) are already aligned within each MCR, the connection learned by\noverlapping modality can also be transferred to non-overlapping modality pair\n(A, C). To unleash the potential of C-MCR, we further introduce a\nsemantic-enhanced inter- and intra-MCR connection method. We first enhance the\nsemantic consistency and completion of embeddings across different modalities\nfor more robust alignment. Then we utilize the inter-MCR alignment to establish\nthe connection, and employ the intra-MCR alignment to better maintain the\nconnection for inputs from non-overlapping modalities. To demonstrate the\neffectiveness of C-MCR, we connect CLIP and CLAP via texts to derive\naudio-visual representations, and integrate CLIP and ULIP via images for\n3D-language representations. Remarkably, without using any paired data, C-MCR\nfor audio-visual achieves state-of-the-art performance on audio-image\nretrieval, audio-visual source localization, and counterfactual audio-image\nrecognition tasks. Furthermore, C-MCR for 3D-language also attains advanced\nzero-shot 3D point cloud classification accuracy on ModelNet40.\n","authors":["Zehan Wang","Yang Zhao","Xize Cheng","Haifeng Huang","Jiageng Liu","Li Tang","Linjun Li","Yongqi Wang","Aoxiong Yin","Ziang Zhang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2305.14381v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.12404v1","updated":"2023-10-19T01:20:12Z","published":"2023-10-19T01:20:12Z","title":"Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative\n  Editing","summary":"  Creating music is iterative, requiring varied methods at each stage. However,\nexisting AI music systems fall short in orchestrating multiple subsystems for\ndiverse needs. To address this gap, we introduce Loop Copilot, a novel system\nthat enables users to generate and iteratively refine music through an\ninteractive, multi-round dialogue interface. The system uses a large language\nmodel to interpret user intentions and select appropriate AI models for task\nexecution. Each backend model is specialized for a specific task, and their\noutputs are aggregated to meet the user's requirements. To ensure musical\ncoherence, essential attributes are maintained in a centralized table. We\nevaluate the effectiveness of the proposed system through semi-structured\ninterviews and questionnaires, highlighting its utility not only in\nfacilitating music creation but also its potential for broader applications.\n","authors":["Yixiao Zhang","Akira Maezawa","Gus Xia","Kazuhiko Yamamoto","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2310.12404v1.pdf","comment":"Source code and demo video are available at\n  \\url{https://sites.google.com/view/loop-copilot}"},{"id":"http://arxiv.org/abs/2310.12399v1","updated":"2023-10-19T01:02:47Z","published":"2023-10-19T01:02:47Z","title":"A New Time Series Similarity Measure and Its Smart Grid Applications","summary":"  Many smart grid applications involve data mining, clustering, classification,\nidentification, and anomaly detection, among others. These applications\nprimarily depend on the measurement of similarity, which is the distance\nbetween different time series or subsequences of a time series. The commonly\nused time series distance measures, namely Euclidean Distance (ED) and Dynamic\nTime Warping (DTW), do not quantify the flexible nature of electricity usage\ndata in terms of temporal dynamics. As a result, there is a need for a new\ndistance measure that can quantify both the amplitude and temporal changes of\nelectricity time series for smart grid applications, e.g., demand response and\nload profiling. This paper introduces a novel distance measure to compare\nelectricity usage patterns. The method consists of two phases that quantify the\neffort required to reshape one time series into another, considering both\namplitude and temporal changes. The proposed method is evaluated against ED and\nDTW using real-world data in three smart grid applications. Overall, the\nproposed measure outperforms ED and DTW in accurately identifying the best load\nscheduling strategy, anomalous days with irregular electricity usage, and\ndetermining electricity users' behind-the-meter (BTM) equipment.\n","authors":["Rui Yuan","S. Ali Pourmousavi","Wen L. Soong","Andrew J. Black","Jon A. R. Liisberg","Julian Lemos-Vinasco"],"pdf_url":"https://arxiv.org/pdf/2310.12399v1.pdf","comment":"7 pages, 6 figures conference"},{"id":"http://arxiv.org/abs/2309.11895v3","updated":"2023-10-19T21:32:01Z","published":"2023-09-21T08:59:13Z","title":"Audio Contrastive based Fine-tuning","summary":"  Audio classification plays a crucial role in speech and sound processing\ntasks with a wide range of applications. There still remains a challenge of\nstriking the right balance between fitting the model to the training data\n(avoiding overfitting) and enabling it to generalise well to a new domain.\nLeveraging the transferability of contrastive learning, we introduce Audio\nContrastive-based Fine-tuning (AudioConFit), an efficient approach\ncharacterised by robust generalisability. Empirical experiments on a variety of\naudio classification tasks demonstrate the effectiveness and robustness of our\napproach, which achieves state-of-the-art results in various settings.\n","authors":["Yang Wang","Qibin Liang","Chenghao Xiao","Yizhi Li","Noura Al Moubayed","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2309.11895v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2211.02809v3","updated":"2023-10-19T20:35:13Z","published":"2022-11-05T04:03:55Z","title":"LAMASSU: Streaming Language-Agnostic Multilingual Speech Recognition and\n  Translation Using Neural Transducers","summary":"  Automatic speech recognition (ASR) and speech translation (ST) can both use\nneural transducers as the model structure. It is thus possible to use a single\ntransducer model to perform both tasks. In real-world applications, such joint\nASR and ST models may need to be streaming and do not require source language\nidentification (i.e. language-agnostic). In this paper, we propose LAMASSU, a\nstreaming language-agnostic multilingual speech recognition and translation\nmodel using neural transducers. Based on the transducer model structure, we\npropose four methods, a unified joint and prediction network for multilingual\noutput, a clustered multilingual encoder, target language identification for\nencoder, and connectionist temporal classification regularization. Experimental\nresults show that LAMASSU not only drastically reduces the model size but also\nreaches the performances of monolingual ASR and bilingual ST models.\n","authors":["Peidong Wang","Eric Sun","Jian Xue","Yu Wu","Long Zhou","Yashesh Gaur","Shujie Liu","Jinyu Li"],"pdf_url":"https://arxiv.org/pdf/2211.02809v3.pdf","comment":"INTERSPEECH 2023"},{"id":"http://arxiv.org/abs/2310.13103v1","updated":"2023-10-19T19:01:26Z","published":"2023-10-19T19:01:26Z","title":"AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting\n  Multiple Experts for Video Deepfake Detection","summary":"  Forged content shared widely on social media platforms is a major social\nproblem that requires increased regulation and poses new challenges to the\nresearch community. The recent proliferation of hyper-realistic deepfake videos\nhas drawn attention to the threat of audio and visual forgeries. Most previous\nwork on detecting AI-generated fake videos only utilizes visual modality or\naudio modality. While there are some methods in the literature that exploit\naudio and visual modalities to detect forged videos, they have not been\ncomprehensively evaluated on multi-modal datasets of deepfake videos involving\nacoustic and visual manipulations. Moreover, these existing methods are mostly\nbased on CNN and suffer from low detection accuracy. Inspired by the recent\nsuccess of Transformer in various fields, to address the challenges posed by\ndeepfake technology, in this paper, we propose an Audio-Visual\nTransformer-based Ensemble Network (AVTENet) framework that considers both\nacoustic manipulation and visual manipulation to achieve effective video\nforgery detection. Specifically, the proposed model integrates several purely\ntransformer-based variants that capture video, audio, and audio-visual salient\ncues to reach a consensus in prediction. For evaluation, we use the recently\nreleased benchmark multi-modal audio-video FakeAVCeleb dataset. For a detailed\nanalysis, we evaluate AVTENet, its variants, and several existing methods on\nmultiple test sets of the FakeAVCeleb dataset. Experimental results show that\nour best model outperforms all existing methods and achieves state-of-the-art\nperformance on Testset-I and Testset-II of the FakeAVCeleb dataset.\n","authors":["Ammarah Hashmi","Sahibzada Adil Shahzad","Chia-Wen Lin","Yu Tsao","Hsin-Min Wang"],"pdf_url":"https://arxiv.org/pdf/2310.13103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13025v1","updated":"2023-10-19T06:51:43Z","published":"2023-10-19T06:51:43Z","title":"Powerset multi-class cross entropy loss for neural speaker diarization","summary":"  Since its introduction in 2019, the whole end-to-end neural diarization\n(EEND) line of work has been addressing speaker diarization as a frame-wise\nmulti-label classification problem with permutation-invariant training. Despite\nEEND showing great promise, a few recent works took a step back and studied the\npossible combination of (local) supervised EEND diarization with (global)\nunsupervised clustering. Yet, these hybrid contributions did not question the\noriginal multi-label formulation. We propose to switch from multi-label (where\nany two speakers can be active at the same time) to powerset multi-class\nclassification (where dedicated classes are assigned to pairs of overlapping\nspeakers). Through extensive experiments on 9 different benchmarks, we show\nthat this formulation leads to significantly better performance (mostly on\noverlapping speech) and robustness to domain mismatch, while eliminating the\ndetection threshold hyperparameter, critical for the multi-label formulation.\n","authors":["Alexis Plaquet","Herv√© Bredin"],"pdf_url":"https://arxiv.org/pdf/2310.13025v1.pdf","comment":null}]},"2023-10-20T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2310.08981v2","updated":"2023-10-20T10:15:46Z","published":"2023-10-13T09:57:09Z","title":"Low-latency Speech Enhancement via Speech Token Generation","summary":"  Existing deep learning based speech enhancement mainly employ a data-driven\napproach, which leverage large amounts of data with a variety of noise types to\nachieve noise removal from noisy signal. However, the high dependence on the\ndata limits its generalization on the unseen complex noises in real-life\nenvironment. In this paper, we focus on the low-latency scenario and regard\nspeech enhancement as a speech generation problem conditioned on the noisy\nsignal, where we generate clean speech instead of identifying and removing\nnoises. Specifically, we propose a conditional generative framework for speech\nenhancement, which models clean speech by acoustic codes of a neural speech\ncodec and generates the speech codes conditioned on past noisy frames in an\nauto-regressive way. Moreover, we propose an explicit-alignment approach to\nalign noisy frames with the generated speech tokens to improve the robustness\nand scalability to different input lengths. Different from other methods that\nleverage multiple stages to generate speech codes, we leverage a single-stage\nspeech generation approach based on the TF-Codec neural codec to achieve high\nspeech quality with low latency. Extensive results on both synthetic and\nreal-recorded test set show its superiority over data-driven approaches in\nterms of noise robustness and temporal speech coherence.\n","authors":["Huaying Xue","Xiulian Peng","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2310.08981v2.pdf","comment":"5 pages, submitted to ICASSP2024"},{"id":"http://arxiv.org/abs/2310.13471v1","updated":"2023-10-20T13:12:35Z","published":"2023-10-20T13:12:35Z","title":"Neural domain alignment for spoken language recognition based on optimal\n  transport","summary":"  Domain shift poses a significant challenge in cross-domain spoken language\nrecognition (SLR) by reducing its effectiveness. Unsupervised domain adaptation\n(UDA) algorithms have been explored to address domain shifts in SLR without\nrelying on class labels in the target domain. One successful UDA approach\nfocuses on learning domain-invariant representations to align feature\ndistributions between domains. However, disregarding the class structure during\nthe learning process of domain-invariant representations can result in\nover-alignment, negatively impacting the classification task. To overcome this\nlimitation, we propose an optimal transport (OT)-based UDA algorithm for a\ncross-domain SLR, leveraging the distribution geometry structure-aware property\nof OT. An OT-based discrepancy measure on a joint distribution over feature and\nlabel information is considered during domain alignment in OT-based UDA. Our\nprevious study discovered that completely aligning the distributions between\nthe source and target domains can introduce a negative transfer, where classes\nor irrelevant classes from the source domain map to a different class in the\ntarget domain during distribution alignment. This negative transfer degrades\nthe performance of the adaptive model. To mitigate this issue, we introduce\ncoupling-weighted partial optimal transport (POT) within our UDA framework for\nSLR, where soft weighting on the OT coupling based on transport cost is\nadaptively set during domain alignment. A cross-domain SLR task was used in the\nexperiments to evaluate the proposed UDA. The results demonstrated that our\nproposed UDA algorithm significantly improved the performance over existing UDA\nalgorithms in a cross-channel SLR task.\n","authors":["Xugang Lu","Peng Shen","Yu Tsao","Hisashi Kawai"],"pdf_url":"https://arxiv.org/pdf/2310.13471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13451v1","updated":"2023-10-20T12:35:54Z","published":"2023-10-20T12:35:54Z","title":"Two-Stage Triplet Loss Training with Curriculum Augmentation for\n  Audio-Visual Retrieval","summary":"  The cross-modal retrieval model leverages the potential of triple loss\noptimization to learn robust embedding spaces. However, existing methods often\ntrain these models in a singular pass, overlooking the distinction between\nsemi-hard and hard triples in the optimization process. The oversight of not\ndistinguishing between semi-hard and hard triples leads to suboptimal model\nperformance. In this paper, we introduce a novel approach rooted in curriculum\nlearning to address this problem. We propose a two-stage training paradigm that\nguides the model's learning process from semi-hard to hard triplets. In the\nfirst stage, the model is trained with a set of semi-hard triplets, starting\nfrom a low-loss base. Subsequently, in the second stage, we augment the\nembeddings using an interpolation technique. This process identifies potential\nhard negatives, alleviating issues arising from high-loss functions due to a\nscarcity of hard triples. Our approach then applies hard triplet mining in the\naugmented embedding space to further optimize the model. Extensive experimental\nresults conducted on two audio-visual datasets show a significant improvement\nof approximately 9.8% in terms of average Mean Average Precision (MAP) over the\ncurrent state-of-the-art method, MSNSCA, for the Audio-Visual Cross-Modal\nRetrieval (AV-CMR) task on the AVE dataset, indicating the effectiveness of our\nproposed method.\n","authors":["Donghuo Zeng","Kazushi Ikeda"],"pdf_url":"https://arxiv.org/pdf/2310.13451v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.13404v1","updated":"2023-10-20T10:22:15Z","published":"2023-10-20T10:22:15Z","title":"Definition-independent Formalization of Soundscapes: Towards a Formal\n  Methodology","summary":"  Soundscapes have been studied by researchers from various disciplines, each\nwith different perspectives, goals, approaches, and terminologies. Accordingly,\ndepending on the field, the concept of a soundscape's components changes,\nconsequently changing the basic definition. This results in complicating\ninterdisciplinary communication and comparison of results. Especially when\nsoundscape-unrelated research areas are involved. For this reason, we present a\npotential formalization that is independent of the underlying soundscape\ndefinition, with the goal of being able to capture the heterogeneous structure\nof the data as well as the different ideologies in one model. In an exemplary\nanalysis of frequency correlation matrices for land use type detection as an\nalternative to features like MFCCs, we show a practical application of our\npresented formalization.\n","authors":["Mikel D. Jedrusiak","Thomas Harweg","Timo Haselhoff","Bryce T. Lawrence","Susanne Moebus","Frank Weichert"],"pdf_url":"https://arxiv.org/pdf/2310.13404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13388v1","updated":"2023-10-20T09:56:22Z","published":"2023-10-20T09:56:22Z","title":"Music Augmentation and Denoising For Peak-Based Audio Fingerprinting","summary":"  Audio fingerprinting is a well-established solution for song identification\nfrom short recording excerpts. Popular methods rely on the extraction of sparse\nrepresentations, generally spectral peaks, and have proven to be accurate,\nfast, and scalable to large collections. However, real-world applications of\naudio identification often happen in noisy environments, which can cause these\nsystems to fail. In this work, we tackle this problem by introducing and\nreleasing a new audio augmentation pipeline that adds noise to music snippets\nin a realistic way, by stochastically mimicking real-world scenarios. We then\npropose and release a deep learning model that removes noisy components from\nspectrograms in order to improve peak-based fingerprinting systems' accuracy.\nWe show that the addition of our model improves the identification performance\nof commonly used audio fingerprinting systems, even under noisy conditions.\n","authors":["Kamil Akesbi","Dorian Desblancs","Benjamin Martin"],"pdf_url":"https://arxiv.org/pdf/2310.13388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14502v2","updated":"2023-10-20T08:55:17Z","published":"2023-07-27T09:20:38Z","title":"The Effect of Spoken Language on Speech Enhancement using\n  Self-Supervised Speech Representation Loss Functions","summary":"  Recent work in the field of speech enhancement (SE) has involved the use of\nself-supervised speech representations (SSSRs) as feature transformations in\nloss functions. However, in prior work, very little attention has been paid to\nthe relationship between the language of the audio used to train the\nself-supervised representation and that used to train the SE system.\nEnhancement models trained using a loss function which incorporates a\nself-supervised representation that shares exactly the language of the noisy\ndata used to train the SE system show better performance than those which do\nnot match exactly. This may lead to enhancement systems which are language\nspecific and as such do not generalise well to unseen languages, unlike models\ntrained using traditional spectrogram or time domain loss functions. In this\nwork, SE models are trained and tested on a number of different languages, with\nself-supervised representations which themselves are trained using different\nlanguage combinations and with differing network structures as loss function\nrepresentations. These models are then tested across unseen languages and their\nperformances are analysed. It is found that the training language of the\nself-supervised representation appears to have a minor effect on enhancement\nperformance, the amount of training data of a particular language, however,\ngreatly affects performance.\n","authors":["George Close","Thomas Hain","Stefan Goetze"],"pdf_url":"https://arxiv.org/pdf/2307.14502v2.pdf","comment":"Accepted at WASPAA 2023"},{"id":"http://arxiv.org/abs/2305.15255v3","updated":"2023-10-20T05:55:39Z","published":"2023-05-24T15:39:43Z","title":"Spoken Question Answering and Speech Continuation Using\n  Spectrogram-Powered LLM","summary":"  We present a novel approach to adapting pre-trained large language models\n(LLMs) to perform question answering (QA) and speech continuation. By endowing\nthe LLM with a pre-trained speech encoder, our model becomes able to take\nspeech inputs and generate speech outputs. The entire system is trained\nend-to-end and operates directly on spectrograms, simplifying our architecture.\nKey to our approach is a training objective that jointly supervises speech\nrecognition, text continuation, and speech synthesis using only paired\nspeech-text pairs, enabling a `cross-modal' chain-of-thought within a single\ndecoding pass. Our method surpasses existing spoken language models in speaker\npreservation and semantic coherence. Furthermore, the proposed model improves\nupon direct initialization in retaining the knowledge of the original LLM as\ndemonstrated through spoken QA datasets. Audio samples can be found at\nhttps://michelleramanovich.github.io/spectron/spectron\n","authors":["Eliya Nachmani","Alon Levkovitch","Roy Hirsch","Julian Salazar","Chulayuth Asawaroengchai","Soroosh Mariooryad","Ehud Rivlin","RJ Skerry-Ryan","Michelle Tadmor Ramanovich"],"pdf_url":"https://arxiv.org/pdf/2305.15255v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13289v1","updated":"2023-10-20T05:41:57Z","published":"2023-10-20T05:41:57Z","title":"SALMONN: Towards Generic Hearing Abilities for Large Language Models","summary":"  Hearing is arguably an essential ability of artificial intelligence (AI)\nagents in the physical world, which refers to the perception and understanding\nof general auditory information consisting of at least three types of sounds:\nspeech, audio events, and music. In this paper, we propose SALMONN, a speech\naudio language music open neural network, built by integrating a pre-trained\ntext-based large language model (LLM) with speech and audio encoders into a\nsingle multimodal model. SALMONN enables the LLM to directly process and\nunderstand general audio inputs and achieve competitive performances on a\nnumber of speech and audio tasks used in training, such as automatic speech\nrecognition and translation, auditory-information-based question answering,\nemotion recognition, speaker verification, and music and audio captioning\n\\textit{etc.} SALMONN also has a diverse set of emergent abilities unseen in\nthe training, which includes but is not limited to speech translation to\nuntrained languages, speech-based slot filling, spoken-query-based question\nanswering, audio-based storytelling, and speech audio co-reasoning\n\\textit{etc}. The presence of the cross-modal emergent abilities is studied,\nand a novel few-shot activation tuning approach is proposed to activate such\nabilities of SALMONN. To our knowledge, SALMONN is the first model of its type\nand can be regarded as a step towards AI with generic hearing abilities. An\ninteractive demo of SALMONN is available at\n\\texttt{\\url{https://github.com/bytedance/SALMONN}}, and the training code and\nmodel checkpoints will be released upon acceptance.\n","authors":["Changli Tang","Wenyi Yu","Guangzhi Sun","Xianzhao Chen","Tian Tan","Wei Li","Lu Lu","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.13289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13267v1","updated":"2023-10-20T04:21:09Z","published":"2023-10-20T04:21:09Z","title":"On the Language Encoder of Contrastive Cross-modal Models","summary":"  Contrastive cross-modal models such as CLIP and CLAP aid various\nvision-language (VL) and audio-language (AL) tasks. However, there has been\nlimited investigation of and improvement in their language encoder, which is\nthe central component of encoding natural language descriptions of image/audio\ninto vector representations. We extensively evaluate how unsupervised and\nsupervised sentence embedding training affect language encoder quality and\ncross-modal task performance. In VL pretraining, we found that sentence\nembedding training language encoder quality and aids in cross-modal tasks,\nimproving contrastive VL models such as CyCLIP. In contrast, AL pretraining\nbenefits less from sentence embedding training, which may result from the\nlimited amount of pretraining data. We analyze the representation spaces to\nunderstand the strengths of sentence embedding training, and find that it\nimproves text-space uniformity, at the cost of decreased cross-modal alignment.\n","authors":["Mengjie Zhao","Junya Ono","Zhi Zhong","Chieh-Hsin Lai","Yuhta Takida","Naoki Murata","Wei-Hsiang Liao","Takashi Shibuya","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2310.13267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13812v1","updated":"2023-10-20T20:58:45Z","published":"2023-10-20T20:58:45Z","title":"Yet Another Model for Arabic Dialect Identification","summary":"  In this paper, we describe a spoken Arabic dialect identification (ADI) model\nfor Arabic that consistently outperforms previously published results on two\nbenchmark datasets: ADI-5 and ADI-17. We explore two architectural variations:\nResNet and ECAPA-TDNN, coupled with two types of acoustic features: MFCCs and\nfeatures exratected from the pre-trained self-supervised model UniSpeech-SAT\nLarge, as well as a fusion of all four variants. We find that individually,\nECAPA-TDNN network outperforms ResNet, and models with UniSpeech-SAT features\noutperform models with MFCCs by a large margin. Furthermore, a fusion of all\nfour variants consistently outperforms individual models. Our best models\noutperform previously reported results on both datasets, with accuracies of\n84.7% and 96.9% on ADI-5 and ADI-17, respectively.\n","authors":["Ajinkya Kulkarni","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2310.13812v1.pdf","comment":"ACCEPTED AT ArabicNLP 2023"},{"id":"http://arxiv.org/abs/2310.13759v1","updated":"2023-10-20T18:43:28Z","published":"2023-10-20T18:43:28Z","title":"Multi-label Open-set Audio Classification","summary":"  Current audio classification models have small class vocabularies relative to\nthe large number of sound event classes of interest in the real world. Thus,\nthey provide a limited view of the world that may miss important yet unexpected\nor unknown sound events. To address this issue, open-set audio classification\ntechniques have been developed to detect sound events from unknown classes.\nAlthough these methods have been applied to a multi-class context in audio,\nsuch as sound scene classification, they have yet to be investigated for\npolyphonic audio in which sound events overlap, requiring the use of\nmulti-label models. In this study, we establish the problem of multi-label\nopen-set audio classification by creating a dataset with varying unknown class\ndistributions and evaluating baseline approaches built upon existing\ntechniques.\n","authors":["Sripathi Sridhar","Mark Cartwright"],"pdf_url":"https://arxiv.org/pdf/2310.13759v1.pdf","comment":"Published at the Workshop on Detection and Classification of Acoustic\n  Scenes and Events, 2023 (DCASE 2023)"},{"id":"http://arxiv.org/abs/2310.14796v1","updated":"2023-10-20T10:50:14Z","published":"2023-10-20T10:50:14Z","title":"A Novel Transfer Learning Method Utilizing Acoustic and Vibration\n  Signals for Rotating Machinery Fault Diagnosis","summary":"  Fault diagnosis of rotating machinery plays a important role for the safety\nand stability of modern industrial systems. However, there is a distribution\ndiscrepancy between training data and data of real-world operation scenarios,\nwhich causing the decrease of performance of existing systems. This paper\nproposed a transfer learning based method utilizing acoustic and vibration\nsignal to address this distribution discrepancy. We designed the acoustic and\nvibration feature fusion MAVgram to offer richer and more reliable information\nof faults, coordinating with a DNN-based classifier to obtain more effective\ndiagnosis representation. The backbone was pre-trained and then fine-tuned to\nobtained excellent performance of the target task. Experimental results\ndemonstrate the effectiveness of the proposed method, and achieved improved\nperformance compared to STgram-MFN.\n","authors":["Zhongliang Chen","Zhuofei Huang","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2310.14796v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.08981v2","updated":"2023-10-20T10:15:46Z","published":"2023-10-13T09:57:09Z","title":"Low-latency Speech Enhancement via Speech Token Generation","summary":"  Existing deep learning based speech enhancement mainly employ a data-driven\napproach, which leverage large amounts of data with a variety of noise types to\nachieve noise removal from noisy signal. However, the high dependence on the\ndata limits its generalization on the unseen complex noises in real-life\nenvironment. In this paper, we focus on the low-latency scenario and regard\nspeech enhancement as a speech generation problem conditioned on the noisy\nsignal, where we generate clean speech instead of identifying and removing\nnoises. Specifically, we propose a conditional generative framework for speech\nenhancement, which models clean speech by acoustic codes of a neural speech\ncodec and generates the speech codes conditioned on past noisy frames in an\nauto-regressive way. Moreover, we propose an explicit-alignment approach to\nalign noisy frames with the generated speech tokens to improve the robustness\nand scalability to different input lengths. Different from other methods that\nleverage multiple stages to generate speech codes, we leverage a single-stage\nspeech generation approach based on the TF-Codec neural codec to achieve high\nspeech quality with low latency. Extensive results on both synthetic and\nreal-recorded test set show its superiority over data-driven approaches in\nterms of noise robustness and temporal speech coherence.\n","authors":["Huaying Xue","Xiulian Peng","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2310.08981v2.pdf","comment":"5 pages, submitted to ICASSP2024"},{"id":"http://arxiv.org/abs/2310.13471v1","updated":"2023-10-20T13:12:35Z","published":"2023-10-20T13:12:35Z","title":"Neural domain alignment for spoken language recognition based on optimal\n  transport","summary":"  Domain shift poses a significant challenge in cross-domain spoken language\nrecognition (SLR) by reducing its effectiveness. Unsupervised domain adaptation\n(UDA) algorithms have been explored to address domain shifts in SLR without\nrelying on class labels in the target domain. One successful UDA approach\nfocuses on learning domain-invariant representations to align feature\ndistributions between domains. However, disregarding the class structure during\nthe learning process of domain-invariant representations can result in\nover-alignment, negatively impacting the classification task. To overcome this\nlimitation, we propose an optimal transport (OT)-based UDA algorithm for a\ncross-domain SLR, leveraging the distribution geometry structure-aware property\nof OT. An OT-based discrepancy measure on a joint distribution over feature and\nlabel information is considered during domain alignment in OT-based UDA. Our\nprevious study discovered that completely aligning the distributions between\nthe source and target domains can introduce a negative transfer, where classes\nor irrelevant classes from the source domain map to a different class in the\ntarget domain during distribution alignment. This negative transfer degrades\nthe performance of the adaptive model. To mitigate this issue, we introduce\ncoupling-weighted partial optimal transport (POT) within our UDA framework for\nSLR, where soft weighting on the OT coupling based on transport cost is\nadaptively set during domain alignment. A cross-domain SLR task was used in the\nexperiments to evaluate the proposed UDA. The results demonstrated that our\nproposed UDA algorithm significantly improved the performance over existing UDA\nalgorithms in a cross-channel SLR task.\n","authors":["Xugang Lu","Peng Shen","Yu Tsao","Hisashi Kawai"],"pdf_url":"https://arxiv.org/pdf/2310.13471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13451v1","updated":"2023-10-20T12:35:54Z","published":"2023-10-20T12:35:54Z","title":"Two-Stage Triplet Loss Training with Curriculum Augmentation for\n  Audio-Visual Retrieval","summary":"  The cross-modal retrieval model leverages the potential of triple loss\noptimization to learn robust embedding spaces. However, existing methods often\ntrain these models in a singular pass, overlooking the distinction between\nsemi-hard and hard triples in the optimization process. The oversight of not\ndistinguishing between semi-hard and hard triples leads to suboptimal model\nperformance. In this paper, we introduce a novel approach rooted in curriculum\nlearning to address this problem. We propose a two-stage training paradigm that\nguides the model's learning process from semi-hard to hard triplets. In the\nfirst stage, the model is trained with a set of semi-hard triplets, starting\nfrom a low-loss base. Subsequently, in the second stage, we augment the\nembeddings using an interpolation technique. This process identifies potential\nhard negatives, alleviating issues arising from high-loss functions due to a\nscarcity of hard triples. Our approach then applies hard triplet mining in the\naugmented embedding space to further optimize the model. Extensive experimental\nresults conducted on two audio-visual datasets show a significant improvement\nof approximately 9.8% in terms of average Mean Average Precision (MAP) over the\ncurrent state-of-the-art method, MSNSCA, for the Audio-Visual Cross-Modal\nRetrieval (AV-CMR) task on the AVE dataset, indicating the effectiveness of our\nproposed method.\n","authors":["Donghuo Zeng","Kazushi Ikeda"],"pdf_url":"https://arxiv.org/pdf/2310.13451v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.13430v1","updated":"2023-10-20T11:41:54Z","published":"2023-10-20T11:41:54Z","title":"HRTF Interpolation using a Spherical Neural Process Meta-Learner","summary":"  Several individualization methods have recently been proposed to estimate a\nsubject's Head-Related Transfer Function (HRTF) using convenient input\nmodalities such as anthropometric measurements or pinnae photographs. There\nexists a need for adaptively correcting the estimation error committed by such\nmethods using a few data point samples from the subject's HRTF, acquired using\nacoustic measurements or perceptual feedback. To this end, we introduce a\nConvolutional Conditional Neural Process meta-learner specialized in HRTF error\ninterpolation. In particular, the model includes a Spherical Convolutional\nNeural Network component to accommodate the spherical geometry of HRTF data. It\nalso exploits potential symmetries between the HRTF's left and right channels\nabout the median axis. In this work, we evaluate the proposed model's\nperformance purely on time-aligned spectrum interpolation grounds under a\nsimplified setup where a generic population-mean HRTF forms the initial\nestimates prior to corrections instead of individualized ones. The trained\nmodel achieves up to 3 dB relative error reduction compared to state-of-the-art\ninterpolation methods despite being trained using only 85 subjects. This\nimprovement translates up to nearly a halving of the data point count required\nto achieve comparable accuracy, in particular from 50 to 28 points to reach an\naverage of -20 dB relative error per interpolated feature. Moreover, we show\nthat the trained model provides well-calibrated uncertainty estimates.\nAccordingly, such estimates can inform the sequential decision problem of\nacquiring as few correcting HRTF data points as needed to meet a desired level\nof HRTF individualization accuracy.\n","authors":["Etienne Thuillier","Craig Jin","Vesa V√§lim√§ki"],"pdf_url":"https://arxiv.org/pdf/2310.13430v1.pdf","comment":"12 pages. 11 figures. Submitted for publication in IEEE/ACM\n  Transactions on Audio, Speech and Language Processing (T-ASL)"},{"id":"http://arxiv.org/abs/2310.13418v1","updated":"2023-10-20T10:56:01Z","published":"2023-10-20T10:56:01Z","title":"GenDistiller: Distilling Pre-trained Language Models based on Generative\n  Models","summary":"  Self-supervised pre-trained models such as HuBERT and WavLM leverage\nunlabeled speech data for representation learning and offer significantly\nimprove for numerous downstream tasks. Despite the success of these methods,\ntheir large memory and strong computational requirements hinder their\napplication on resource restricted devices. Therefore, this paper introduces\nGenDistiller, a novel knowledge distillation framework to distill hidden\nrepresentations from teacher network based on generative language model. The\ngenerative structure enables the proposed model to generate the target teacher\nhidden layers autoregressively, considering the interactions between hidden\nlayers without instroducing additional inputs. A two-dimensional attention\nmechanism is implemented to ensure the causality of hidden layers, while\npreserving bidirectional attention in the time dimension. Experiments reveal\nthe advantage of the generative distiller over the baseline system that\npredicts the hidden layers of teacher network directly without a generatvie\nmodel.\n","authors":["Yingying Gao","Shilei Zhang","Zihao Cui","Yanhan Xu","Chao Deng","Junlan Feng"],"pdf_url":"https://arxiv.org/pdf/2310.13418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13404v1","updated":"2023-10-20T10:22:15Z","published":"2023-10-20T10:22:15Z","title":"Definition-independent Formalization of Soundscapes: Towards a Formal\n  Methodology","summary":"  Soundscapes have been studied by researchers from various disciplines, each\nwith different perspectives, goals, approaches, and terminologies. Accordingly,\ndepending on the field, the concept of a soundscape's components changes,\nconsequently changing the basic definition. This results in complicating\ninterdisciplinary communication and comparison of results. Especially when\nsoundscape-unrelated research areas are involved. For this reason, we present a\npotential formalization that is independent of the underlying soundscape\ndefinition, with the goal of being able to capture the heterogeneous structure\nof the data as well as the different ideologies in one model. In an exemplary\nanalysis of frequency correlation matrices for land use type detection as an\nalternative to features like MFCCs, we show a practical application of our\npresented formalization.\n","authors":["Mikel D. Jedrusiak","Thomas Harweg","Timo Haselhoff","Bryce T. Lawrence","Susanne Moebus","Frank Weichert"],"pdf_url":"https://arxiv.org/pdf/2310.13404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13388v1","updated":"2023-10-20T09:56:22Z","published":"2023-10-20T09:56:22Z","title":"Music Augmentation and Denoising For Peak-Based Audio Fingerprinting","summary":"  Audio fingerprinting is a well-established solution for song identification\nfrom short recording excerpts. Popular methods rely on the extraction of sparse\nrepresentations, generally spectral peaks, and have proven to be accurate,\nfast, and scalable to large collections. However, real-world applications of\naudio identification often happen in noisy environments, which can cause these\nsystems to fail. In this work, we tackle this problem by introducing and\nreleasing a new audio augmentation pipeline that adds noise to music snippets\nin a realistic way, by stochastically mimicking real-world scenarios. We then\npropose and release a deep learning model that removes noisy components from\nspectrograms in order to improve peak-based fingerprinting systems' accuracy.\nWe show that the addition of our model improves the identification performance\nof commonly used audio fingerprinting systems, even under noisy conditions.\n","authors":["Kamil Akesbi","Dorian Desblancs","Benjamin Martin"],"pdf_url":"https://arxiv.org/pdf/2310.13388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14502v2","updated":"2023-10-20T08:55:17Z","published":"2023-07-27T09:20:38Z","title":"The Effect of Spoken Language on Speech Enhancement using\n  Self-Supervised Speech Representation Loss Functions","summary":"  Recent work in the field of speech enhancement (SE) has involved the use of\nself-supervised speech representations (SSSRs) as feature transformations in\nloss functions. However, in prior work, very little attention has been paid to\nthe relationship between the language of the audio used to train the\nself-supervised representation and that used to train the SE system.\nEnhancement models trained using a loss function which incorporates a\nself-supervised representation that shares exactly the language of the noisy\ndata used to train the SE system show better performance than those which do\nnot match exactly. This may lead to enhancement systems which are language\nspecific and as such do not generalise well to unseen languages, unlike models\ntrained using traditional spectrogram or time domain loss functions. In this\nwork, SE models are trained and tested on a number of different languages, with\nself-supervised representations which themselves are trained using different\nlanguage combinations and with differing network structures as loss function\nrepresentations. These models are then tested across unseen languages and their\nperformances are analysed. It is found that the training language of the\nself-supervised representation appears to have a minor effect on enhancement\nperformance, the amount of training data of a particular language, however,\ngreatly affects performance.\n","authors":["George Close","Thomas Hain","Stefan Goetze"],"pdf_url":"https://arxiv.org/pdf/2307.14502v2.pdf","comment":"Accepted at WASPAA 2023"},{"id":"http://arxiv.org/abs/2305.15255v3","updated":"2023-10-20T05:55:39Z","published":"2023-05-24T15:39:43Z","title":"Spoken Question Answering and Speech Continuation Using\n  Spectrogram-Powered LLM","summary":"  We present a novel approach to adapting pre-trained large language models\n(LLMs) to perform question answering (QA) and speech continuation. By endowing\nthe LLM with a pre-trained speech encoder, our model becomes able to take\nspeech inputs and generate speech outputs. The entire system is trained\nend-to-end and operates directly on spectrograms, simplifying our architecture.\nKey to our approach is a training objective that jointly supervises speech\nrecognition, text continuation, and speech synthesis using only paired\nspeech-text pairs, enabling a `cross-modal' chain-of-thought within a single\ndecoding pass. Our method surpasses existing spoken language models in speaker\npreservation and semantic coherence. Furthermore, the proposed model improves\nupon direct initialization in retaining the knowledge of the original LLM as\ndemonstrated through spoken QA datasets. Audio samples can be found at\nhttps://michelleramanovich.github.io/spectron/spectron\n","authors":["Eliya Nachmani","Alon Levkovitch","Roy Hirsch","Julian Salazar","Chulayuth Asawaroengchai","Soroosh Mariooryad","Ehud Rivlin","RJ Skerry-Ryan","Michelle Tadmor Ramanovich"],"pdf_url":"https://arxiv.org/pdf/2305.15255v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13289v1","updated":"2023-10-20T05:41:57Z","published":"2023-10-20T05:41:57Z","title":"SALMONN: Towards Generic Hearing Abilities for Large Language Models","summary":"  Hearing is arguably an essential ability of artificial intelligence (AI)\nagents in the physical world, which refers to the perception and understanding\nof general auditory information consisting of at least three types of sounds:\nspeech, audio events, and music. In this paper, we propose SALMONN, a speech\naudio language music open neural network, built by integrating a pre-trained\ntext-based large language model (LLM) with speech and audio encoders into a\nsingle multimodal model. SALMONN enables the LLM to directly process and\nunderstand general audio inputs and achieve competitive performances on a\nnumber of speech and audio tasks used in training, such as automatic speech\nrecognition and translation, auditory-information-based question answering,\nemotion recognition, speaker verification, and music and audio captioning\n\\textit{etc.} SALMONN also has a diverse set of emergent abilities unseen in\nthe training, which includes but is not limited to speech translation to\nuntrained languages, speech-based slot filling, spoken-query-based question\nanswering, audio-based storytelling, and speech audio co-reasoning\n\\textit{etc}. The presence of the cross-modal emergent abilities is studied,\nand a novel few-shot activation tuning approach is proposed to activate such\nabilities of SALMONN. To our knowledge, SALMONN is the first model of its type\nand can be regarded as a step towards AI with generic hearing abilities. An\ninteractive demo of SALMONN is available at\n\\texttt{\\url{https://github.com/bytedance/SALMONN}}, and the training code and\nmodel checkpoints will be released upon acceptance.\n","authors":["Changli Tang","Wenyi Yu","Guangzhi Sun","Xianzhao Chen","Tian Tan","Wei Li","Lu Lu","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.13289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07200v3","updated":"2023-10-20T04:29:40Z","published":"2023-07-14T07:30:59Z","title":"Reproducing the Acoustic Velocity Vectors in a Spherical Listening\n  Region","summary":"  Acoustic velocity vectors are related to the human's perception of sound at\nlow frequencies and have been widely used in Ambisonics. This paper proposes a\nspatial sound field reproduction algorithm based on matching the acoustic\nvelocity vectors in the spherical listening region. Using the sound field\ntranslation formula, the spherical harmonic coefficients of the acoustic\nvelocity vectors in the spherical listening region are derived from the\nspherical harmonic coefficients of the pressure, which can be measured by a\nhigher-order microphone array. Unlike previous work in which the acoustic\nvelocity vectors are only controlled on the boundary of the listening region or\nat discrete sweet spots, this work directly manipulates the acoustic velocity\nvectors in the whole spherical listening region, which allows the listener to\nmove beyond the sweet spots. Simulations show the proposed reproduction\nalgorithm can accurately reproduce the acoustic velocity vectors in the\nspherical listening region, with better performance at low frequencies.\n","authors":["Jiarui Wang","Thushara Abhayapala","Jihui Aimee Zhang","Prasanga Samarasinghe"],"pdf_url":"https://arxiv.org/pdf/2307.07200v3.pdf","comment":"Submitted to IEEE Signal Processing Letters"},{"id":"http://arxiv.org/abs/2310.13267v1","updated":"2023-10-20T04:21:09Z","published":"2023-10-20T04:21:09Z","title":"On the Language Encoder of Contrastive Cross-modal Models","summary":"  Contrastive cross-modal models such as CLIP and CLAP aid various\nvision-language (VL) and audio-language (AL) tasks. However, there has been\nlimited investigation of and improvement in their language encoder, which is\nthe central component of encoding natural language descriptions of image/audio\ninto vector representations. We extensively evaluate how unsupervised and\nsupervised sentence embedding training affect language encoder quality and\ncross-modal task performance. In VL pretraining, we found that sentence\nembedding training language encoder quality and aids in cross-modal tasks,\nimproving contrastive VL models such as CyCLIP. In contrast, AL pretraining\nbenefits less from sentence embedding training, which may result from the\nlimited amount of pretraining data. We analyze the representation spaces to\nunderstand the strengths of sentence embedding training, and find that it\nimproves text-space uniformity, at the cost of decreased cross-modal alignment.\n","authors":["Mengjie Zhao","Junya Ono","Zhi Zhong","Chieh-Hsin Lai","Yuhta Takida","Naoki Murata","Wei-Hsiang Liao","Takashi Shibuya","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2310.13267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13812v1","updated":"2023-10-20T20:58:45Z","published":"2023-10-20T20:58:45Z","title":"Yet Another Model for Arabic Dialect Identification","summary":"  In this paper, we describe a spoken Arabic dialect identification (ADI) model\nfor Arabic that consistently outperforms previously published results on two\nbenchmark datasets: ADI-5 and ADI-17. We explore two architectural variations:\nResNet and ECAPA-TDNN, coupled with two types of acoustic features: MFCCs and\nfeatures exratected from the pre-trained self-supervised model UniSpeech-SAT\nLarge, as well as a fusion of all four variants. We find that individually,\nECAPA-TDNN network outperforms ResNet, and models with UniSpeech-SAT features\noutperform models with MFCCs by a large margin. Furthermore, a fusion of all\nfour variants consistently outperforms individual models. Our best models\noutperform previously reported results on both datasets, with accuracies of\n84.7% and 96.9% on ADI-5 and ADI-17, respectively.\n","authors":["Ajinkya Kulkarni","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2310.13812v1.pdf","comment":"ACCEPTED AT ArabicNLP 2023"},{"id":"http://arxiv.org/abs/2310.13759v1","updated":"2023-10-20T18:43:28Z","published":"2023-10-20T18:43:28Z","title":"Multi-label Open-set Audio Classification","summary":"  Current audio classification models have small class vocabularies relative to\nthe large number of sound event classes of interest in the real world. Thus,\nthey provide a limited view of the world that may miss important yet unexpected\nor unknown sound events. To address this issue, open-set audio classification\ntechniques have been developed to detect sound events from unknown classes.\nAlthough these methods have been applied to a multi-class context in audio,\nsuch as sound scene classification, they have yet to be investigated for\npolyphonic audio in which sound events overlap, requiring the use of\nmulti-label models. In this study, we establish the problem of multi-label\nopen-set audio classification by creating a dataset with varying unknown class\ndistributions and evaluating baseline approaches built upon existing\ntechniques.\n","authors":["Sripathi Sridhar","Mark Cartwright"],"pdf_url":"https://arxiv.org/pdf/2310.13759v1.pdf","comment":"Published at the Workshop on Detection and Classification of Acoustic\n  Scenes and Events, 2023 (DCASE 2023)"},{"id":"http://arxiv.org/abs/2310.14796v1","updated":"2023-10-20T10:50:14Z","published":"2023-10-20T10:50:14Z","title":"A Novel Transfer Learning Method Utilizing Acoustic and Vibration\n  Signals for Rotating Machinery Fault Diagnosis","summary":"  Fault diagnosis of rotating machinery plays a important role for the safety\nand stability of modern industrial systems. However, there is a distribution\ndiscrepancy between training data and data of real-world operation scenarios,\nwhich causing the decrease of performance of existing systems. This paper\nproposed a transfer learning based method utilizing acoustic and vibration\nsignal to address this distribution discrepancy. We designed the acoustic and\nvibration feature fusion MAVgram to offer richer and more reliable information\nof faults, coordinating with a DNN-based classifier to obtain more effective\ndiagnosis representation. The backbone was pre-trained and then fine-tuned to\nobtained excellent performance of the target task. Experimental results\ndemonstrate the effectiveness of the proposed method, and achieved improved\nperformance compared to STgram-MFN.\n","authors":["Zhongliang Chen","Zhuofei Huang","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2310.14796v1.pdf","comment":null}]},"2023-10-21T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2310.11069v2","updated":"2023-10-21T12:42:30Z","published":"2023-10-17T08:33:02Z","title":"VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System","summary":"  Arabic is a complex language with many varieties and dialects spoken by over\n450 millions all around the world. Due to the linguistic diversity and\nvariations, it is challenging to build a robust and generalized ASR system for\nArabic. In this work, we address this gap by developing and demoing a system,\ndubbed VoxArabica, for dialect identification (DID) as well as automatic speech\nrecognition (ASR) of Arabic. We train a wide range of models such as HuBERT\n(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR\ntasks. Our DID models are trained to identify 17 different dialects in addition\nto MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.\nAdditionally, for the remaining dialects in ASR, we provide the option to\nchoose various models such as Whisper and MMS in a zero-shot setting. We\nintegrate these models into a single web interface with diverse features such\nas audio recording, file upload, model selection, and the option to raise flags\nfor incorrect outputs. Overall, we believe VoxArabica will be useful for a wide\nrange of audiences concerned with Arabic research. Our system is currently\nrunning at https://cdce-206-12-100-168.ngrok.io/.\n","authors":["Abdul Waheed","Bashar Talafha","Peter Suvellin","Abdelrahman Elmadney","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2310.11069v2.pdf","comment":"Accepted at ArabicNLP conference co-located with EMNLP'23. First\n  three authors contributed equally"},{"id":"http://arxiv.org/abs/2310.14044v1","updated":"2023-10-21T15:41:50Z","published":"2023-10-21T15:41:50Z","title":"Composer Style-specific Symbolic Music Generation Using Vector Quantized\n  Discrete Diffusion Models","summary":"  Emerging Denoising Diffusion Probabilistic Models (DDPM) have become\nincreasingly utilised because of promising results they have achieved in\ndiverse generative tasks with continuous data, such as image and sound\nsynthesis. Nonetheless, the success of diffusion models has not been fully\nextended to discrete symbolic music. We propose to combine a vector quantized\nvariational autoencoder (VQ-VAE) and discrete diffusion models for the\ngeneration of symbolic music with desired composer styles. The trained VQ-VAE\ncan represent symbolic music as a sequence of indexes that correspond to\nspecific entries in a learned codebook. Subsequently, a discrete diffusion\nmodel is used to model the VQ-VAE's discrete latent space. The diffusion model\nis trained to generate intermediate music sequences consisting of codebook\nindexes, which are then decoded to symbolic music using the VQ-VAE's decoder.\nThe results demonstrate our model can generate symbolic music with target\ncomposer styles that meet the given conditions with a high accuracy of 72.36%.\n","authors":["Jincheng Zhang","Jingjing Tang","Charalampos Saitis","Gy√∂rgy Fazekas"],"pdf_url":"https://arxiv.org/pdf/2310.14044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14040v1","updated":"2023-10-21T15:35:43Z","published":"2023-10-21T15:35:43Z","title":"Fast Diffusion GAN Model for Symbolic Music Generation Controlled by\n  Emotions","summary":"  Diffusion models have shown promising results for a wide range of generative\ntasks with continuous data, such as image and audio synthesis. However, little\nprogress has been made on using diffusion models to generate discrete symbolic\nmusic because this new class of generative models are not well suited for\ndiscrete data while its iterative sampling process is computationally\nexpensive. In this work, we propose a diffusion model combined with a\nGenerative Adversarial Network, aiming to (i) alleviate one of the remaining\nchallenges in algorithmic music generation which is the control of generation\ntowards a target emotion, and (ii) mitigate the slow sampling drawback of\ndiffusion models applied to symbolic music generation. We first used a trained\nVariational Autoencoder to obtain embeddings of a symbolic music dataset with\nemotion labels and then used those to train a diffusion model. Our results\ndemonstrate the successful control of our diffusion model to generate symbolic\nmusic with a desired emotion. Our model achieves several orders of magnitude\nimprovement in computational cost, requiring merely four time steps to denoise\nwhile the steps required by current state-of-the-art diffusion models for\nsymbolic music generation is in the order of thousands.\n","authors":["Jincheng Zhang","Gy√∂rgy Fazekas","Charalampos Saitis"],"pdf_url":"https://arxiv.org/pdf/2310.14040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14018v1","updated":"2023-10-21T14:02:23Z","published":"2023-10-21T14:02:23Z","title":"Temporal convolutional neural networks to generate a head-related\n  impulse response from one direction to another","summary":"  Virtual sound synthesis is a technology that allows users to perceive spatial\nsound through headphones or earphones. However, accurate virtual sound requires\nan individual head-related transfer function (HRTF), which can be difficult to\nmeasure due to the need for a specialized environment. In this study, we\nproposed a method to generate HRTFs from one direction to the other. To this\nend, we used temporal convolutional neural networks (TCNs) to generate\nhead-related impulse responses (HRIRs). To train the TCNs, publicly available\ndatasets in the horizontal plane were used. Using the trained networks, we\nsuccessfully generated HRIRs for directions other than the front direction in\nthe dataset. We found that the proposed method successfully generated HRIRs for\npublicly available datasets. To test the generalization of the method, we\nmeasured the HRIRs of a new dataset and tested whether the trained networks\ncould be used for this new dataset. Although the similarity evaluated by\nspectral distortion was slightly degraded, behavioral experiments with human\nparticipants showed that the generated HRIRs were equivalent to the measured\nones. These results suggest that the proposed TCNs can be used to generate\npersonalized HRIRs from one direction to another, which could contribute to the\npersonalization of virtual sound.\n","authors":["Tatsuki Kobayashi","Yoshiko Maruyama","Isao Nambu","Shohei Yano","Yasuhiro Wada"],"pdf_url":"https://arxiv.org/pdf/2310.14018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13974v1","updated":"2023-10-21T11:26:24Z","published":"2023-10-21T11:26:24Z","title":"Automatic Pronunciation Assessment -- A Review","summary":"  Pronunciation assessment and its application in computer-aided pronunciation\ntraining (CAPT) have seen impressive progress in recent years. With the rapid\ngrowth in language processing and deep learning over the past few years, there\nis a need for an updated review. In this paper, we review methods employed in\npronunciation assessment for both phonemic and prosodic. We categorize the main\nchallenges observed in prominent research trends, and highlight existing\nlimitations, and available resources. This is followed by a discussion of the\nremaining challenges and possible directions for future work.\n","authors":["Yassine El Kheir","Ahmed Ali","Shammur Absar Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2310.13974v1.pdf","comment":"9 pages, accepted to EMNLP Findings"},{"id":"http://arxiv.org/abs/2309.05855v2","updated":"2023-10-21T10:54:08Z","published":"2023-09-11T22:34:06Z","title":"Instabilities in Convnets for Raw Audio","summary":"  What makes waveform-based deep learning so hard? Despite numerous attempts at\ntraining convolutional neural networks (convnets) for filterbank design, they\noften fail to outperform hand-crafted baselines. These baselines are linear\ntime-invariant systems: as such, they can be approximated by convnets with wide\nreceptive fields. Yet, in practice, gradient-based optimization leads to\nsuboptimal approximations. In our article, we approach this phenomenon from the\nperspective of initialization. We present a theory of large deviations for the\nenergy response of FIR filterbanks with random Gaussian weights. We find that\ndeviations worsen for large filters and locally periodic input signals, which\nare both typical for audio signal processing applications. Numerical\nsimulations align with our theory and suggest that the condition number of a\nconvolutional layer follows a logarithmic scaling law between the number and\nlength of the filters, which is reminiscent of discrete wavelet bases.\n","authors":["Daniel Haider","Vincent Lostanlen","Martin Ehler","Peter Balazs"],"pdf_url":"https://arxiv.org/pdf/2309.05855v2.pdf","comment":"4 pages, 5 figures, 1 page appendix, under review for IEEE SPL"},{"id":"http://arxiv.org/abs/2305.14032v3","updated":"2023-10-21T07:12:33Z","published":"2023-05-23T13:04:07Z","title":"Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on\n  Respiratory Sound Classification","summary":"  Respiratory sound contains crucial information for the early diagnosis of\nfatal lung diseases. Since the COVID-19 pandemic, there has been a growing\ninterest in contact-free medical care based on electronic stethoscopes. To this\nend, cutting-edge deep learning models have been developed to diagnose lung\ndiseases; however, it is still challenging due to the scarcity of medical data.\nIn this study, we demonstrate that the pretrained model on large-scale visual\nand audio datasets can be generalized to the respiratory sound classification\ntask. In addition, we introduce a straightforward Patch-Mix augmentation, which\nrandomly mixes patches between different samples, with Audio Spectrogram\nTransformer (AST). We further propose a novel and effective Patch-Mix\nContrastive Learning to distinguish the mixed representations in the latent\nspace. Our method achieves state-of-the-art performance on the ICBHI dataset,\noutperforming the prior leading score by an improvement of 4.08%.\n","authors":["Sangmin Bae","June-Woo Kim","Won-Yang Cho","Hyerim Baek","Soyoun Son","Byungjo Lee","Changwan Ha","Kyongpil Tae","Sungnyun Kim","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2305.14032v3.pdf","comment":"INTERSPEECH 2023, Code URL:\n  https://github.com/raymin0223/patch-mix_contrastive_learning"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.11069v2","updated":"2023-10-21T12:42:30Z","published":"2023-10-17T08:33:02Z","title":"VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System","summary":"  Arabic is a complex language with many varieties and dialects spoken by over\n450 millions all around the world. Due to the linguistic diversity and\nvariations, it is challenging to build a robust and generalized ASR system for\nArabic. In this work, we address this gap by developing and demoing a system,\ndubbed VoxArabica, for dialect identification (DID) as well as automatic speech\nrecognition (ASR) of Arabic. We train a wide range of models such as HuBERT\n(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR\ntasks. Our DID models are trained to identify 17 different dialects in addition\nto MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.\nAdditionally, for the remaining dialects in ASR, we provide the option to\nchoose various models such as Whisper and MMS in a zero-shot setting. We\nintegrate these models into a single web interface with diverse features such\nas audio recording, file upload, model selection, and the option to raise flags\nfor incorrect outputs. Overall, we believe VoxArabica will be useful for a wide\nrange of audiences concerned with Arabic research. Our system is currently\nrunning at https://cdce-206-12-100-168.ngrok.io/.\n","authors":["Abdul Waheed","Bashar Talafha","Peter Suvellin","Abdelrahman Elmadney","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2310.11069v2.pdf","comment":"Accepted at ArabicNLP conference co-located with EMNLP'23. First\n  three authors contributed equally"},{"id":"http://arxiv.org/abs/2310.14044v1","updated":"2023-10-21T15:41:50Z","published":"2023-10-21T15:41:50Z","title":"Composer Style-specific Symbolic Music Generation Using Vector Quantized\n  Discrete Diffusion Models","summary":"  Emerging Denoising Diffusion Probabilistic Models (DDPM) have become\nincreasingly utilised because of promising results they have achieved in\ndiverse generative tasks with continuous data, such as image and sound\nsynthesis. Nonetheless, the success of diffusion models has not been fully\nextended to discrete symbolic music. We propose to combine a vector quantized\nvariational autoencoder (VQ-VAE) and discrete diffusion models for the\ngeneration of symbolic music with desired composer styles. The trained VQ-VAE\ncan represent symbolic music as a sequence of indexes that correspond to\nspecific entries in a learned codebook. Subsequently, a discrete diffusion\nmodel is used to model the VQ-VAE's discrete latent space. The diffusion model\nis trained to generate intermediate music sequences consisting of codebook\nindexes, which are then decoded to symbolic music using the VQ-VAE's decoder.\nThe results demonstrate our model can generate symbolic music with target\ncomposer styles that meet the given conditions with a high accuracy of 72.36%.\n","authors":["Jincheng Zhang","Jingjing Tang","Charalampos Saitis","Gy√∂rgy Fazekas"],"pdf_url":"https://arxiv.org/pdf/2310.14044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14040v1","updated":"2023-10-21T15:35:43Z","published":"2023-10-21T15:35:43Z","title":"Fast Diffusion GAN Model for Symbolic Music Generation Controlled by\n  Emotions","summary":"  Diffusion models have shown promising results for a wide range of generative\ntasks with continuous data, such as image and audio synthesis. However, little\nprogress has been made on using diffusion models to generate discrete symbolic\nmusic because this new class of generative models are not well suited for\ndiscrete data while its iterative sampling process is computationally\nexpensive. In this work, we propose a diffusion model combined with a\nGenerative Adversarial Network, aiming to (i) alleviate one of the remaining\nchallenges in algorithmic music generation which is the control of generation\ntowards a target emotion, and (ii) mitigate the slow sampling drawback of\ndiffusion models applied to symbolic music generation. We first used a trained\nVariational Autoencoder to obtain embeddings of a symbolic music dataset with\nemotion labels and then used those to train a diffusion model. Our results\ndemonstrate the successful control of our diffusion model to generate symbolic\nmusic with a desired emotion. Our model achieves several orders of magnitude\nimprovement in computational cost, requiring merely four time steps to denoise\nwhile the steps required by current state-of-the-art diffusion models for\nsymbolic music generation is in the order of thousands.\n","authors":["Jincheng Zhang","Gy√∂rgy Fazekas","Charalampos Saitis"],"pdf_url":"https://arxiv.org/pdf/2310.14040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14018v1","updated":"2023-10-21T14:02:23Z","published":"2023-10-21T14:02:23Z","title":"Temporal convolutional neural networks to generate a head-related\n  impulse response from one direction to another","summary":"  Virtual sound synthesis is a technology that allows users to perceive spatial\nsound through headphones or earphones. However, accurate virtual sound requires\nan individual head-related transfer function (HRTF), which can be difficult to\nmeasure due to the need for a specialized environment. In this study, we\nproposed a method to generate HRTFs from one direction to the other. To this\nend, we used temporal convolutional neural networks (TCNs) to generate\nhead-related impulse responses (HRIRs). To train the TCNs, publicly available\ndatasets in the horizontal plane were used. Using the trained networks, we\nsuccessfully generated HRIRs for directions other than the front direction in\nthe dataset. We found that the proposed method successfully generated HRIRs for\npublicly available datasets. To test the generalization of the method, we\nmeasured the HRIRs of a new dataset and tested whether the trained networks\ncould be used for this new dataset. Although the similarity evaluated by\nspectral distortion was slightly degraded, behavioral experiments with human\nparticipants showed that the generated HRIRs were equivalent to the measured\nones. These results suggest that the proposed TCNs can be used to generate\npersonalized HRIRs from one direction to another, which could contribute to the\npersonalization of virtual sound.\n","authors":["Tatsuki Kobayashi","Yoshiko Maruyama","Isao Nambu","Shohei Yano","Yasuhiro Wada"],"pdf_url":"https://arxiv.org/pdf/2310.14018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14016v1","updated":"2023-10-21T13:56:23Z","published":"2023-10-21T13:56:23Z","title":"SwG-former: Sliding-window Graph Convolutional Network Integrated with\n  Conformer for Sound Event Localization and Detection","summary":"  Sound event localization and detection (SELD) is a joint task of sound event\ndetection (SED) and direction of arrival (DoA) estimation. SED mainly relies on\ntemporal dependencies to distinguish different sound classes, while DoA\nestimation depends on spatial correlations to estimate source directions. To\njointly optimize two subtasks, the SELD system should extract spatial\ncorrelations and model temporal dependencies simultaneously. However, numerous\nmodels mainly extract spatial correlations and model temporal dependencies\nseparately. In this paper, the interdependence of spatial-temporal information\nin audio signals is exploited for simultaneous extraction to enhance the model\nperformance. In response, a novel graph representation leveraging graph\nconvolutional network (GCN) in non-Euclidean space is developed to extract\nspatial-temporal information concurrently. A sliding-window graph (SwG) module\nis designed based on the graph representation. It exploits sliding-windows with\ndifferent sizes to learn temporal context information and dynamically\nconstructs graph vertices in the frequency-channel (F-C) domain to capture\nspatial correlations. Furthermore, as the cornerstone of message passing, a\nrobust Conv2dAgg function is proposed and embedded into the SwG module to\naggregate the features of neighbor vertices. To improve the performance of SELD\nin a natural spatial acoustic environment, a general and efficient SwG-former\nmodel is proposed by integrating the SwG module with the Conformer. It exhibits\nsuperior performance in comparison to recent advanced SELD models. To further\nvalidate the generality and efficiency of the SwG-former, it is seamlessly\nintegrated into the event-independent network version 2 (EINV2) called\nSwG-EINV2. The SwG-EINV2 surpasses the state-of-the-art (SOTA) methods under\nthe same acoustic environment.\n","authors":["Weiming Huang","Qinghua Huang","Liyan Ma","Zhengyu Chen","Chuan Wang"],"pdf_url":"https://arxiv.org/pdf/2310.14016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13974v1","updated":"2023-10-21T11:26:24Z","published":"2023-10-21T11:26:24Z","title":"Automatic Pronunciation Assessment -- A Review","summary":"  Pronunciation assessment and its application in computer-aided pronunciation\ntraining (CAPT) have seen impressive progress in recent years. With the rapid\ngrowth in language processing and deep learning over the past few years, there\nis a need for an updated review. In this paper, we review methods employed in\npronunciation assessment for both phonemic and prosodic. We categorize the main\nchallenges observed in prominent research trends, and highlight existing\nlimitations, and available resources. This is followed by a discussion of the\nremaining challenges and possible directions for future work.\n","authors":["Yassine El Kheir","Ahmed Ali","Shammur Absar Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2310.13974v1.pdf","comment":"9 pages, accepted to EMNLP Findings"},{"id":"http://arxiv.org/abs/2309.05855v2","updated":"2023-10-21T10:54:08Z","published":"2023-09-11T22:34:06Z","title":"Instabilities in Convnets for Raw Audio","summary":"  What makes waveform-based deep learning so hard? Despite numerous attempts at\ntraining convolutional neural networks (convnets) for filterbank design, they\noften fail to outperform hand-crafted baselines. These baselines are linear\ntime-invariant systems: as such, they can be approximated by convnets with wide\nreceptive fields. Yet, in practice, gradient-based optimization leads to\nsuboptimal approximations. In our article, we approach this phenomenon from the\nperspective of initialization. We present a theory of large deviations for the\nenergy response of FIR filterbanks with random Gaussian weights. We find that\ndeviations worsen for large filters and locally periodic input signals, which\nare both typical for audio signal processing applications. Numerical\nsimulations align with our theory and suggest that the condition number of a\nconvolutional layer follows a logarithmic scaling law between the number and\nlength of the filters, which is reminiscent of discrete wavelet bases.\n","authors":["Daniel Haider","Vincent Lostanlen","Martin Ehler","Peter Balazs"],"pdf_url":"https://arxiv.org/pdf/2309.05855v2.pdf","comment":"4 pages, 5 figures, 1 page appendix, under review for IEEE SPL"},{"id":"http://arxiv.org/abs/2305.14032v3","updated":"2023-10-21T07:12:33Z","published":"2023-05-23T13:04:07Z","title":"Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on\n  Respiratory Sound Classification","summary":"  Respiratory sound contains crucial information for the early diagnosis of\nfatal lung diseases. Since the COVID-19 pandemic, there has been a growing\ninterest in contact-free medical care based on electronic stethoscopes. To this\nend, cutting-edge deep learning models have been developed to diagnose lung\ndiseases; however, it is still challenging due to the scarcity of medical data.\nIn this study, we demonstrate that the pretrained model on large-scale visual\nand audio datasets can be generalized to the respiratory sound classification\ntask. In addition, we introduce a straightforward Patch-Mix augmentation, which\nrandomly mixes patches between different samples, with Audio Spectrogram\nTransformer (AST). We further propose a novel and effective Patch-Mix\nContrastive Learning to distinguish the mixed representations in the latent\nspace. Our method achieves state-of-the-art performance on the ICBHI dataset,\noutperforming the prior leading score by an improvement of 4.08%.\n","authors":["Sangmin Bae","June-Woo Kim","Won-Yang Cho","Hyerim Baek","Soyoun Son","Byungjo Lee","Changwan Ha","Kyongpil Tae","Sungnyun Kim","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2305.14032v3.pdf","comment":"INTERSPEECH 2023, Code URL:\n  https://github.com/raymin0223/patch-mix_contrastive_learning"}]},"2023-10-22T00:00:00Z":{"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.12837v2","updated":"2023-10-22T11:59:06Z","published":"2023-10-19T15:40:42Z","title":"Deep Beamforming for Speech Enhancement and Speaker Localization with an\n  Array Response-Aware Loss Function","summary":"  Recent research advances in deep neural network (DNN)-based beamformers have\nshown great promise for speech enhancement under adverse acoustic conditions.\nDifferent network architectures and input features have been explored in\nestimating beamforming weights. In this paper, we propose a deep beamformer\nbased on an efficient convolutional recurrent network (CRN) trained with a\nnovel ARray RespOnse-aWare (ARROW) loss function. The ARROW loss exploits the\narray responses of the target and interferer by using the ground truth relative\ntransfer functions (RTFs). The DNN-based beamforming system, trained with ARROW\nloss through supervised learning, is able to perform speech enhancement and\nspeaker localization jointly. Experimental results have shown that the proposed\ndeep beamformer, trained with the linearly weighted scale-invariant\nsource-to-noise ratio (SI-SNR) and ARROW loss functions, achieves superior\nperformance in speech enhancement and speaker localization compared to two\nbaselines.\n","authors":["Hsinyu Chang","Yicheng Hsu","Mingsian R. Bai"],"pdf_url":"https://arxiv.org/pdf/2310.12837v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2207.11749v3","updated":"2023-10-22T14:49:58Z","published":"2022-07-24T14:04:34Z","title":"Source Separation of Unknown Numbers of Single-Channel Underwater\n  Acoustic Signals Based on Autoencoders","summary":"  The separation of single-channel underwater acoustic signals is a challenging\nproblem with practical significance. Few existing studies focus on the source\nseparation problem with unknown numbers of signals, and how to evaluate the\nperformances of the systems is not yet clear. We propose a solution with a\nfixed number of output channels to address these two problems, enabling it to\navoid the dimensional disaster caused by the permutation problem induced by the\nalignment of outputs to targets. Specifically, we propose a two-step algorithm\nbased on autoencoders and a new performance evaluation method for situations\nwith mute channels. Experiments conducted on simulated mixtures of radiated\nship noise show that the proposed solution can achieve similar separation\nperformance to that attained with a known number of signals. The proposed\nalgorithm achieved competitive performance as two algorithms developed for\nknown numbers of signals, which is highly explainable and extensible and get\nthe state of the art under this framework.\n","authors":["Qinggang Sun","Kejun Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11749v3.pdf","comment":"14 pages, 4 figures, 3 tables. For codes, see\n  https://github.com/QinggangSUN/unknown_number_source_separation"},{"id":"http://arxiv.org/abs/2310.14301v1","updated":"2023-10-22T13:52:06Z","published":"2023-10-22T13:52:06Z","title":"An overview of text-to-speech systems and media applications","summary":"  Producing synthetic voice, similar to human-like sound, is an emerging\nnovelty of modern interactive media systems. Text-To-Speech (TTS) systems try\nto generate synthetic and authentic voices via text input. Besides, well known\nand familiar dubbing, announcing and narrating voices, as valuable possessions\nof any media organization, can be kept forever by utilizing TTS and Voice\nConversion (VC) algorithms . The emergence of deep learning approaches has made\nsuch TTS systems more accurate and accessible. To understand TTS systems\nbetter, this paper investigates the key components of such systems including\ntext analysis, acoustic modelling and vocoding. The paper then provides details\nof important state-of-the-art TTS systems based on deep learning. Finally, a\ncomparison is made between recently released systems in term of backbone\narchitecture, type of input and conversion, vocoder used and subjective\nassessment (MOS). Accordingly, Tacotron 2, Transformer TTS, WaveNet and\nFastSpeech 1 are among the most successful TTS systems ever released. In the\ndiscussion section, some suggestions are made to develop a TTS system with\nregard to the intended application.\n","authors":["Mohammad Reza Hasanabadi"],"pdf_url":"https://arxiv.org/pdf/2310.14301v1.pdf","comment":"Accepted in ABU Technical Review journal 2023/6"},{"id":"http://arxiv.org/abs/2310.14300v1","updated":"2023-10-22T13:44:31Z","published":"2023-10-22T13:44:31Z","title":"MFCC-GAN Codec: A New AI-based Audio Coding","summary":"  In this paper, we proposed AI-based audio coding using MFCC features in an\nadversarial setting. We combined a conventional encoder with an adversarial\nlearning decoder to better reconstruct the original waveform. Since GAN gives\nimplicit density estimation, therefore, such models are less prone to\noverfitting. We compared our work with five well-known codecs namely AAC, AC3,\nOpus, Vorbis, and Speex, performing on bitrates from 2kbps to 128kbps.\nMFCCGAN_36k achieved the state-of-the-art result in terms of SNR despite a\nlower bitrate in comparison to AC3_128k, AAC_112k, Vorbis_48k, Opus_48k, and\nSpeex_48K. On the other hand, MFCCGAN_13k also achieved high SNR=27 which is\nequal to that of AC3_128k, and AAC_112k while having a significantly lower\nbitrate (13 kbps). MFCCGAN_36k achieved higher NISQA-MOS results compared to\nAAC_48k while having a 20% lower bitrate. Furthermore, MFCCGAN_13k obtained\nNISQAMOS= 3.9 which is much higher than AAC_24k, AAC_32k, AC3_32k, and AAC_48k.\nFor future work, we finally suggest adopting loss functions optimizing\nintelligibility and perceptual metrics in the MFCCGAN structure to improve\nquality and intelligibility simultaneously.\n","authors":["Mohammad Reza Hasanabadi"],"pdf_url":"https://arxiv.org/pdf/2310.14300v1.pdf","comment":"Accepted in ABU Technical Review journal 2023/3"},{"id":"http://arxiv.org/abs/2210.15631v3","updated":"2023-10-22T13:03:09Z","published":"2022-10-27T17:21:14Z","title":"Exploring Effective Distillation of Self-Supervised Speech Models for\n  Automatic Speech Recognition","summary":"  Recent years have witnessed great strides in self-supervised learning (SSL)\non the speech processing. The SSL model is normally pre-trained on a great\nvariety of unlabelled data and a large model size is preferred to increase the\nmodeling capacity. However, this might limit its potential applications due to\nthe expensive computation and memory costs introduced by the oversize model.\nMiniaturization for SSL models has become an important research direction of\npractical value. To this end, we explore the effective distillation of\nHuBERT-based SSL models for automatic speech recognition (ASR). First, in order\nto establish a strong baseline, a comprehensive study on different student\nmodel structures is conducted. On top of this, as a supplement to the\nregression loss widely adopted in previous works, a discriminative loss is\nintroduced for HuBERT to enhance the distillation performance, especially in\nlow-resource scenarios. In addition, we design a simple and effective algorithm\nto distill the front-end input from waveform to Fbank feature, resulting in 17%\nparameter reduction and doubling inference speed, at marginal performance\ndegradation.\n","authors":["Yujin Wang","Changli Tang","Ziyang Ma","Zhisheng Zheng","Xie Chen","Wei-Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2210.15631v3.pdf","comment":"Accepted by ASRU 2023"},{"id":"http://arxiv.org/abs/2310.14278v1","updated":"2023-10-22T11:57:33Z","published":"2023-10-22T11:57:33Z","title":"Conversational Speech Recognition by Learning Audio-textual Cross-modal\n  Contextual Representation","summary":"  Automatic Speech Recognition (ASR) in conversational settings presents unique\nchallenges, including extracting relevant contextual information from previous\nconversational turns. Due to irrelevant content, error propagation, and\nredundancy, existing methods struggle to extract longer and more effective\ncontexts. To address this issue, we introduce a novel Conversational ASR\nsystem, extending the Conformer encoder-decoder model with cross-modal\nconversational representation. Our approach leverages a cross-modal extractor\nthat combines pre-trained speech and text models through a specialized encoder\nand a modal-level mask input. This enables the extraction of richer historical\nspeech context without explicit error propagation. We also incorporate\nconditional latent variational modules to learn conversational level attributes\nsuch as role preference and topic coherence. By introducing both cross-modal\nand conversational representations into the decoder, our model retains context\nover longer sentences without information loss, achieving relative accuracy\nimprovements of 8.8% and 23% on Mandarin conversation datasets HKUST and\nMagicData-RAMC, respectively, compared to the standard Conformer model.\n","authors":["Kun Wei","Bei Li","Hang Lv","Quan Lu","Ning Jiang","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2310.14278v1.pdf","comment":"Submitted to TASLP"},{"id":"http://arxiv.org/abs/2310.14181v1","updated":"2023-10-22T05:09:04Z","published":"2023-10-22T05:09:04Z","title":"A Study on Prosodic Entrainment in Relation to Therapist Empathy in\n  Counseling Conversation","summary":"  Counseling is carried out as spoken conversation between a therapist and a\nclient. The empathy level expressed by the therapist is considered an important\nindex of the quality of counseling and often assessed by an observer or the\nclient. This research investigates the entrainment of speech prosody in\nrelation to subjectively rated empathy. Experimental results show that the\nentrainment of intensity is more influential to empathy observation than that\nof pitch or speech rate in client-therapist interaction. The observer and the\nclient have different perceptions of therapist empathy with the same entrained\nphenomena in pitch and intensity. The client's intention to make adjustment on\npitch variation and intensity of speech is considered an indicator of the\nclient's perception of counseling quality.\n","authors":["Dehua Tao","Tan Lee","Harold Chui","Sarah Luk"],"pdf_url":"https://arxiv.org/pdf/2310.14181v1.pdf","comment":"Accepted by INTERSPEECH 2023"},{"id":"http://arxiv.org/abs/2310.14178v1","updated":"2023-10-22T05:00:04Z","published":"2023-10-22T05:00:04Z","title":"Modeling Intrapersonal and Interpersonal Influences for Automatic\n  Estimation of Therapist Empathy in Counseling Conversation","summary":"  Counseling is usually conducted through spoken conversation between a\ntherapist and a client. The empathy level of therapist is a key indicator of\noutcomes. Presuming that therapist's empathy expression is shaped by their past\nbehavior and their perception of the client's behavior, we propose a model to\nestimate the therapist empathy by considering both intrapersonal and\ninterpersonal influences. These dynamic influences are captured by applying an\nattention mechanism to the therapist turn and the historical turns of both\ntherapist and client. Our findings suggest that the integration of dynamic\ninfluences enhances empathy level estimation. The influence-derived embedding\nshould constitute a minor portion in the target turn representation for optimal\nempathy estimation. The client's turns (interpersonal influence) appear to\nslightly surpass the therapist's own turns (intrapersonal influence) in empathy\nestimation effectiveness. It is noted that concentrating exclusively on recent\nhistorical turns can significantly impact the estimation of therapist empathy.\n","authors":["Dehua Tao","Tan Lee","Harold Chui","Sarah Luk"],"pdf_url":"https://arxiv.org/pdf/2310.14178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14173v1","updated":"2023-10-22T04:15:59Z","published":"2023-10-22T04:15:59Z","title":"First-Shot Unsupervised Anomalous Sound Detection With Unknown Anomalies\n  Estimated by Metadata-Assisted Audio Generation","summary":"  First-shot (FS) unsupervised anomalous sound detection (ASD) is a brand-new\ntask introduced in DCASE 2023 Challenge Task 2, where the anomalous sounds for\nthe target machine types are unseen in training. Existing methods often rely on\nthe availability of normal and abnormal sound data from the target machines.\nHowever, due to the lack of anomalous sound data for the target machine types,\nit becomes challenging when adapting the existing ASD methods to the first-shot\ntask. In this paper, we propose a new framework for the first-shot unsupervised\nASD, where metadata-assisted audio generation is used to estimate unknown\nanomalies, by utilising the available machine information (i.e., metadata and\nsound data) to fine-tune a text-to-audio generation model for generating the\nanomalous sounds that contain unique acoustic characteristics accounting for\neach different machine types. We then use the method of Time-Weighted Frequency\ndomain audio Representation with Gaussian Mixture Model (TWFR-GMM) as the\nbackbone to achieve the first-shot unsupervised ASD. Our proposed FS-TWFR-GMM\nmethod achieves competitive performance amongst top systems in DCASE 2023\nChallenge Task 2, while requiring only 1% model parameters for detection, as\nvalidated in our experiments.\n","authors":["Hejing Zhang","Qiaoxi Zhu","Jian Guan","Haohe Liu","Feiyang Xiao","Jiantong Tian","Xinhao Mei","Xubo Liu","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.14173v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2212.10901v3","updated":"2023-10-22T02:12:16Z","published":"2022-12-21T10:20:54Z","title":"ALCAP: Alignment-Augmented Music Captioner","summary":"  Music captioning has gained significant attention in the wake of the rising\nprominence of streaming media platforms. Traditional approaches often\nprioritize either the audio or lyrics aspect of the music, inadvertently\nignoring the intricate interplay between the two. However, a comprehensive\nunderstanding of music necessitates the integration of both these elements. In\nthis study, we delve into this overlooked realm by introducing a method to\nsystematically learn multimodal alignment between audio and lyrics through\ncontrastive learning. This not only recognizes and emphasizes the synergy\nbetween audio and lyrics but also paves the way for models to achieve deeper\ncross-modal coherence, thereby producing high-quality captions. We provide both\ntheoretical and empirical results demonstrating the advantage of the proposed\nmethod, which achieves new state-of-the-art on two music captioning datasets.\n","authors":["Zihao He","Weituo Hao","Wei-Tsung Lu","Changyou Chen","Kristina Lerman","Xuchen Song"],"pdf_url":"https://arxiv.org/pdf/2212.10901v3.pdf","comment":null},{"id":"http://arxiv.org/abs/1811.09620v3","updated":"2023-10-22T04:43:08Z","published":"2018-11-22T17:46:51Z","title":"TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre\n  Transfer","summary":"  In this work, we address the problem of musical timbre transfer, where the\ngoal is to manipulate the timbre of a sound sample from one instrument to match\nanother instrument while preserving other musical content, such as pitch,\nrhythm, and loudness. In principle, one could apply image-based style transfer\ntechniques to a time-frequency representation of an audio signal, but this\ndepends on having a representation that allows independent manipulation of\ntimbre as well as high-quality waveform generation. We introduce TimbreTron, a\nmethod for musical timbre transfer which applies \"image\" domain style transfer\nto a time-frequency representation of the audio signal, and then produces a\nhigh-quality waveform using a conditional WaveNet synthesizer. We show that the\nConstant Q Transform (CQT) representation is particularly well-suited to\nconvolutional architectures due to its approximate pitch equivariance. Based on\nhuman perceptual evaluations, we confirmed that TimbreTron recognizably\ntransferred the timbre while otherwise preserving the musical content, for both\nmonophonic and polyphonic samples.\n","authors":["Sicong Huang","Qiyang Li","Cem Anil","Xuchan Bao","Sageev Oore","Roger B. Grosse"],"pdf_url":"https://arxiv.org/pdf/1811.09620v3.pdf","comment":"17 pages, published as a conference paper at ICLR 2019"}],"Sound":[{"id":"http://arxiv.org/abs/2207.11749v3","updated":"2023-10-22T14:49:58Z","published":"2022-07-24T14:04:34Z","title":"Source Separation of Unknown Numbers of Single-Channel Underwater\n  Acoustic Signals Based on Autoencoders","summary":"  The separation of single-channel underwater acoustic signals is a challenging\nproblem with practical significance. Few existing studies focus on the source\nseparation problem with unknown numbers of signals, and how to evaluate the\nperformances of the systems is not yet clear. We propose a solution with a\nfixed number of output channels to address these two problems, enabling it to\navoid the dimensional disaster caused by the permutation problem induced by the\nalignment of outputs to targets. Specifically, we propose a two-step algorithm\nbased on autoencoders and a new performance evaluation method for situations\nwith mute channels. Experiments conducted on simulated mixtures of radiated\nship noise show that the proposed solution can achieve similar separation\nperformance to that attained with a known number of signals. The proposed\nalgorithm achieved competitive performance as two algorithms developed for\nknown numbers of signals, which is highly explainable and extensible and get\nthe state of the art under this framework.\n","authors":["Qinggang Sun","Kejun Wang"],"pdf_url":"https://arxiv.org/pdf/2207.11749v3.pdf","comment":"14 pages, 4 figures, 3 tables. For codes, see\n  https://github.com/QinggangSUN/unknown_number_source_separation"},{"id":"http://arxiv.org/abs/2310.14301v1","updated":"2023-10-22T13:52:06Z","published":"2023-10-22T13:52:06Z","title":"An overview of text-to-speech systems and media applications","summary":"  Producing synthetic voice, similar to human-like sound, is an emerging\nnovelty of modern interactive media systems. Text-To-Speech (TTS) systems try\nto generate synthetic and authentic voices via text input. Besides, well known\nand familiar dubbing, announcing and narrating voices, as valuable possessions\nof any media organization, can be kept forever by utilizing TTS and Voice\nConversion (VC) algorithms . The emergence of deep learning approaches has made\nsuch TTS systems more accurate and accessible. To understand TTS systems\nbetter, this paper investigates the key components of such systems including\ntext analysis, acoustic modelling and vocoding. The paper then provides details\nof important state-of-the-art TTS systems based on deep learning. Finally, a\ncomparison is made between recently released systems in term of backbone\narchitecture, type of input and conversion, vocoder used and subjective\nassessment (MOS). Accordingly, Tacotron 2, Transformer TTS, WaveNet and\nFastSpeech 1 are among the most successful TTS systems ever released. In the\ndiscussion section, some suggestions are made to develop a TTS system with\nregard to the intended application.\n","authors":["Mohammad Reza Hasanabadi"],"pdf_url":"https://arxiv.org/pdf/2310.14301v1.pdf","comment":"Accepted in ABU Technical Review journal 2023/6"},{"id":"http://arxiv.org/abs/2310.14300v1","updated":"2023-10-22T13:44:31Z","published":"2023-10-22T13:44:31Z","title":"MFCC-GAN Codec: A New AI-based Audio Coding","summary":"  In this paper, we proposed AI-based audio coding using MFCC features in an\nadversarial setting. We combined a conventional encoder with an adversarial\nlearning decoder to better reconstruct the original waveform. Since GAN gives\nimplicit density estimation, therefore, such models are less prone to\noverfitting. We compared our work with five well-known codecs namely AAC, AC3,\nOpus, Vorbis, and Speex, performing on bitrates from 2kbps to 128kbps.\nMFCCGAN_36k achieved the state-of-the-art result in terms of SNR despite a\nlower bitrate in comparison to AC3_128k, AAC_112k, Vorbis_48k, Opus_48k, and\nSpeex_48K. On the other hand, MFCCGAN_13k also achieved high SNR=27 which is\nequal to that of AC3_128k, and AAC_112k while having a significantly lower\nbitrate (13 kbps). MFCCGAN_36k achieved higher NISQA-MOS results compared to\nAAC_48k while having a 20% lower bitrate. Furthermore, MFCCGAN_13k obtained\nNISQAMOS= 3.9 which is much higher than AAC_24k, AAC_32k, AC3_32k, and AAC_48k.\nFor future work, we finally suggest adopting loss functions optimizing\nintelligibility and perceptual metrics in the MFCCGAN structure to improve\nquality and intelligibility simultaneously.\n","authors":["Mohammad Reza Hasanabadi"],"pdf_url":"https://arxiv.org/pdf/2310.14300v1.pdf","comment":"Accepted in ABU Technical Review journal 2023/3"},{"id":"http://arxiv.org/abs/2210.15631v3","updated":"2023-10-22T13:03:09Z","published":"2022-10-27T17:21:14Z","title":"Exploring Effective Distillation of Self-Supervised Speech Models for\n  Automatic Speech Recognition","summary":"  Recent years have witnessed great strides in self-supervised learning (SSL)\non the speech processing. The SSL model is normally pre-trained on a great\nvariety of unlabelled data and a large model size is preferred to increase the\nmodeling capacity. However, this might limit its potential applications due to\nthe expensive computation and memory costs introduced by the oversize model.\nMiniaturization for SSL models has become an important research direction of\npractical value. To this end, we explore the effective distillation of\nHuBERT-based SSL models for automatic speech recognition (ASR). First, in order\nto establish a strong baseline, a comprehensive study on different student\nmodel structures is conducted. On top of this, as a supplement to the\nregression loss widely adopted in previous works, a discriminative loss is\nintroduced for HuBERT to enhance the distillation performance, especially in\nlow-resource scenarios. In addition, we design a simple and effective algorithm\nto distill the front-end input from waveform to Fbank feature, resulting in 17%\nparameter reduction and doubling inference speed, at marginal performance\ndegradation.\n","authors":["Yujin Wang","Changli Tang","Ziyang Ma","Zhisheng Zheng","Xie Chen","Wei-Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2210.15631v3.pdf","comment":"Accepted by ASRU 2023"},{"id":"http://arxiv.org/abs/2310.14278v1","updated":"2023-10-22T11:57:33Z","published":"2023-10-22T11:57:33Z","title":"Conversational Speech Recognition by Learning Audio-textual Cross-modal\n  Contextual Representation","summary":"  Automatic Speech Recognition (ASR) in conversational settings presents unique\nchallenges, including extracting relevant contextual information from previous\nconversational turns. Due to irrelevant content, error propagation, and\nredundancy, existing methods struggle to extract longer and more effective\ncontexts. To address this issue, we introduce a novel Conversational ASR\nsystem, extending the Conformer encoder-decoder model with cross-modal\nconversational representation. Our approach leverages a cross-modal extractor\nthat combines pre-trained speech and text models through a specialized encoder\nand a modal-level mask input. This enables the extraction of richer historical\nspeech context without explicit error propagation. We also incorporate\nconditional latent variational modules to learn conversational level attributes\nsuch as role preference and topic coherence. By introducing both cross-modal\nand conversational representations into the decoder, our model retains context\nover longer sentences without information loss, achieving relative accuracy\nimprovements of 8.8% and 23% on Mandarin conversation datasets HKUST and\nMagicData-RAMC, respectively, compared to the standard Conformer model.\n","authors":["Kun Wei","Bei Li","Hang Lv","Quan Lu","Ning Jiang","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2310.14278v1.pdf","comment":"Submitted to TASLP"},{"id":"http://arxiv.org/abs/2310.14173v1","updated":"2023-10-22T04:15:59Z","published":"2023-10-22T04:15:59Z","title":"First-Shot Unsupervised Anomalous Sound Detection With Unknown Anomalies\n  Estimated by Metadata-Assisted Audio Generation","summary":"  First-shot (FS) unsupervised anomalous sound detection (ASD) is a brand-new\ntask introduced in DCASE 2023 Challenge Task 2, where the anomalous sounds for\nthe target machine types are unseen in training. Existing methods often rely on\nthe availability of normal and abnormal sound data from the target machines.\nHowever, due to the lack of anomalous sound data for the target machine types,\nit becomes challenging when adapting the existing ASD methods to the first-shot\ntask. In this paper, we propose a new framework for the first-shot unsupervised\nASD, where metadata-assisted audio generation is used to estimate unknown\nanomalies, by utilising the available machine information (i.e., metadata and\nsound data) to fine-tune a text-to-audio generation model for generating the\nanomalous sounds that contain unique acoustic characteristics accounting for\neach different machine types. We then use the method of Time-Weighted Frequency\ndomain audio Representation with Gaussian Mixture Model (TWFR-GMM) as the\nbackbone to achieve the first-shot unsupervised ASD. Our proposed FS-TWFR-GMM\nmethod achieves competitive performance amongst top systems in DCASE 2023\nChallenge Task 2, while requiring only 1% model parameters for detection, as\nvalidated in our experiments.\n","authors":["Hejing Zhang","Qiaoxi Zhu","Jian Guan","Haohe Liu","Feiyang Xiao","Jiantong Tian","Xinhao Mei","Xubo Liu","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.14173v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2212.10901v3","updated":"2023-10-22T02:12:16Z","published":"2022-12-21T10:20:54Z","title":"ALCAP: Alignment-Augmented Music Captioner","summary":"  Music captioning has gained significant attention in the wake of the rising\nprominence of streaming media platforms. Traditional approaches often\nprioritize either the audio or lyrics aspect of the music, inadvertently\nignoring the intricate interplay between the two. However, a comprehensive\nunderstanding of music necessitates the integration of both these elements. In\nthis study, we delve into this overlooked realm by introducing a method to\nsystematically learn multimodal alignment between audio and lyrics through\ncontrastive learning. This not only recognizes and emphasizes the synergy\nbetween audio and lyrics but also paves the way for models to achieve deeper\ncross-modal coherence, thereby producing high-quality captions. We provide both\ntheoretical and empirical results demonstrating the advantage of the proposed\nmethod, which achieves new state-of-the-art on two music captioning datasets.\n","authors":["Zihao He","Weituo Hao","Wei-Tsung Lu","Changyou Chen","Kristina Lerman","Xuchen Song"],"pdf_url":"https://arxiv.org/pdf/2212.10901v3.pdf","comment":null},{"id":"http://arxiv.org/abs/1811.09620v3","updated":"2023-10-22T04:43:08Z","published":"2018-11-22T17:46:51Z","title":"TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre\n  Transfer","summary":"  In this work, we address the problem of musical timbre transfer, where the\ngoal is to manipulate the timbre of a sound sample from one instrument to match\nanother instrument while preserving other musical content, such as pitch,\nrhythm, and loudness. In principle, one could apply image-based style transfer\ntechniques to a time-frequency representation of an audio signal, but this\ndepends on having a representation that allows independent manipulation of\ntimbre as well as high-quality waveform generation. We introduce TimbreTron, a\nmethod for musical timbre transfer which applies \"image\" domain style transfer\nto a time-frequency representation of the audio signal, and then produces a\nhigh-quality waveform using a conditional WaveNet synthesizer. We show that the\nConstant Q Transform (CQT) representation is particularly well-suited to\nconvolutional architectures due to its approximate pitch equivariance. Based on\nhuman perceptual evaluations, we confirmed that TimbreTron recognizably\ntransferred the timbre while otherwise preserving the musical content, for both\nmonophonic and polyphonic samples.\n","authors":["Sicong Huang","Qiyang Li","Cem Anil","Xuchan Bao","Sageev Oore","Roger B. Grosse"],"pdf_url":"https://arxiv.org/pdf/1811.09620v3.pdf","comment":"17 pages, published as a conference paper at ICLR 2019"}]},"2023-10-23T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2310.15130v1","updated":"2023-10-23T17:34:31Z","published":"2023-10-23T17:34:31Z","title":"Novel-View Acoustic Synthesis from 3D Reconstructed Rooms","summary":"  We investigate the benefit of combining blind audio recordings with 3D scene\ninformation for novel-view acoustic synthesis. Given audio recordings from 2-4\nmicrophones and the 3D geometry and material of a scene containing multiple\nunknown sound sources, we estimate the sound anywhere in the scene. We identify\nthe main challenges of novel-view acoustic synthesis as sound source\nlocalization, separation, and dereverberation. While naively training an\nend-to-end network fails to produce high-quality results, we show that\nincorporating room impulse responses (RIRs) derived from 3D reconstructed rooms\nenables the same network to jointly tackle these tasks. Our method outperforms\nexisting methods designed for the individual tasks, demonstrating its\neffectiveness at utilizing 3D visual information. In a simulated study on the\nMatterport3D-NVAS dataset, our model achieves near-perfect accuracy on source\nlocalization, a PSNR of 26.44 dB and a SDR of 14.23 dB for source separation\nand dereverberation, resulting in a PSNR of 25.55 dB and a SDR of 14.20 dB on\nnovel-view acoustic synthesis. Code, pretrained model, and video results are\navailable on the project webpage (https://github.com/apple/ml-nvas3d).\n","authors":["Byeongjoo Ahn","Karren Yang","Brian Hamilton","Jonathan Sheaffer","Anurag Ranjan","Miguel Sarabia","Oncel Tuzel","Jen-Hao Rick Chang"],"pdf_url":"https://arxiv.org/pdf/2310.15130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14954v1","updated":"2023-10-23T13:55:49Z","published":"2023-10-23T13:55:49Z","title":"Key Frame Mechanism For Efficient Conformer Based End-to-end Speech\n  Recognition","summary":"  Recently, Conformer as a backbone network for end-to-end automatic speech\nrecognition achieved state-of-the-art performance. The Conformer block\nleverages a self-attention mechanism to capture global information, along with\na convolutional neural network to capture local information, resulting in\nimproved performance. However, the Conformer-based model encounters an issue\nwith the self-attention mechanism, as computational complexity grows\nquadratically with the length of the input sequence. Inspired by previous\nConnectionist Temporal Classification (CTC) guided blank skipping during\ndecoding, we introduce intermediate CTC outputs as guidance into the\ndownsampling procedure of the Conformer encoder. We define the frame with\nnon-blank output as key frame. Specifically, we introduce the key frame-based\nself-attention (KFSA) mechanism, a novel method to reduce the computation of\nthe self-attention mechanism using key frames. The structure of our proposed\napproach comprises two encoders. Following the initial encoder, we introduce an\nintermediate CTC loss function to compute the label frame, enabling us to\nextract the key frames and blank frames for KFSA. Furthermore, we introduce the\nkey frame-based downsampling (KFDS) mechanism to operate on high-dimensional\nacoustic features directly and drop the frames corresponding to blank labels,\nwhich results in new acoustic feature sequences as input to the second encoder.\nBy using the proposed method, which achieves comparable or higher performance\nthan vanilla Conformer and other similar work such as Efficient Conformer.\nMeantime, our proposed method can discard more than 60\\% useless frames during\nmodel training and inference, which will accelerate the inference speed\nsignificantly. This work code is available in\n{https://github.com/scufan1990/Key-Frame-Mechanism-For-Efficient-Conformer}\n","authors":["Peng Fan","Changhao Shan","Jianwei Zhang","Sining Sun","Qing Yang"],"pdf_url":"https://arxiv.org/pdf/2310.14954v1.pdf","comment":"This manuscript has been accepted by IEEE Signal Processing Letters\n  for publication"},{"id":"http://arxiv.org/abs/2310.14952v1","updated":"2023-10-23T13:54:53Z","published":"2023-10-23T13:54:53Z","title":"8+8=4: Formalizing Time Units to Handle Symbolic Music Durations","summary":"  This paper focuses on the nominal durations of musical events (notes and\nrests) in a symbolic musical score, and on how to conveniently handle these in\ncomputer applications. We propose the usage of a temporal unit that is directly\nrelated to the graphical symbols in musical scores and pair this with a set of\noperations that cover typical computations in music applications. We formalize\nthis time unit and the more commonly used approach in a single mathematical\nframework, as semirings, algebraic structures that enable an abstract\ndescription of algorithms/processing pipelines. We then discuss some practical\nuse cases and highlight when our system can improve such pipelines by making\nthem more efficient in terms of data type used and the number of computations.\n","authors":["Emmanouil Karystinaios","Francesco Foscarin","Florent Jacquemard","Masahiko Sakai","Satoshi Tojo","Gerhard Widmer"],"pdf_url":"https://arxiv.org/pdf/2310.14952v1.pdf","comment":"In Proceedings of the International Symposium on Computer Music\n  Multidisciplinary Research (CMMR 2023), Tokyo, Japan"},{"id":"http://arxiv.org/abs/2310.14946v1","updated":"2023-10-23T13:45:21Z","published":"2023-10-23T13:45:21Z","title":"Intuitive Multilingual Audio-Visual Speech Recognition with a\n  Single-Trained Model","summary":"  We present a novel approach to multilingual audio-visual speech recognition\ntasks by introducing a single model on a multilingual dataset. Motivated by a\nhuman cognitive system where humans can intuitively distinguish different\nlanguages without any conscious effort or guidance, we propose a model that can\ncapture which language is given as an input speech by distinguishing the\ninherent similarities and differences between languages. To do so, we design a\nprompt fine-tuning technique into the largely pre-trained audio-visual\nrepresentation model so that the network can recognize the language class as\nwell as the speech with the corresponding language. Our work contributes to\ndeveloping robust and efficient multilingual audio-visual speech recognition\nsystems, reducing the need for language-specific models.\n","authors":["Joanna Hong","Se Jin Park","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2310.14946v1.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2310.14806v1","updated":"2023-10-23T11:00:27Z","published":"2023-10-23T11:00:27Z","title":"Leveraging Timestamp Information for Serialized Joint Streaming\n  Recognition and Translation","summary":"  The growing need for instant spoken language transcription and translation is\ndriven by increased global communication and cross-lingual interactions. This\nhas made offering translations in multiple languages essential for user\napplications. Traditional approaches to automatic speech recognition (ASR) and\nspeech translation (ST) have often relied on separate systems, leading to\ninefficiencies in computational resources, and increased synchronization\ncomplexity in real time. In this paper, we propose a streaming\nTransformer-Transducer (T-T) model able to jointly produce many-to-one and\none-to-many transcription and translation using a single decoder. We introduce\na novel method for joint token-level serialized output training based on\ntimestamp information to effectively produce ASR and ST outputs in the\nstreaming setting. Experiments on {it,es,de}->en prove the effectiveness of our\napproach, enabling the generation of one-to-many joint outputs with a single\ndecoder for the first time.\n","authors":["Sara Papi","Peidong Wang","Junkun Chen","Jian Xue","Naoyuki Kanda","Jinyu Li","Yashesh Gaur"],"pdf_url":"https://arxiv.org/pdf/2310.14806v1.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2310.14778v1","updated":"2023-10-23T10:29:33Z","published":"2023-10-23T10:29:33Z","title":"Audio-Visual Speaker Tracking: Progress, Challenges, and Future\n  Directions","summary":"  Audio-visual speaker tracking has drawn increasing attention over the past\nfew years due to its academic values and wide application. Audio and visual\nmodalities can provide complementary information for localization and tracking.\nWith audio and visual information, the Bayesian-based filter can solve the\nproblem of data association, audio-visual fusion and track management. In this\npaper, we conduct a comprehensive overview of audio-visual speaker tracking. To\nour knowledge, this is the first extensive survey over the past five years. We\nintroduce the family of Bayesian filters and summarize the methods for\nobtaining audio-visual measurements. In addition, the existing trackers and\ntheir performance on AV16.3 dataset are summarized. In the past few years, deep\nlearning techniques have thrived, which also boosts the development of audio\nvisual speaker tracking. The influence of deep learning techniques in terms of\nmeasurement extraction and state estimation is also discussed. At last, we\ndiscuss the connections between audio-visual speaker tracking and other areas\nsuch as speech separation and distributed speaker tracking.\n","authors":["Jinzheng Zhao","Yong Xu","Xinyuan Qian","Davide Berghi","Peipei Wu","Meng Cui","Jianyuan Sun","Philip J. B. Jackson","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.14778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14580v1","updated":"2023-10-23T05:38:41Z","published":"2023-10-23T05:38:41Z","title":"Acoustic BPE for Speech Generation with Discrete Tokens","summary":"  Discrete audio tokens derived from self-supervised learning models have\ngained widespread usage in speech generation. However, current practice of\ndirectly utilizing audio tokens poses challenges for sequence modeling due to\nthe length of the token sequence. Additionally, this approach places the burden\non the model to establish correlations between tokens, further complicating the\nmodeling process. To address this issue, we propose acoustic BPE which encodes\nfrequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE\neffectively reduces the sequence length and leverages the prior morphological\ninformation present in token sequence, which alleviates the modeling challenges\nof token correlation. Through comprehensive investigations on a speech language\nmodel trained with acoustic BPE, we confirm the notable advantages it offers,\nincluding faster inference and improved syntax capturing capabilities. In\naddition, we propose a novel rescore method to select the optimal synthetic\nspeech among multiple candidates generated by rich-diversity TTS system.\nExperiments prove that rescore selection aligns closely with human preference,\nwhich highlights acoustic BPE's potential to other speech generation tasks.\n","authors":["Feiyu Shen","Yiwei Guo","Chenpeng Du","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2310.14580v1.pdf","comment":"5 pages, 2 figures; submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.15399v1","updated":"2023-10-23T23:01:33Z","published":"2023-10-23T23:01:33Z","title":"GESI: Gammachirp Envelope Similarity Index for Predicting\n  Intelligibility of Simulated Hearing Loss Sounds","summary":"  We proposed a new objective intelligibility measure (OIM), called the\nGammachirp Envelope Similarity Index (GESI), which can predict the speech\nintelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing\n(NH) listeners. GESI is an intrusive method that computes the SI metric using\nthe gammachirp filterbank (GCFB), the modulation filterbank, and the extended\ncosine similarity measure. GESI can accept the level asymmetry of the reference\nand test sounds and reflect the HI listener's hearing level as it appears on\nthe audiogram. A unique feature of GESI is its ability to incorporate an\nindividual participant's listening condition into the SI prediction. We\nconducted four SI experiments on male and female speech sounds in both\nlaboratory and crowdsourced remote environments. We then evaluated GESI and the\nconventional OIMs, STOI, ESTOI, MBSTOI, and HASPI, for their ability to predict\nmean and individual SI values with and without the use of simulated HL sounds.\nGESI outperformed the other OIMs in all evaluations. STOI, ESTOI, and MBSTOI\ndid not predict SI at all, even when using the simulated HL sounds. HASPI did\nnot predict the difference between the laboratory and remote experiments on\nmale speech sounds and the individual SI values. GESI may provide a first step\ntoward SI prediction for individual HI listeners whose HL is caused solely by\nperipheral dysfunction.\n","authors":["Ayako Yamamoto","Toshio Irino","Fuki Miyazaki","Honoka Tamaru"],"pdf_url":"https://arxiv.org/pdf/2310.15399v1.pdf","comment":"This paper was submitted to Speech Communication on September 14,\n  2023"},{"id":"http://arxiv.org/abs/2301.11757v3","updated":"2023-10-23T20:47:30Z","published":"2023-01-27T14:52:53Z","title":"Mo√ªsai: Text-to-Music Generation with Long-Context Latent Diffusion","summary":"  Recent years have seen the rapid development of large generative models for\ntext; however, much less research has explored the connection between text and\nanother \"language\" of communication -- music. Music, much like text, can convey\nemotions, stories, and ideas, and has its own unique structure and syntax. In\nour work, we bridge text and music via a text-to-music generation model that is\nhighly efficient, expressive, and can handle long-term structure. Specifically,\nwe develop Mo\\^usai, a cascading two-stage latent diffusion model that can\ngenerate multiple minutes of high-quality stereo music at 48kHz from textual\ndescriptions. Moreover, our model features high efficiency, which enables\nreal-time inference on a single consumer GPU with a reasonable speed. Through\nexperiments and property analyses, we show our model's competence over a\nvariety of criteria compared with existing music generation models. Lastly, to\npromote the open-source culture, we provide a collection of open-source\nlibraries with the hope of facilitating future work in the field. We\nopen-source the following: Codes:\nhttps://github.com/archinetai/audio-diffusion-pytorch; music samples for this\npaper: http://bit.ly/44ozWDH; all music samples for all models:\nhttps://bit.ly/audio-diffusion.\n","authors":["Flavio Schneider","Ojasv Kamal","Zhijing Jin","Bernhard Sch√∂lkopf"],"pdf_url":"https://arxiv.org/pdf/2301.11757v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15261v1","updated":"2023-10-23T18:09:31Z","published":"2023-10-23T18:09:31Z","title":"Modality Dropout for Multimodal Device Directed Speech Detection using\n  Verbal and Non-Verbal Features","summary":"  Device-directed speech detection (DDSD) is the binary classification task of\ndistinguishing between queries directed at a voice assistant versus side\nconversation or background speech. State-of-the-art DDSD systems use verbal\ncues, e.g acoustic, text and/or automatic speech recognition system (ASR)\nfeatures, to classify speech as device-directed or otherwise, and often have to\ncontend with one or more of these modalities being unavailable when deployed in\nreal-world settings. In this paper, we investigate fusion schemes for DDSD\nsystems that can be made more robust to missing modalities. Concurrently, we\nstudy the use of non-verbal cues, specifically prosody features, in addition to\nverbal cues for DDSD. We present different approaches to combine scores and\nembeddings from prosody with the corresponding verbal cues, finding that\nprosody improves DDSD performance by upto 8.5% in terms of false acceptance\nrate (FA) at a given fixed operating point via non-linear intermediate fusion,\nwhile our use of modality dropout techniques improves the performance of these\nmodels by 7.4% in terms of FA when evaluated with missing modalities during\ninference time.\n","authors":["Gautam Krishna","Sameer Dharur","Oggi Rudovic","Pranay Dighe","Saurabh Adya","Ahmed Hussen Abdelaziz","Ahmed H Tewfik"],"pdf_url":"https://arxiv.org/pdf/2310.15261v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2310.15247v1","updated":"2023-10-23T18:01:36Z","published":"2023-10-23T18:01:36Z","title":"SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis","summary":"  Sound design involves creatively selecting, recording, and editing sound\neffects for various media like cinema, video games, and virtual/augmented\nreality. One of the most time-consuming steps when designing sound is\nsynchronizing audio with video. In some cases, environmental recordings from\nvideo shoots are available, which can aid in the process. However, in video\ngames and animations, no reference audio exists, requiring manual annotation of\nevent timings from the video. We propose a system to extract repetitive actions\nonsets from a video, which are then used - in conjunction with audio or textual\nembeddings - to condition a diffusion model trained to generate a new\nsynchronized sound effects audio track. In this way, we leave complete creative\ncontrol to the sound designer while removing the burden of synchronization with\nvideo. Furthermore, editing the onset track or changing the conditioning\nembedding requires much less effort than editing the audio track itself,\nsimplifying the sonification process. We provide sound examples, source code,\nand pretrained models to faciliate reproducibility\n","authors":["Marco Comunit√†","Riccardo F. Gramaccioni","Emilian Postolache","Emanuele Rodol√†","Danilo Comminiello","Joshua D. Reiss"],"pdf_url":"https://arxiv.org/pdf/2310.15247v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.15130v1","updated":"2023-10-23T17:34:31Z","published":"2023-10-23T17:34:31Z","title":"Novel-View Acoustic Synthesis from 3D Reconstructed Rooms","summary":"  We investigate the benefit of combining blind audio recordings with 3D scene\ninformation for novel-view acoustic synthesis. Given audio recordings from 2-4\nmicrophones and the 3D geometry and material of a scene containing multiple\nunknown sound sources, we estimate the sound anywhere in the scene. We identify\nthe main challenges of novel-view acoustic synthesis as sound source\nlocalization, separation, and dereverberation. While naively training an\nend-to-end network fails to produce high-quality results, we show that\nincorporating room impulse responses (RIRs) derived from 3D reconstructed rooms\nenables the same network to jointly tackle these tasks. Our method outperforms\nexisting methods designed for the individual tasks, demonstrating its\neffectiveness at utilizing 3D visual information. In a simulated study on the\nMatterport3D-NVAS dataset, our model achieves near-perfect accuracy on source\nlocalization, a PSNR of 26.44 dB and a SDR of 14.23 dB for source separation\nand dereverberation, resulting in a PSNR of 25.55 dB and a SDR of 14.20 dB on\nnovel-view acoustic synthesis. Code, pretrained model, and video results are\navailable on the project webpage (https://github.com/apple/ml-nvas3d).\n","authors":["Byeongjoo Ahn","Karren Yang","Brian Hamilton","Jonathan Sheaffer","Anurag Ranjan","Miguel Sarabia","Oncel Tuzel","Jen-Hao Rick Chang"],"pdf_url":"https://arxiv.org/pdf/2310.15130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14982v1","updated":"2023-10-23T14:29:48Z","published":"2023-10-23T14:29:48Z","title":"Delayed Memory Unit: Modelling Temporal Dependency Through Delay Gate","summary":"  Recurrent Neural Networks (RNNs) are renowned for their adeptness in modeling\ntemporal dependencies, a trait that has driven their widespread adoption for\nsequential data processing. Nevertheless, vanilla RNNs are confronted with the\nwell-known issue of gradient vanishing and exploding, posing a significant\nchallenge for learning and establishing long-range dependencies. Additionally,\ngated RNNs tend to be over-parameterized, resulting in poor network\ngeneralization. To address these challenges, we propose a novel Delayed Memory\nUnit (DMU) in this paper, wherein a delay line structure, coupled with delay\ngates, is introduced to facilitate temporal interaction and temporal credit\nassignment, so as to enhance the temporal modeling capabilities of vanilla\nRNNs. Particularly, the DMU is designed to directly distribute the input\ninformation to the optimal time instant in the future, rather than aggregating\nand redistributing it over time through intricate network dynamics. Our\nproposed DMU demonstrates superior temporal modeling capabilities across a\nbroad range of sequential modeling tasks, utilizing considerably fewer\nparameters than other state-of-the-art gated RNN models in applications such as\nspeech recognition, radar gesture recognition, ECG waveform segmentation, and\npermuted sequential image classification.\n","authors":["Pengfei Sun","Jibin Wu","Malu Zhang","Paul Devos","Dick Botteldooren"],"pdf_url":"https://arxiv.org/pdf/2310.14982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14954v1","updated":"2023-10-23T13:55:49Z","published":"2023-10-23T13:55:49Z","title":"Key Frame Mechanism For Efficient Conformer Based End-to-end Speech\n  Recognition","summary":"  Recently, Conformer as a backbone network for end-to-end automatic speech\nrecognition achieved state-of-the-art performance. The Conformer block\nleverages a self-attention mechanism to capture global information, along with\na convolutional neural network to capture local information, resulting in\nimproved performance. However, the Conformer-based model encounters an issue\nwith the self-attention mechanism, as computational complexity grows\nquadratically with the length of the input sequence. Inspired by previous\nConnectionist Temporal Classification (CTC) guided blank skipping during\ndecoding, we introduce intermediate CTC outputs as guidance into the\ndownsampling procedure of the Conformer encoder. We define the frame with\nnon-blank output as key frame. Specifically, we introduce the key frame-based\nself-attention (KFSA) mechanism, a novel method to reduce the computation of\nthe self-attention mechanism using key frames. The structure of our proposed\napproach comprises two encoders. Following the initial encoder, we introduce an\nintermediate CTC loss function to compute the label frame, enabling us to\nextract the key frames and blank frames for KFSA. Furthermore, we introduce the\nkey frame-based downsampling (KFDS) mechanism to operate on high-dimensional\nacoustic features directly and drop the frames corresponding to blank labels,\nwhich results in new acoustic feature sequences as input to the second encoder.\nBy using the proposed method, which achieves comparable or higher performance\nthan vanilla Conformer and other similar work such as Efficient Conformer.\nMeantime, our proposed method can discard more than 60\\% useless frames during\nmodel training and inference, which will accelerate the inference speed\nsignificantly. This work code is available in\n{https://github.com/scufan1990/Key-Frame-Mechanism-For-Efficient-Conformer}\n","authors":["Peng Fan","Changhao Shan","Jianwei Zhang","Sining Sun","Qing Yang"],"pdf_url":"https://arxiv.org/pdf/2310.14954v1.pdf","comment":"This manuscript has been accepted by IEEE Signal Processing Letters\n  for publication"},{"id":"http://arxiv.org/abs/2310.14952v1","updated":"2023-10-23T13:54:53Z","published":"2023-10-23T13:54:53Z","title":"8+8=4: Formalizing Time Units to Handle Symbolic Music Durations","summary":"  This paper focuses on the nominal durations of musical events (notes and\nrests) in a symbolic musical score, and on how to conveniently handle these in\ncomputer applications. We propose the usage of a temporal unit that is directly\nrelated to the graphical symbols in musical scores and pair this with a set of\noperations that cover typical computations in music applications. We formalize\nthis time unit and the more commonly used approach in a single mathematical\nframework, as semirings, algebraic structures that enable an abstract\ndescription of algorithms/processing pipelines. We then discuss some practical\nuse cases and highlight when our system can improve such pipelines by making\nthem more efficient in terms of data type used and the number of computations.\n","authors":["Emmanouil Karystinaios","Francesco Foscarin","Florent Jacquemard","Masahiko Sakai","Satoshi Tojo","Gerhard Widmer"],"pdf_url":"https://arxiv.org/pdf/2310.14952v1.pdf","comment":"In Proceedings of the International Symposium on Computer Music\n  Multidisciplinary Research (CMMR 2023), Tokyo, Japan"},{"id":"http://arxiv.org/abs/2310.14946v1","updated":"2023-10-23T13:45:21Z","published":"2023-10-23T13:45:21Z","title":"Intuitive Multilingual Audio-Visual Speech Recognition with a\n  Single-Trained Model","summary":"  We present a novel approach to multilingual audio-visual speech recognition\ntasks by introducing a single model on a multilingual dataset. Motivated by a\nhuman cognitive system where humans can intuitively distinguish different\nlanguages without any conscious effort or guidance, we propose a model that can\ncapture which language is given as an input speech by distinguishing the\ninherent similarities and differences between languages. To do so, we design a\nprompt fine-tuning technique into the largely pre-trained audio-visual\nrepresentation model so that the network can recognize the language class as\nwell as the speech with the corresponding language. Our work contributes to\ndeveloping robust and efficient multilingual audio-visual speech recognition\nsystems, reducing the need for language-specific models.\n","authors":["Joanna Hong","Se Jin Park","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2310.14946v1.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2310.14823v1","updated":"2023-10-23T11:42:38Z","published":"2023-10-23T11:42:38Z","title":"Prompt-driven Target Speech Diarization","summary":"  We introduce a novel task named `target speech diarization', which seeks to\ndetermine `when target event occurred' within an audio signal. We devise a\nneural architecture called Prompt-driven Target Speech Diarization (PTSD), that\nworks with diverse prompts that specify the target speech events of interest.\nWe train and evaluate PTSD using sim2spk, sim3spk and sim4spk datasets, which\nare derived from the Librispeech. We show that the proposed framework\naccurately localizes target speech events. Furthermore, our framework exhibits\nversatility through its impressive performance in three diarization-related\ntasks: target speaker voice activity detection, overlapped speech detection and\ngender diarization. In particular, PTSD achieves comparable performance to\nspecialized models across these tasks on both real and simulated data. This\nwork serves as a reference benchmark and provides valuable insights into\nprompt-driven target speech processing.\n","authors":["Yidi Jiang","Zhengyang Chen","Ruijie Tao","Liqun Deng","Yanmin Qian","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2310.14823v1.pdf","comment":"submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.14806v1","updated":"2023-10-23T11:00:27Z","published":"2023-10-23T11:00:27Z","title":"Leveraging Timestamp Information for Serialized Joint Streaming\n  Recognition and Translation","summary":"  The growing need for instant spoken language transcription and translation is\ndriven by increased global communication and cross-lingual interactions. This\nhas made offering translations in multiple languages essential for user\napplications. Traditional approaches to automatic speech recognition (ASR) and\nspeech translation (ST) have often relied on separate systems, leading to\ninefficiencies in computational resources, and increased synchronization\ncomplexity in real time. In this paper, we propose a streaming\nTransformer-Transducer (T-T) model able to jointly produce many-to-one and\none-to-many transcription and translation using a single decoder. We introduce\na novel method for joint token-level serialized output training based on\ntimestamp information to effectively produce ASR and ST outputs in the\nstreaming setting. Experiments on {it,es,de}->en prove the effectiveness of our\napproach, enabling the generation of one-to-many joint outputs with a single\ndecoder for the first time.\n","authors":["Sara Papi","Peidong Wang","Junkun Chen","Jian Xue","Naoyuki Kanda","Jinyu Li","Yashesh Gaur"],"pdf_url":"https://arxiv.org/pdf/2310.14806v1.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2310.14778v1","updated":"2023-10-23T10:29:33Z","published":"2023-10-23T10:29:33Z","title":"Audio-Visual Speaker Tracking: Progress, Challenges, and Future\n  Directions","summary":"  Audio-visual speaker tracking has drawn increasing attention over the past\nfew years due to its academic values and wide application. Audio and visual\nmodalities can provide complementary information for localization and tracking.\nWith audio and visual information, the Bayesian-based filter can solve the\nproblem of data association, audio-visual fusion and track management. In this\npaper, we conduct a comprehensive overview of audio-visual speaker tracking. To\nour knowledge, this is the first extensive survey over the past five years. We\nintroduce the family of Bayesian filters and summarize the methods for\nobtaining audio-visual measurements. In addition, the existing trackers and\ntheir performance on AV16.3 dataset are summarized. In the past few years, deep\nlearning techniques have thrived, which also boosts the development of audio\nvisual speaker tracking. The influence of deep learning techniques in terms of\nmeasurement extraction and state estimation is also discussed. At last, we\ndiscuss the connections between audio-visual speaker tracking and other areas\nsuch as speech separation and distributed speaker tracking.\n","authors":["Jinzheng Zhao","Yong Xu","Xinyuan Qian","Davide Berghi","Peipei Wu","Meng Cui","Jianyuan Sun","Philip J. B. Jackson","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.14778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14663v1","updated":"2023-10-23T07:59:46Z","published":"2023-10-23T07:59:46Z","title":"DPP-TTS: Diversifying prosodic features of speech via determinantal\n  point processes","summary":"  With the rapid advancement in deep generative models, recent neural\nText-To-Speech(TTS) models have succeeded in synthesizing human-like speech.\nThere have been some efforts to generate speech with various prosody beyond\nmonotonous prosody patterns. However, previous works have several limitations.\nFirst, typical TTS models depend on the scaled sampling temperature for\nboosting the diversity of prosody. Speech samples generated at high sampling\ntemperatures often lack perceptual prosodic diversity, which can adversely\naffect the naturalness of the speech. Second, the diversity among samples is\nneglected since the sampling procedure often focuses on a single speech sample\nrather than multiple ones. In this paper, we propose DPP-TTS: a text-to-speech\nmodel based on Determinantal Point Processes (DPPs) with a prosody diversifying\nmodule. Our TTS model is capable of generating speech samples that\nsimultaneously consider perceptual diversity in each sample and among multiple\nsamples. We demonstrate that DPP-TTS generates speech samples with more\ndiversified prosody than baselines in the side-by-side comparison test\nconsidering the naturalness of speech at the same time.\n","authors":["Seongho Joo","Hyukhun Koh","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2310.14663v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.14580v1","updated":"2023-10-23T05:38:41Z","published":"2023-10-23T05:38:41Z","title":"Acoustic BPE for Speech Generation with Discrete Tokens","summary":"  Discrete audio tokens derived from self-supervised learning models have\ngained widespread usage in speech generation. However, current practice of\ndirectly utilizing audio tokens poses challenges for sequence modeling due to\nthe length of the token sequence. Additionally, this approach places the burden\non the model to establish correlations between tokens, further complicating the\nmodeling process. To address this issue, we propose acoustic BPE which encodes\nfrequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE\neffectively reduces the sequence length and leverages the prior morphological\ninformation present in token sequence, which alleviates the modeling challenges\nof token correlation. Through comprehensive investigations on a speech language\nmodel trained with acoustic BPE, we confirm the notable advantages it offers,\nincluding faster inference and improved syntax capturing capabilities. In\naddition, we propose a novel rescore method to select the optimal synthetic\nspeech among multiple candidates generated by rich-diversity TTS system.\nExperiments prove that rescore selection aligns closely with human preference,\nwhich highlights acoustic BPE's potential to other speech generation tasks.\n","authors":["Feiyu Shen","Yiwei Guo","Chenpeng Du","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2310.14580v1.pdf","comment":"5 pages, 2 figures; submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.15399v1","updated":"2023-10-23T23:01:33Z","published":"2023-10-23T23:01:33Z","title":"GESI: Gammachirp Envelope Similarity Index for Predicting\n  Intelligibility of Simulated Hearing Loss Sounds","summary":"  We proposed a new objective intelligibility measure (OIM), called the\nGammachirp Envelope Similarity Index (GESI), which can predict the speech\nintelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing\n(NH) listeners. GESI is an intrusive method that computes the SI metric using\nthe gammachirp filterbank (GCFB), the modulation filterbank, and the extended\ncosine similarity measure. GESI can accept the level asymmetry of the reference\nand test sounds and reflect the HI listener's hearing level as it appears on\nthe audiogram. A unique feature of GESI is its ability to incorporate an\nindividual participant's listening condition into the SI prediction. We\nconducted four SI experiments on male and female speech sounds in both\nlaboratory and crowdsourced remote environments. We then evaluated GESI and the\nconventional OIMs, STOI, ESTOI, MBSTOI, and HASPI, for their ability to predict\nmean and individual SI values with and without the use of simulated HL sounds.\nGESI outperformed the other OIMs in all evaluations. STOI, ESTOI, and MBSTOI\ndid not predict SI at all, even when using the simulated HL sounds. HASPI did\nnot predict the difference between the laboratory and remote experiments on\nmale speech sounds and the individual SI values. GESI may provide a first step\ntoward SI prediction for individual HI listeners whose HL is caused solely by\nperipheral dysfunction.\n","authors":["Ayako Yamamoto","Toshio Irino","Fuki Miyazaki","Honoka Tamaru"],"pdf_url":"https://arxiv.org/pdf/2310.15399v1.pdf","comment":"This paper was submitted to Speech Communication on September 14,\n  2023"},{"id":"http://arxiv.org/abs/2301.11757v3","updated":"2023-10-23T20:47:30Z","published":"2023-01-27T14:52:53Z","title":"Mo√ªsai: Text-to-Music Generation with Long-Context Latent Diffusion","summary":"  Recent years have seen the rapid development of large generative models for\ntext; however, much less research has explored the connection between text and\nanother \"language\" of communication -- music. Music, much like text, can convey\nemotions, stories, and ideas, and has its own unique structure and syntax. In\nour work, we bridge text and music via a text-to-music generation model that is\nhighly efficient, expressive, and can handle long-term structure. Specifically,\nwe develop Mo\\^usai, a cascading two-stage latent diffusion model that can\ngenerate multiple minutes of high-quality stereo music at 48kHz from textual\ndescriptions. Moreover, our model features high efficiency, which enables\nreal-time inference on a single consumer GPU with a reasonable speed. Through\nexperiments and property analyses, we show our model's competence over a\nvariety of criteria compared with existing music generation models. Lastly, to\npromote the open-source culture, we provide a collection of open-source\nlibraries with the hope of facilitating future work in the field. We\nopen-source the following: Codes:\nhttps://github.com/archinetai/audio-diffusion-pytorch; music samples for this\npaper: http://bit.ly/44ozWDH; all music samples for all models:\nhttps://bit.ly/audio-diffusion.\n","authors":["Flavio Schneider","Ojasv Kamal","Zhijing Jin","Bernhard Sch√∂lkopf"],"pdf_url":"https://arxiv.org/pdf/2301.11757v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15261v1","updated":"2023-10-23T18:09:31Z","published":"2023-10-23T18:09:31Z","title":"Modality Dropout for Multimodal Device Directed Speech Detection using\n  Verbal and Non-Verbal Features","summary":"  Device-directed speech detection (DDSD) is the binary classification task of\ndistinguishing between queries directed at a voice assistant versus side\nconversation or background speech. State-of-the-art DDSD systems use verbal\ncues, e.g acoustic, text and/or automatic speech recognition system (ASR)\nfeatures, to classify speech as device-directed or otherwise, and often have to\ncontend with one or more of these modalities being unavailable when deployed in\nreal-world settings. In this paper, we investigate fusion schemes for DDSD\nsystems that can be made more robust to missing modalities. Concurrently, we\nstudy the use of non-verbal cues, specifically prosody features, in addition to\nverbal cues for DDSD. We present different approaches to combine scores and\nembeddings from prosody with the corresponding verbal cues, finding that\nprosody improves DDSD performance by upto 8.5% in terms of false acceptance\nrate (FA) at a given fixed operating point via non-linear intermediate fusion,\nwhile our use of modality dropout techniques improves the performance of these\nmodels by 7.4% in terms of FA when evaluated with missing modalities during\ninference time.\n","authors":["Gautam Krishna","Sameer Dharur","Oggi Rudovic","Pranay Dighe","Saurabh Adya","Ahmed Hussen Abdelaziz","Ahmed H Tewfik"],"pdf_url":"https://arxiv.org/pdf/2310.15261v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2310.15247v1","updated":"2023-10-23T18:01:36Z","published":"2023-10-23T18:01:36Z","title":"SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis","summary":"  Sound design involves creatively selecting, recording, and editing sound\neffects for various media like cinema, video games, and virtual/augmented\nreality. One of the most time-consuming steps when designing sound is\nsynchronizing audio with video. In some cases, environmental recordings from\nvideo shoots are available, which can aid in the process. However, in video\ngames and animations, no reference audio exists, requiring manual annotation of\nevent timings from the video. We propose a system to extract repetitive actions\nonsets from a video, which are then used - in conjunction with audio or textual\nembeddings - to condition a diffusion model trained to generate a new\nsynchronized sound effects audio track. In this way, we leave complete creative\ncontrol to the sound designer while removing the burden of synchronization with\nvideo. Furthermore, editing the onset track or changing the conditioning\nembedding requires much less effort than editing the audio track itself,\nsimplifying the sonification process. We provide sound examples, source code,\nand pretrained models to faciliate reproducibility\n","authors":["Marco Comunit√†","Riccardo F. Gramaccioni","Emilian Postolache","Emanuele Rodol√†","Danilo Comminiello","Joshua D. Reiss"],"pdf_url":"https://arxiv.org/pdf/2310.15247v1.pdf","comment":null}]},"2023-10-24T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2310.14270v2","updated":"2023-10-24T13:07:58Z","published":"2023-10-22T11:34:30Z","title":"Diffusion-Based Adversarial Purification for Speaker Verification","summary":"  Recently, automatic speaker verification (ASV) based on deep learning is\neasily contaminated by adversarial attacks, which is a new type of attack that\ninjects imperceptible perturbations to audio signals so as to make ASV produce\nwrong decisions. This poses a significant threat to the security and\nreliability of ASV systems. To address this issue, we propose a Diffusion-Based\nAdversarial Purification (DAP) method that enhances the robustness of ASV\nsystems against such adversarial attacks. Our method leverages a conditional\ndenoising diffusion probabilistic model to effectively purify the adversarial\nexamples and mitigate the impact of perturbations. DAP first introduces\ncontrolled noise into adversarial examples, and then performs a reverse\ndenoising process to reconstruct clean audio. Experimental results demonstrate\nthe efficacy of the proposed DAP in enhancing the security of ASV and meanwhile\nminimizing the distortion of the purified audio signals.\n","authors":["Yibo Bai","Xiao-Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.14270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05374v3","updated":"2023-10-24T06:48:55Z","published":"2023-10-09T03:10:49Z","title":"Improving End-to-End Speech Processing by Efficient Text Data\n  Utilization with Latent Synthesis","summary":"  Training a high performance end-to-end speech (E2E) processing model requires\nan enormous amount of labeled speech data, especially in the era of\ndata-centric artificial intelligence. However, labeled speech data are usually\nscarcer and more expensive for collection, compared to textual data. We propose\nLatent Synthesis (LaSyn), an efficient textual data utilization framework for\nE2E speech processing models. We train a latent synthesizer to convert textual\ndata into an intermediate latent representation of a pre-trained speech model.\nThese pseudo acoustic representations of textual data augment acoustic data for\nmodel training. We evaluate LaSyn on low-resource automatic speech recognition\n(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an\nE2E baseline trained on LibriSpeech train-clean-100, with relative word error\nrate reductions over 22.3% on different test sets. For SLU, LaSyn improves our\nE2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for\nslot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)\nand EM-Tree accuracies on STOP respectively. With fewer parameters, the results\nof LaSyn are competitive to published state-of-the-art works. The results\ndemonstrate the quality of the augmented training data.\n","authors":["Jianqiao Lu","Wenyong Huang","Nianzu Zheng","Xingshan Zeng","Yu Ting Yeung","Xiao Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05374v3.pdf","comment":"15 pages, 8 figures, 8 tables, Accepted to EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2307.14491v2","updated":"2023-10-24T17:48:24Z","published":"2023-07-26T20:30:34Z","title":"A Unified Framework for Modality-Agnostic Deepfakes Detection","summary":"  As AI-generated content (AIGC) thrives, deepfakes have expanded from\nsingle-modality falsification to cross-modal fake content creation, where\neither audio or visual components can be manipulated. While using two unimodal\ndetectors can detect audio-visual deepfakes, cross-modal forgery clues could be\noverlooked. Existing multimodal deepfake detection methods typically establish\ncorrespondence between the audio and visual modalities for binary real/fake\nclassification, and require the co-occurrence of both modalities. However, in\nreal-world multi-modal applications, missing modality scenarios may occur where\neither modality is unavailable. In such cases, audio-visual detection methods\nare less practical than two independent unimodal methods. Consequently, the\ndetector can not always obtain the number or type of manipulated modalities\nbeforehand, necessitating a fake-modality-agnostic audio-visual detector. In\nthis work, we introduce a comprehensive framework that is agnostic to fake\nmodalities, which facilitates the identification of multimodal deepfakes and\nhandles situations with missing modalities, regardless of the manipulations\nembedded in audio, video, or even cross-modal forms. To enhance the modeling of\ncross-modal forgery clues, we employ audio-visual speech recognition (AVSR) as\na preliminary task. This efficiently extracts speech correlations across\nmodalities, a feature challenging for deepfakes to replicate. Additionally, we\npropose a dual-label detection approach that follows the structure of AVSR to\nsupport the independent detection of each modality. Extensive experiments on\nthree audio-visual datasets show that our scheme outperforms state-of-the-art\ndetection methods with promising performance on modality-agnostic audio/video\ndeepfakes.\n","authors":["Cai Yu","Peng Chen","Jiahe Tian","Jin Liu","Jiao Dai","Xi Wang","Yesheng Chai","Shan Jia","Siwei Lyu","Jizhong Han"],"pdf_url":"https://arxiv.org/pdf/2307.14491v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2309.08030v2","updated":"2023-10-24T15:43:11Z","published":"2023-09-14T21:07:53Z","title":"AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised\n  Features for Audio-Visual Speech Enhancement","summary":"  Speech enhancement systems are typically trained using pairs of clean and\nnoisy speech. In audio-visual speech enhancement (AVSE), there is not as much\nground-truth clean data available; most audio-visual datasets are collected in\nreal-world environments with background noise and reverberation, hampering the\ndevelopment of AVSE. In this work, we introduce AV2Wav, a resynthesis-based\naudio-visual speech enhancement approach that can generate clean speech despite\nthe challenges of real-world training data. We obtain a subset of nearly clean\nspeech from an audio-visual corpus using a neural quality estimator, and then\ntrain a diffusion model on this subset to generate waveforms conditioned on\ncontinuous speech representations from AV-HuBERT with noise-robust training. We\nuse continuous rather than discrete representations to retain prosody and\nspeaker information. With this vocoding task alone, the model can perform\nspeech enhancement better than a masking-based baseline. We further fine-tune\nthe diffusion model on clean/noisy utterance pairs to improve the performance.\nOur approach outperforms a masking-based baseline in terms of both automatic\nmetrics and a human listening test and is close in quality to the target speech\nin the listening test. Audio samples can be found at\nhttps://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.\n","authors":["Ju-Chieh Chou","Chung-Ming Chien","Karen Livescu"],"pdf_url":"https://arxiv.org/pdf/2309.08030v2.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.15930v1","updated":"2023-10-24T15:27:50Z","published":"2023-10-24T15:27:50Z","title":"CDSD: Chinese Dysarthria Speech Database","summary":"  We present the Chinese Dysarthria Speech Database (CDSD) as a valuable\nresource for dysarthria research. This database comprises speech data from 24\nparticipants with dysarthria. Among these participants, one recorded an\nadditional 10 hours of speech data, while each recorded one hour, resulting in\n34 hours of speech material. To accommodate participants with varying cognitive\nlevels, our text pool primarily consists of content from the AISHELL-1 dataset\nand speeches by primary and secondary school students. When participants read\nthese texts, they must use a mobile device or the ZOOM F8n multi-track field\nrecorder to record their speeches. In this paper, we elucidate the data\ncollection and annotation processes and present an approach for establishing a\nbaseline for dysarthric speech recognition. Furthermore, we conducted a\nspeaker-dependent dysarthric speech recognition experiment using an additional\n10 hours of speech data from one of our participants. Our research findings\nindicate that, through extensive data-driven model training, fine-tuning\nlimited quantities of specific individual data yields commendable results in\nspeaker-dependent dysarthric speech recognition. However, we observe\nsignificant variations in recognition results among different dysarthric\nspeakers. These insights provide valuable reference points for\nspeaker-dependent dysarthric speech recognition.\n","authors":["Mengyi Sun","Ming Gao","Xinchen Kang","Shiru Wang","Jun Du","Dengfeng Yao","Su-Jing Wang"],"pdf_url":"https://arxiv.org/pdf/2310.15930v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.15672v1","updated":"2023-10-24T09:31:03Z","published":"2023-10-24T09:31:03Z","title":"How Much Context Does My Attention-Based ASR System Need?","summary":"  For the task of speech recognition, the use of more than 30 seconds of\nacoustic context during training is uncommon, and under-investigated in\nliterature. In this work, we examine the effect of scaling the sequence length\nused to train/evaluate (dense-attention based) acoustic and language models on\nspeech recognition performance. For these experiments a dataset of roughly\n100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5\nseconds to 1 hour being explored. Zero-shot evaluations on long-format datasets\nEarnings-22 and Tedlium demonstrate a benefit from training with around 80\nseconds of acoustic context, showing up to a 14.9% relative improvement from a\nlimited context baseline. Furthermore, we perform a system combination with\nlong-context transformer language models via beam search for a fully\nlong-context ASR system, with results that are competitive with the current\nstate-of-the-art.\n","authors":["Robert Flynn","Anton Ragni"],"pdf_url":"https://arxiv.org/pdf/2310.15672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15663v1","updated":"2023-10-24T09:21:42Z","published":"2023-10-24T09:21:42Z","title":"FOLEY-VAE: Generaci√≥n de efectos de audio para cine con inteligencia\n  artificial","summary":"  In this research, we present an interface based on Variational Autoencoders\ntrained with a wide range of natural sounds for the innovative creation of\nFoley effects. The model can transfer new sound features to prerecorded audio\nor microphone-captured speech in real time. In addition, it allows interactive\nmodification of latent variables, facilitating precise and customized artistic\nadjustments. Taking as a starting point our previous study on Variational\nAutoencoders presented at this same congress last year, we analyzed an existing\nimplementation: RAVE [1]. This model has been specifically trained for audio\neffects production. Various audio effects have been successfully generated,\nranging from electromagnetic, science fiction, and water sounds, among others\npublished with this work. This innovative approach has been the basis for the\nartistic creation of the first Spanish short film with sound effects assisted\nby artificial intelligence. This milestone illustrates palpably the\ntransformative potential of this technology in the film industry, opening the\ndoor to new possibilities for sound creation and the improvement of artistic\nquality in film productions.\n","authors":["Mateo C√°mara","Jos√© Luis Blanco"],"pdf_url":"https://arxiv.org/pdf/2310.15663v1.pdf","comment":"9 pages, in Spanish, Tecniac\\'ustica"},{"id":"http://arxiv.org/abs/2310.15648v1","updated":"2023-10-24T09:08:20Z","published":"2023-10-24T09:08:20Z","title":"Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio\n  Models","summary":"  The introduction of large-scale audio datasets, such as AudioSet, paved the\nway for Transformers to conquer the audio domain and replace CNNs as the\nstate-of-the-art neural network architecture for many tasks. Audio Spectrogram\nTransformers are excellent at exploiting large datasets, creating powerful\npre-trained models that surpass CNNs when fine-tuned on downstream tasks.\nHowever, current popular Audio Spectrogram Transformers are demanding in terms\nof computational complexity compared to CNNs. Recently, we have shown that, by\nemploying Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch\nup with and even outperform Transformers on large datasets. In this work, we\nextend this line of research and increase the capacity of efficient CNNs by\nintroducing dynamic CNN blocks, constructed of dynamic non-linearities, dynamic\nconvolutions and attention mechanisms. We show that these dynamic CNNs\noutperform traditional efficient CNNs, in terms of the performance-complexity\ntrade-off and parameter efficiency, at the task of audio tagging on the\nlarge-scale AudioSet. Our experiments further indicate that the introduced\ndynamic CNNs achieve better performance on downstream tasks and scale up well,\nattaining Transformer performance and even outperforming them on AudioSet and\nseveral downstream tasks.\n","authors":["Florian Schmid","Khaled Koutini","Gerhard Widmer"],"pdf_url":"https://arxiv.org/pdf/2310.15648v1.pdf","comment":"Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing. Source Code available at:\n  https://github.com/fschmid56/EfficientAT"},{"id":"http://arxiv.org/abs/2310.15425v1","updated":"2023-10-24T00:43:54Z","published":"2023-10-24T00:43:54Z","title":"The Mason-Alberta Phonetic Segmenter: A forced alignment system based on\n  deep neural networks and interpolation","summary":"  Forced alignment systems automatically determine boundaries between segments\nin speech data, given an orthographic transcription. These tools are\ncommonplace in phonetics to facilitate the use of speech data that would be\ninfeasible to manually transcribe and segment. In the present paper, we\ndescribe a new neural network-based forced alignment system, the Mason-Alberta\nPhonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two\npossible improvements we pursue for forced alignment systems. The first is\ntreating the acoustic model in a forced aligner as a tagging task, rather than\na classification task, motivated by the common understanding that segments in\nspeech are not truly discrete and commonly overlap. The second is an\ninterpolation technique to allow boundaries more precise than the common 10 ms\nlimit in modern forced alignment systems. We compare configurations of our\nsystem to a state-of-the-art system, the Montreal Forced Aligner. The tagging\napproach did not generally yield improved results over the Montreal Forced\nAligner. However, a system with the interpolation technique had a 27.92%\nincrease relative to the Montreal Forced Aligner in the amount of boundaries\nwithin 10 ms of the target on the test set. We also reflect on the task and\ntraining process for acoustic modeling in forced alignment, highlighting how\nthe output targets for these models do not match phoneticians' conception of\nsimilarity between phones and that reconciliation of this tension may require\nrethinking the task and output targets or how speech itself should be\nsegmented.\n","authors":["Matthew C. Kelley","Scott James Perry","Benjamin V. Tucker"],"pdf_url":"https://arxiv.org/pdf/2310.15425v1.pdf","comment":"submitted for publication"},{"id":"http://arxiv.org/abs/2301.08730v3","updated":"2023-10-24T20:19:51Z","published":"2023-01-20T18:49:58Z","title":"Novel-View Acoustic Synthesis","summary":"  We introduce the novel-view acoustic synthesis (NVAS) task: given the sight\nand sound observed at a source viewpoint, can we synthesize the sound of that\nscene from an unseen target viewpoint? We propose a neural rendering approach:\nVisually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize\nthe sound of an arbitrary point in space by analyzing the input audio-visual\ncues. To benchmark this task, we collect two first-of-their-kind large-scale\nmulti-view audio-visual datasets, one synthetic and one real. We show that our\nmodel successfully reasons about the spatial cues and synthesizes faithful\naudio on both datasets. To our knowledge, this work represents the very first\nformulation, dataset, and approach to solve the novel-view acoustic synthesis\ntask, which has exciting potential applications ranging from AR/VR to art and\ndesign. Unlocked by this work, we believe that the future of novel-view\nsynthesis is in multi-modal learning from videos.\n","authors":["Changan Chen","Alexander Richard","Roman Shapovalov","Vamsi Krishna Ithapu","Natalia Neverova","Kristen Grauman","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2301.08730v3.pdf","comment":"Accepted at CVPR 2023. Project page:\n  https://vision.cs.utexas.edu/projects/nvas"},{"id":"http://arxiv.org/abs/2310.16140v1","updated":"2023-10-24T19:27:50Z","published":"2023-10-24T19:27:50Z","title":"IA Para el Mantenimiento Predictivo en Canteras: Modelado","summary":"  Dependence on raw materials, especially in the mining sector, is a key part\nof today's economy. Aggregates are vital, being the second most used raw\nmaterial after water. Digitally transforming this sector is key to optimizing\noperations. However, supervision and maintenance (predictive and corrective)\nare challenges little explored in this sector, due to the particularities of\nthe sector, machinery and environmental conditions. All this, despite the\nsuccesses achieved in other scenarios in monitoring with acoustic and contact\nsensors. We present an unsupervised learning scheme that trains a variational\nautoencoder model on a set of sound records. This is the first such dataset\ncollected during processing plant operations, containing information from\ndifferent points of the processing line. Our results demonstrate the model's\nability to reconstruct and represent in latent space the recorded sounds, the\ndifferences in operating conditions and between different equipment. In the\nfuture, this should facilitate the classification of sounds, as well as the\ndetection of anomalies and degradation patterns in the operation of the\nmachinery.\n","authors":["Fernando Marcos","Rodrigo Tamaki","Mateo C√°mara","Virginia Yag√ºe","Jos√© Luis Blanco"],"pdf_url":"https://arxiv.org/pdf/2310.16140v1.pdf","comment":"10 pages, in Spanish language, 5 figures. Presented in Tecniacustica\n  2023 conference (Cuenca, Spain)"},{"id":"http://arxiv.org/abs/2309.13876v3","updated":"2023-10-24T18:28:22Z","published":"2023-09-25T05:01:34Z","title":"Reproducing Whisper-Style Training Using an Open-Source Toolkit and\n  Publicly Available Data","summary":"  Pre-training speech models on large volumes of data has achieved remarkable\nsuccess. OpenAI Whisper is a multilingual multitask model trained on 680k hours\nof supervised speech data. It generalizes well to various speech recognition\nand translation benchmarks even in a zero-shot setup. However, the full\npipeline for developing such models (from data collection to training) is not\npublicly accessible, which makes it difficult for researchers to further\nimprove its performance and address training-related issues such as efficiency,\nrobustness, fairness, and bias. This work presents an Open Whisper-style Speech\nModel (OWSM), which reproduces Whisper-style training using an open-source\ntoolkit and publicly available data. OWSM even supports more translation\ndirections and can be more efficient to train. We will publicly release all\nscripts used for data preparation, training, inference, and scoring as well as\npre-trained models and training logs to promote open science.\n","authors":["Yifan Peng","Jinchuan Tian","Brian Yan","Dan Berrebbi","Xuankai Chang","Xinjian Li","Jiatong Shi","Siddhant Arora","William Chen","Roshan Sharma","Wangyou Zhang","Yui Sudo","Muhammad Shakeel","Jee-weon Jung","Soumi Maiti","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2309.13876v3.pdf","comment":"Accepted at ASRU 2023"},{"id":"http://arxiv.org/abs/2310.16109v1","updated":"2023-10-24T18:21:03Z","published":"2023-10-24T18:21:03Z","title":"Complex Image Generation SwinTransformer Network for Audio Denoising","summary":"  Achieving high-performance audio denoising is still a challenging task in\nreal-world applications. Existing time-frequency methods often ignore the\nquality of generated frequency domain images. This paper converts the audio\ndenoising problem into an image generation task. We first develop a complex\nimage generation SwinTransformer network to capture more information from the\ncomplex Fourier domain. We then impose structure similarity and detailed loss\nfunctions to generate high-quality images and develop an SDR loss to minimize\nthe difference between denoised and clean audios. Extensive experiments on two\nbenchmark datasets demonstrate that our proposed model is better than\nstate-of-the-art methods.\n","authors":["Youshan Zhang","Jialu Li"],"pdf_url":"https://arxiv.org/pdf/2310.16109v1.pdf","comment":null}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.14654v2","updated":"2023-10-24T06:03:14Z","published":"2023-10-23T07:50:10Z","title":"SPRING-INX: A Multilingual Indian Language Speech Corpus by SPRING Lab,\n  IIT Madras","summary":"  India is home to a multitude of languages of which 22 languages are\nrecognised by the Indian Constitution as official. Building speech based\napplications for the Indian population is a difficult problem owing to limited\ndata and the number of languages and accents to accommodate. To encourage the\nlanguage technology community to build speech based applications in Indian\nlanguages, we are open sourcing SPRING-INX data which has about 2000 hours of\nlegally sourced and manually transcribed speech data for ASR system building in\nAssamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi\nand Tamil. This endeavor is by SPRING Lab , Indian Institute of Technology\nMadras and is a part of National Language Translation Mission (NLTM), funded by\nthe Indian Ministry of Electronics and Information Technology (MeitY),\nGovernment of India. We describe the data collection and data cleaning process\nalong with the data statistics in this paper.\n","authors":["Nithya R","Malavika S","Jordan F","Arjun Gangwar","Metilda N J","S Umesh","Rithik Sarab","Akhilesh Kumar Dubey","Govind Divakaran","Samudra Vijaya K","Suryakanth V Gangashetty"],"pdf_url":"https://arxiv.org/pdf/2310.14654v2.pdf","comment":"3 pages, About SPRING-INX Data"},{"id":"http://arxiv.org/abs/2310.14270v2","updated":"2023-10-24T13:07:58Z","published":"2023-10-22T11:34:30Z","title":"Diffusion-Based Adversarial Purification for Speaker Verification","summary":"  Recently, automatic speaker verification (ASV) based on deep learning is\neasily contaminated by adversarial attacks, which is a new type of attack that\ninjects imperceptible perturbations to audio signals so as to make ASV produce\nwrong decisions. This poses a significant threat to the security and\nreliability of ASV systems. To address this issue, we propose a Diffusion-Based\nAdversarial Purification (DAP) method that enhances the robustness of ASV\nsystems against such adversarial attacks. Our method leverages a conditional\ndenoising diffusion probabilistic model to effectively purify the adversarial\nexamples and mitigate the impact of perturbations. DAP first introduces\ncontrolled noise into adversarial examples, and then performs a reverse\ndenoising process to reconstruct clean audio. Experimental results demonstrate\nthe efficacy of the proposed DAP in enhancing the security of ASV and meanwhile\nminimizing the distortion of the purified audio signals.\n","authors":["Yibo Bai","Xiao-Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.14270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05374v3","updated":"2023-10-24T06:48:55Z","published":"2023-10-09T03:10:49Z","title":"Improving End-to-End Speech Processing by Efficient Text Data\n  Utilization with Latent Synthesis","summary":"  Training a high performance end-to-end speech (E2E) processing model requires\nan enormous amount of labeled speech data, especially in the era of\ndata-centric artificial intelligence. However, labeled speech data are usually\nscarcer and more expensive for collection, compared to textual data. We propose\nLatent Synthesis (LaSyn), an efficient textual data utilization framework for\nE2E speech processing models. We train a latent synthesizer to convert textual\ndata into an intermediate latent representation of a pre-trained speech model.\nThese pseudo acoustic representations of textual data augment acoustic data for\nmodel training. We evaluate LaSyn on low-resource automatic speech recognition\n(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an\nE2E baseline trained on LibriSpeech train-clean-100, with relative word error\nrate reductions over 22.3% on different test sets. For SLU, LaSyn improves our\nE2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for\nslot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)\nand EM-Tree accuracies on STOP respectively. With fewer parameters, the results\nof LaSyn are competitive to published state-of-the-art works. The results\ndemonstrate the quality of the augmented training data.\n","authors":["Jianqiao Lu","Wenyong Huang","Nianzu Zheng","Xingshan Zeng","Yu Ting Yeung","Xiao Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05374v3.pdf","comment":"15 pages, 8 figures, 8 tables, Accepted to EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2307.14491v2","updated":"2023-10-24T17:48:24Z","published":"2023-07-26T20:30:34Z","title":"A Unified Framework for Modality-Agnostic Deepfakes Detection","summary":"  As AI-generated content (AIGC) thrives, deepfakes have expanded from\nsingle-modality falsification to cross-modal fake content creation, where\neither audio or visual components can be manipulated. While using two unimodal\ndetectors can detect audio-visual deepfakes, cross-modal forgery clues could be\noverlooked. Existing multimodal deepfake detection methods typically establish\ncorrespondence between the audio and visual modalities for binary real/fake\nclassification, and require the co-occurrence of both modalities. However, in\nreal-world multi-modal applications, missing modality scenarios may occur where\neither modality is unavailable. In such cases, audio-visual detection methods\nare less practical than two independent unimodal methods. Consequently, the\ndetector can not always obtain the number or type of manipulated modalities\nbeforehand, necessitating a fake-modality-agnostic audio-visual detector. In\nthis work, we introduce a comprehensive framework that is agnostic to fake\nmodalities, which facilitates the identification of multimodal deepfakes and\nhandles situations with missing modalities, regardless of the manipulations\nembedded in audio, video, or even cross-modal forms. To enhance the modeling of\ncross-modal forgery clues, we employ audio-visual speech recognition (AVSR) as\na preliminary task. This efficiently extracts speech correlations across\nmodalities, a feature challenging for deepfakes to replicate. Additionally, we\npropose a dual-label detection approach that follows the structure of AVSR to\nsupport the independent detection of each modality. Extensive experiments on\nthree audio-visual datasets show that our scheme outperforms state-of-the-art\ndetection methods with promising performance on modality-agnostic audio/video\ndeepfakes.\n","authors":["Cai Yu","Peng Chen","Jiahe Tian","Jin Liu","Jiao Dai","Xi Wang","Yesheng Chai","Shan Jia","Siwei Lyu","Jizhong Han"],"pdf_url":"https://arxiv.org/pdf/2307.14491v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2309.08030v2","updated":"2023-10-24T15:43:11Z","published":"2023-09-14T21:07:53Z","title":"AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised\n  Features for Audio-Visual Speech Enhancement","summary":"  Speech enhancement systems are typically trained using pairs of clean and\nnoisy speech. In audio-visual speech enhancement (AVSE), there is not as much\nground-truth clean data available; most audio-visual datasets are collected in\nreal-world environments with background noise and reverberation, hampering the\ndevelopment of AVSE. In this work, we introduce AV2Wav, a resynthesis-based\naudio-visual speech enhancement approach that can generate clean speech despite\nthe challenges of real-world training data. We obtain a subset of nearly clean\nspeech from an audio-visual corpus using a neural quality estimator, and then\ntrain a diffusion model on this subset to generate waveforms conditioned on\ncontinuous speech representations from AV-HuBERT with noise-robust training. We\nuse continuous rather than discrete representations to retain prosody and\nspeaker information. With this vocoding task alone, the model can perform\nspeech enhancement better than a masking-based baseline. We further fine-tune\nthe diffusion model on clean/noisy utterance pairs to improve the performance.\nOur approach outperforms a masking-based baseline in terms of both automatic\nmetrics and a human listening test and is close in quality to the target speech\nin the listening test. Audio samples can be found at\nhttps://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.\n","authors":["Ju-Chieh Chou","Chung-Ming Chien","Karen Livescu"],"pdf_url":"https://arxiv.org/pdf/2309.08030v2.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.15930v1","updated":"2023-10-24T15:27:50Z","published":"2023-10-24T15:27:50Z","title":"CDSD: Chinese Dysarthria Speech Database","summary":"  We present the Chinese Dysarthria Speech Database (CDSD) as a valuable\nresource for dysarthria research. This database comprises speech data from 24\nparticipants with dysarthria. Among these participants, one recorded an\nadditional 10 hours of speech data, while each recorded one hour, resulting in\n34 hours of speech material. To accommodate participants with varying cognitive\nlevels, our text pool primarily consists of content from the AISHELL-1 dataset\nand speeches by primary and secondary school students. When participants read\nthese texts, they must use a mobile device or the ZOOM F8n multi-track field\nrecorder to record their speeches. In this paper, we elucidate the data\ncollection and annotation processes and present an approach for establishing a\nbaseline for dysarthric speech recognition. Furthermore, we conducted a\nspeaker-dependent dysarthric speech recognition experiment using an additional\n10 hours of speech data from one of our participants. Our research findings\nindicate that, through extensive data-driven model training, fine-tuning\nlimited quantities of specific individual data yields commendable results in\nspeaker-dependent dysarthric speech recognition. However, we observe\nsignificant variations in recognition results among different dysarthric\nspeakers. These insights provide valuable reference points for\nspeaker-dependent dysarthric speech recognition.\n","authors":["Mengyi Sun","Ming Gao","Xinchen Kang","Shiru Wang","Jun Du","Dengfeng Yao","Su-Jing Wang"],"pdf_url":"https://arxiv.org/pdf/2310.15930v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.15845v1","updated":"2023-10-24T13:57:55Z","published":"2023-10-24T13:57:55Z","title":"Pre-training Music Classification Models via Music Source Separation","summary":"  In this paper, we study whether music source separation can be used as a\npre-training strategy for music representation learning, targeted at music\nclassification tasks. To this end, we first pre-train U-Net networks under\nvarious music source separation objectives, such as the isolation of vocal or\ninstrumental sources from a musical piece; afterwards, we attach a\nconvolutional tail network to the pre-trained U-Net and jointly finetune the\nwhole network. The features learned by the separation network are also\npropagated to the tail network through skip connections. Experimental results\nin two widely used and publicly available datasets indicate that pre-training\nthe U-Nets with a music source separation objective can improve performance\ncompared to both training the whole network from scratch and using the tail\nnetwork as a standalone in two music classification tasks: music auto-tagging,\nwhen vocal separation is used, and music genre classification for the case of\nmulti-source separation.\n","authors":["Christos Garoufis","Athanasia Zlatintsi","Petros Maragos"],"pdf_url":"https://arxiv.org/pdf/2310.15845v1.pdf","comment":"5 pages (4+references), 3 figures. ICASSP-24 submission"},{"id":"http://arxiv.org/abs/2310.15672v1","updated":"2023-10-24T09:31:03Z","published":"2023-10-24T09:31:03Z","title":"How Much Context Does My Attention-Based ASR System Need?","summary":"  For the task of speech recognition, the use of more than 30 seconds of\nacoustic context during training is uncommon, and under-investigated in\nliterature. In this work, we examine the effect of scaling the sequence length\nused to train/evaluate (dense-attention based) acoustic and language models on\nspeech recognition performance. For these experiments a dataset of roughly\n100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5\nseconds to 1 hour being explored. Zero-shot evaluations on long-format datasets\nEarnings-22 and Tedlium demonstrate a benefit from training with around 80\nseconds of acoustic context, showing up to a 14.9% relative improvement from a\nlimited context baseline. Furthermore, we perform a system combination with\nlong-context transformer language models via beam search for a fully\nlong-context ASR system, with results that are competitive with the current\nstate-of-the-art.\n","authors":["Robert Flynn","Anton Ragni"],"pdf_url":"https://arxiv.org/pdf/2310.15672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15663v1","updated":"2023-10-24T09:21:42Z","published":"2023-10-24T09:21:42Z","title":"FOLEY-VAE: Generaci√≥n de efectos de audio para cine con inteligencia\n  artificial","summary":"  In this research, we present an interface based on Variational Autoencoders\ntrained with a wide range of natural sounds for the innovative creation of\nFoley effects. The model can transfer new sound features to prerecorded audio\nor microphone-captured speech in real time. In addition, it allows interactive\nmodification of latent variables, facilitating precise and customized artistic\nadjustments. Taking as a starting point our previous study on Variational\nAutoencoders presented at this same congress last year, we analyzed an existing\nimplementation: RAVE [1]. This model has been specifically trained for audio\neffects production. Various audio effects have been successfully generated,\nranging from electromagnetic, science fiction, and water sounds, among others\npublished with this work. This innovative approach has been the basis for the\nartistic creation of the first Spanish short film with sound effects assisted\nby artificial intelligence. This milestone illustrates palpably the\ntransformative potential of this technology in the film industry, opening the\ndoor to new possibilities for sound creation and the improvement of artistic\nquality in film productions.\n","authors":["Mateo C√°mara","Jos√© Luis Blanco"],"pdf_url":"https://arxiv.org/pdf/2310.15663v1.pdf","comment":"9 pages, in Spanish, Tecniac\\'ustica"},{"id":"http://arxiv.org/abs/2310.15648v1","updated":"2023-10-24T09:08:20Z","published":"2023-10-24T09:08:20Z","title":"Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio\n  Models","summary":"  The introduction of large-scale audio datasets, such as AudioSet, paved the\nway for Transformers to conquer the audio domain and replace CNNs as the\nstate-of-the-art neural network architecture for many tasks. Audio Spectrogram\nTransformers are excellent at exploiting large datasets, creating powerful\npre-trained models that surpass CNNs when fine-tuned on downstream tasks.\nHowever, current popular Audio Spectrogram Transformers are demanding in terms\nof computational complexity compared to CNNs. Recently, we have shown that, by\nemploying Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch\nup with and even outperform Transformers on large datasets. In this work, we\nextend this line of research and increase the capacity of efficient CNNs by\nintroducing dynamic CNN blocks, constructed of dynamic non-linearities, dynamic\nconvolutions and attention mechanisms. We show that these dynamic CNNs\noutperform traditional efficient CNNs, in terms of the performance-complexity\ntrade-off and parameter efficiency, at the task of audio tagging on the\nlarge-scale AudioSet. Our experiments further indicate that the introduced\ndynamic CNNs achieve better performance on downstream tasks and scale up well,\nattaining Transformer performance and even outperforming them on AudioSet and\nseveral downstream tasks.\n","authors":["Florian Schmid","Khaled Koutini","Gerhard Widmer"],"pdf_url":"https://arxiv.org/pdf/2310.15648v1.pdf","comment":"Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing. Source Code available at:\n  https://github.com/fschmid56/EfficientAT"},{"id":"http://arxiv.org/abs/2310.02971v2","updated":"2023-10-24T04:47:19Z","published":"2023-10-04T17:07:32Z","title":"Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech\n  Model","summary":"  Prompting and adapter tuning have emerged as efficient alternatives to\nfine-tuning (FT) methods. However, existing studies on speech prompting focused\non classification tasks and failed on more complex sequence generation tasks.\nBesides, adapter tuning is primarily applied with a focus on encoder-only\nself-supervised models. Our experiments show that prompting on Wav2Seq, a\nself-supervised encoder-decoder model, surpasses previous works in sequence\ngeneration tasks. It achieves a remarkable 53% relative improvement in word\nerror rate for ASR and a 27% in F1 score for slot filling. Additionally,\nprompting competes with the FT method in the low-resource scenario. Moreover,\nwe show the transferability of prompting and adapter tuning on Wav2Seq in\ncross-lingual ASR. When limited trainable parameters are involved, prompting\nand adapter tuning consistently outperform conventional FT across 7 languages.\nNotably, in the low-resource scenario, prompting consistently outperforms\nadapter tuning.\n","authors":["Kai-Wei Chang","Ming-Hsin Chen","Yun-Ping Lin","Jing Neng Hsu","Paul Kuo-Ming Huang","Chien-yu Huang","Shang-Wen Li","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2310.02971v2.pdf","comment":"Accepted to IEEE ASRU 2023"},{"id":"http://arxiv.org/abs/2310.01128v2","updated":"2023-10-24T02:08:21Z","published":"2023-10-02T12:02:07Z","title":"Disentangling Voice and Content with Self-Supervision for Speaker\n  Recognition","summary":"  For speaker recognition, it is difficult to extract an accurate speaker\nrepresentation from speech because of its mixture of speaker traits and\ncontent. This paper proposes a disentanglement framework that simultaneously\nmodels speaker traits and content variability in speech. It is realized with\nthe use of three Gaussian inference layers, each consisting of a learnable\ntransition model that extracts distinct speech components. Notably, a\nstrengthened transition model is specifically designed to model complex speech\ndynamics. We also propose a self-supervision method to dynamically disentangle\ncontent without the use of labels other than speaker identities. The efficacy\nof the proposed framework is validated via experiments conducted on the\nVoxCeleb and SITW datasets with 9.56% and 8.24% average reductions in EER and\nminDCF, respectively. Since neither additional model training nor data is\nspecifically needed, it is easily applicable in practical use.\n","authors":["Tianchi Liu","Kong Aik Lee","Qiongqiong Wang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2310.01128v2.pdf","comment":"Accepted to NeurIPS 2023 (main track)"},{"id":"http://arxiv.org/abs/2310.15425v1","updated":"2023-10-24T00:43:54Z","published":"2023-10-24T00:43:54Z","title":"The Mason-Alberta Phonetic Segmenter: A forced alignment system based on\n  deep neural networks and interpolation","summary":"  Forced alignment systems automatically determine boundaries between segments\nin speech data, given an orthographic transcription. These tools are\ncommonplace in phonetics to facilitate the use of speech data that would be\ninfeasible to manually transcribe and segment. In the present paper, we\ndescribe a new neural network-based forced alignment system, the Mason-Alberta\nPhonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two\npossible improvements we pursue for forced alignment systems. The first is\ntreating the acoustic model in a forced aligner as a tagging task, rather than\na classification task, motivated by the common understanding that segments in\nspeech are not truly discrete and commonly overlap. The second is an\ninterpolation technique to allow boundaries more precise than the common 10 ms\nlimit in modern forced alignment systems. We compare configurations of our\nsystem to a state-of-the-art system, the Montreal Forced Aligner. The tagging\napproach did not generally yield improved results over the Montreal Forced\nAligner. However, a system with the interpolation technique had a 27.92%\nincrease relative to the Montreal Forced Aligner in the amount of boundaries\nwithin 10 ms of the target on the test set. We also reflect on the task and\ntraining process for acoustic modeling in forced alignment, highlighting how\nthe output targets for these models do not match phoneticians' conception of\nsimilarity between phones and that reconciliation of this tension may require\nrethinking the task and output targets or how speech itself should be\nsegmented.\n","authors":["Matthew C. Kelley","Scott James Perry","Benjamin V. Tucker"],"pdf_url":"https://arxiv.org/pdf/2310.15425v1.pdf","comment":"submitted for publication"},{"id":"http://arxiv.org/abs/2301.08730v3","updated":"2023-10-24T20:19:51Z","published":"2023-01-20T18:49:58Z","title":"Novel-View Acoustic Synthesis","summary":"  We introduce the novel-view acoustic synthesis (NVAS) task: given the sight\nand sound observed at a source viewpoint, can we synthesize the sound of that\nscene from an unseen target viewpoint? We propose a neural rendering approach:\nVisually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize\nthe sound of an arbitrary point in space by analyzing the input audio-visual\ncues. To benchmark this task, we collect two first-of-their-kind large-scale\nmulti-view audio-visual datasets, one synthetic and one real. We show that our\nmodel successfully reasons about the spatial cues and synthesizes faithful\naudio on both datasets. To our knowledge, this work represents the very first\nformulation, dataset, and approach to solve the novel-view acoustic synthesis\ntask, which has exciting potential applications ranging from AR/VR to art and\ndesign. Unlocked by this work, we believe that the future of novel-view\nsynthesis is in multi-modal learning from videos.\n","authors":["Changan Chen","Alexander Richard","Roman Shapovalov","Vamsi Krishna Ithapu","Natalia Neverova","Kristen Grauman","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2301.08730v3.pdf","comment":"Accepted at CVPR 2023. Project page:\n  https://vision.cs.utexas.edu/projects/nvas"},{"id":"http://arxiv.org/abs/2310.16140v1","updated":"2023-10-24T19:27:50Z","published":"2023-10-24T19:27:50Z","title":"IA Para el Mantenimiento Predictivo en Canteras: Modelado","summary":"  Dependence on raw materials, especially in the mining sector, is a key part\nof today's economy. Aggregates are vital, being the second most used raw\nmaterial after water. Digitally transforming this sector is key to optimizing\noperations. However, supervision and maintenance (predictive and corrective)\nare challenges little explored in this sector, due to the particularities of\nthe sector, machinery and environmental conditions. All this, despite the\nsuccesses achieved in other scenarios in monitoring with acoustic and contact\nsensors. We present an unsupervised learning scheme that trains a variational\nautoencoder model on a set of sound records. This is the first such dataset\ncollected during processing plant operations, containing information from\ndifferent points of the processing line. Our results demonstrate the model's\nability to reconstruct and represent in latent space the recorded sounds, the\ndifferences in operating conditions and between different equipment. In the\nfuture, this should facilitate the classification of sounds, as well as the\ndetection of anomalies and degradation patterns in the operation of the\nmachinery.\n","authors":["Fernando Marcos","Rodrigo Tamaki","Mateo C√°mara","Virginia Yag√ºe","Jos√© Luis Blanco"],"pdf_url":"https://arxiv.org/pdf/2310.16140v1.pdf","comment":"10 pages, in Spanish language, 5 figures. Presented in Tecniacustica\n  2023 conference (Cuenca, Spain)"},{"id":"http://arxiv.org/abs/2309.13876v3","updated":"2023-10-24T18:28:22Z","published":"2023-09-25T05:01:34Z","title":"Reproducing Whisper-Style Training Using an Open-Source Toolkit and\n  Publicly Available Data","summary":"  Pre-training speech models on large volumes of data has achieved remarkable\nsuccess. OpenAI Whisper is a multilingual multitask model trained on 680k hours\nof supervised speech data. It generalizes well to various speech recognition\nand translation benchmarks even in a zero-shot setup. However, the full\npipeline for developing such models (from data collection to training) is not\npublicly accessible, which makes it difficult for researchers to further\nimprove its performance and address training-related issues such as efficiency,\nrobustness, fairness, and bias. This work presents an Open Whisper-style Speech\nModel (OWSM), which reproduces Whisper-style training using an open-source\ntoolkit and publicly available data. OWSM even supports more translation\ndirections and can be more efficient to train. We will publicly release all\nscripts used for data preparation, training, inference, and scoring as well as\npre-trained models and training logs to promote open science.\n","authors":["Yifan Peng","Jinchuan Tian","Brian Yan","Dan Berrebbi","Xuankai Chang","Xinjian Li","Jiatong Shi","Siddhant Arora","William Chen","Roshan Sharma","Wangyou Zhang","Yui Sudo","Muhammad Shakeel","Jee-weon Jung","Soumi Maiti","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2309.13876v3.pdf","comment":"Accepted at ASRU 2023"},{"id":"http://arxiv.org/abs/2310.16109v1","updated":"2023-10-24T18:21:03Z","published":"2023-10-24T18:21:03Z","title":"Complex Image Generation SwinTransformer Network for Audio Denoising","summary":"  Achieving high-performance audio denoising is still a challenging task in\nreal-world applications. Existing time-frequency methods often ignore the\nquality of generated frequency domain images. This paper converts the audio\ndenoising problem into an image generation task. We first develop a complex\nimage generation SwinTransformer network to capture more information from the\ncomplex Fourier domain. We then impose structure similarity and detailed loss\nfunctions to generate high-quality images and develop an SDR loss to minimize\nthe difference between denoised and clean audios. Extensive experiments on two\nbenchmark datasets demonstrate that our proposed model is better than\nstate-of-the-art methods.\n","authors":["Youshan Zhang","Jialu Li"],"pdf_url":"https://arxiv.org/pdf/2310.16109v1.pdf","comment":null}]},"2023-10-25T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2310.11069v3","updated":"2023-10-25T06:20:39Z","published":"2023-10-17T08:33:02Z","title":"VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System","summary":"  Arabic is a complex language with many varieties and dialects spoken by over\n450 millions all around the world. Due to the linguistic diversity and\nvariations, it is challenging to build a robust and generalized ASR system for\nArabic. In this work, we address this gap by developing and demoing a system,\ndubbed VoxArabica, for dialect identification (DID) as well as automatic speech\nrecognition (ASR) of Arabic. We train a wide range of models such as HuBERT\n(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR\ntasks. Our DID models are trained to identify 17 different dialects in addition\nto MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.\nAdditionally, for the remaining dialects in ASR, we provide the option to\nchoose various models such as Whisper and MMS in a zero-shot setting. We\nintegrate these models into a single web interface with diverse features such\nas audio recording, file upload, model selection, and the option to raise flags\nfor incorrect outputs. Overall, we believe VoxArabica will be useful for a wide\nrange of audiences concerned with Arabic research. Our system is currently\nrunning at https://cdce-206-12-100-168.ngrok.io/.\n","authors":["Abdul Waheed","Bashar Talafha","Peter Suvellin","AbdelRahim Elmadany","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2310.11069v3.pdf","comment":"Accepted at ArabicNLP conference co-located with EMNLP'23. First\n  three authors contributed equally"},{"id":"http://arxiv.org/abs/2211.04921v4","updated":"2023-10-25T17:43:33Z","published":"2022-11-09T14:45:56Z","title":"Global, and Local Optimization Beamforming for Broadband Sources","summary":"  This paper presents an alternative energy function for Global Optimization\n(GO) beamforming, tailored to acoustic broadband sources. Given, that\nproperties such as the source location, multipole rotation, or flow conditions\nare parameterized over the frequency, a CSM-fitting can be performed for all\nfrequencies at once. A numerical analysis shows that the nonlinear energy\nfunction for the standard GO problem is equivalent to the source's Point Spread\nFunction (PSF) and contains local minima at the grating- and side lobes'\nlocations. The energy function is improved with the proposed broadband energy,\nas it averages the PSF. Further, it simplifies the process of identifying\nsources and reconstructing their spectra from the results. The paper shows that\nthe method is superior on synthetic monopoles compared to standard GO and\nCLEAN-SC. For real-world data the results of the proposed method and CLEAN-SC\nare similar, and outperform standard GO. The main difference is that source\nassumption violations cause noisy maps for CLEAN-SC and cause wrong spectral\nestimations of the proposed method. By using reasonable initial values, the GO\nproblem reduces to a Local Optimization problem with similar results. Further,\nthe proposed method is able to identify synthetic multipoles with different\npole amplitudes and unknown pole rotations.\n","authors":["Armin Goudarzi"],"pdf_url":"https://arxiv.org/pdf/2211.04921v4.pdf","comment":"Submitted to JASA"},{"id":"http://arxiv.org/abs/2110.03427v3","updated":"2023-10-25T15:21:08Z","published":"2021-10-05T16:38:57Z","title":"Is Attention always needed? A Case Study on Language Identification from\n  Speech","summary":"  Language Identification (LID) is a crucial preliminary process in the field\nof Automatic Speech Recognition (ASR) that involves the identification of a\nspoken language from audio samples. Contemporary systems that can process\nspeech in multiple languages require users to expressly designate one or more\nlanguages prior to utilization. The LID task assumes a significant role in\nscenarios where ASR systems are unable to comprehend the spoken language in\nmultilingual settings, leading to unsuccessful speech recognition outcomes. The\npresent study introduces convolutional recurrent neural network (CRNN) based\nLID, designed to operate on the Mel-frequency Cepstral Coefficient (MFCC)\ncharacteristics of audio samples. Furthermore, we replicate certain\nstate-of-the-art methodologies, specifically the Convolutional Neural Network\n(CNN) and Attention-based Convolutional Recurrent Neural Network (CRNN with\nattention), and conduct a comparative analysis with our CRNN-based approach. We\nconducted comprehensive evaluations on thirteen distinct Indian languages and\nour model resulted in over 98\\% classification accuracy. The LID model exhibits\nhigh-performance levels ranging from 97% to 100% for languages that are\nlinguistically similar. The proposed LID model exhibits a high degree of\nextensibility to additional languages and demonstrates a strong resistance to\nnoise, achieving 91.2% accuracy in a noisy setting when applied to a European\nLanguage (EU) dataset.\n","authors":["Atanu Mandal","Santanu Pal","Indranil Dutta","Mahidas Bhattacharya","Sudip Kumar Naskar"],"pdf_url":"https://arxiv.org/pdf/2110.03427v3.pdf","comment":"Accepted for publication in Natural Language Engineering"},{"id":"http://arxiv.org/abs/2310.16621v1","updated":"2023-10-25T13:20:54Z","published":"2023-10-25T13:20:54Z","title":"ArTST: Arabic Text and Speech Transformer","summary":"  We present ArTST, a pre-trained Arabic text and speech transformer for\nsupporting open-source speech technologies for the Arabic language. The model\narchitecture follows the unified-modal framework, SpeechT5, that was recently\nreleased for English, and is focused on Modern Standard Arabic (MSA), with\nplans to extend the model for dialectal and code-switched Arabic in future\neditions. We pre-trained the model from scratch on MSA speech and text data,\nand fine-tuned it for the following tasks: Automatic Speech Recognition (ASR),\nText-To-Speech synthesis (TTS), and spoken dialect identification. In our\nexperiments comparing ArTST with SpeechT5, as well as with previously reported\nresults in these tasks, ArTST performs on a par with or exceeding the current\nstate-of-the-art in all three tasks. Moreover, we find that our pre-training is\nconducive for generalization, which is particularly evident in the low-resource\nTTS task. The pre-trained model as well as the fine-tuned ASR and TTS models\nare released for research use.\n","authors":["Hawau Olamide Toyin","Amirbek Djanibekov","Ajinkya Kulkarni","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2310.16621v1.pdf","comment":"11 pages, 1 figure, SIGARAB ArabicNLP 2023"},{"id":"http://arxiv.org/abs/2310.16609v1","updated":"2023-10-25T13:07:07Z","published":"2023-10-25T13:07:07Z","title":"Back Transcription as a Method for Evaluating Robustness of Natural\n  Language Understanding Models to Speech Recognition Errors","summary":"  In a spoken dialogue system, an NLU model is preceded by a speech recognition\nsystem that can deteriorate the performance of natural language understanding.\nThis paper proposes a method for investigating the impact of speech recognition\nerrors on the performance of natural language understanding models. The\nproposed method combines the back transcription procedure with a fine-grained\ntechnique for categorizing the errors that affect the performance of NLU\nmodels. The method relies on the usage of synthesized speech for NLU\nevaluation. We show that the use of synthesized speech in place of audio\nrecording does not change the outcomes of the presented technique in a\nsignificant way.\n","authors":["Marek Kubis","Pawe≈Ç Sk√≥rzewski","Marcin Sowa≈Ñski","Tomasz Ziƒôtkiewicz"],"pdf_url":"https://arxiv.org/pdf/2310.16609v1.pdf","comment":"Accepted to EMNLP 2023 main conference"},{"id":"http://arxiv.org/abs/2310.16550v1","updated":"2023-10-25T11:04:32Z","published":"2023-10-25T11:04:32Z","title":"Dynamic Processing Neural Network Architecture For Hearing Loss\n  Compensation","summary":"  This paper proposes neural networks for compensating sensorineural hearing\nloss. The aim of the hearing loss compensation task is to transform a speech\nsignal to increase speech intelligibility after further processing by a person\nwith a hearing impairment, which is modeled by a hearing loss model. We propose\nan interpretable model called dynamic processing network, which has a structure\nsimilar to band-wise dynamic compressor. The network is differentiable, and\ntherefore allows to learn its parameters to maximize speech intelligibility.\nMore generic models based on convolutional layers were tested as well. The\nperformance of the tested architectures was assessed using spectro-temporal\nobjective index (STOI) with hearing-threshold noise and hearing aid speech\nintelligibility (HASPI) metrics. The dynamic processing network gave a\nsignificant improvement of STOI and HASPI in comparison to popular compressive\ngain prescription rule Camfit. A large enough convolutional network could\noutperform the interpretable model with the cost of larger computational load.\nFinally, a combination of the dynamic processing network with convolutional\nneural network gave the best results in terms of STOI and HASPI.\n","authors":["Szymon Drgas","Lars Bramsl√∏w","Archontis Politis","Gaurav Naithani","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2310.16550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16481v1","updated":"2023-10-25T09:05:48Z","published":"2023-10-25T09:05:48Z","title":"A Novel Approach for Object Based Audio Broadcasting","summary":"  Object Based Audio (OBA) provides a new kind of audio experience, delivered\nto the audience to personalize and customize their experience of listening and\nto give them choice of what and how to hear their audio content. OBA can be\napplied to different platforms such as broadcasting, streaming and cinema\nsound. This paper presents a novel approach for creating object-based audio on\nthe production side. The approach here presents Sample-by-Sample Object Based\nAudio (SSOBA) embedding. SSOBA places audio object samples in such a way that\nallows audiences to easily individualize their chosen audio sources according\nto their interests and needs. SSOBA is an extra service and not an alternative,\nso it is also compliant with legacy audio players. The biggest advantage of\nSSOBA is that it does not require any special additional hardware in the\nbroadcasting chain and it is therefore easy to implement and equip legacy\nplayers and decoders with enhanced ability. Input audio objects, number of\noutput channels and sampling rates are three important factors affecting SSOBA\nperformance and specifying it to be lossless or lossy. SSOBA adopts\ninterpolation at the decoder side to compensate for eliminated samples. Both\nsubjective and objective experiments are carried out to evaluate the output\nresults at each step. MUSHRA subjective experiments conducted after the\nencoding step shows good-quality performance of SSOBA with up to five objects.\nSNR measurements and objective experiments, performed after decoding and\ninterpolation, show significant successful recovery and separation of audio\nobjects. Experimental results show that a minimum sampling rate of 96 kHz is\nindicated to encode up to five objects in a Stereo-mode channel to acquire good\nsubjective and objective results simultaneously.\n","authors":["Mohammad Reza Hasanabadi"],"pdf_url":"https://arxiv.org/pdf/2310.16481v1.pdf","comment":"Accepted in ABU Technical Review Journal 2020/9"},{"id":"http://arxiv.org/abs/2306.02858v4","updated":"2023-10-25T06:23:31Z","published":"2023-06-05T13:17:27Z","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video\n  Understanding","summary":"  We present Video-LLaMA a multi-modal framework that empowers Large Language\nModels (LLMs) with the capability of understanding both visual and auditory\ncontent in the video. Video-LLaMA bootstraps cross-modal training from the\nfrozen pre-trained visual and audio encoders and the frozen LLMs. Unlike\nprevious works that complement LLMs to process the visual or audio signals\nonly, Video-LLaMA enables video comprehension by tackling two challenges: (1)\ncapturing the temporal changes in visual scenes, (2) integrating audio-visual\nsignals. To counter the first challenge, we propose a Video Q-former to\nassemble a pre-trained image encoder into our video encoder and introduce a\nvideo-to-text generation task to learn video-language correspondence. For the\nsecond challenge, we leverage ImageBind, a universal embedding model aligning\nmultiple modalities, as the pre-trained audio encoder and introduce an Audio\nQ-former on top of ImageBind to learn reasonable auditory query embeddings for\nthe LLM module. To align the output of both visual and audio encoders with\nLLM's embedding space, we first train Video-LLaMA on massive\nvideo/image-caption pairs and then tune our model with visual-instruction\ndatasets of moderate amount but higher quality. We found Video-LLaMA shows the\nability to perceive and comprehend video content and generate meaningful\nresponses grounded in the visual and auditory information presented in the\nvideos.\n","authors":["Hang Zhang","Xin Li","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2306.02858v4.pdf","comment":"Accepted by EMNLP 2023's demo track; Code, Pretrained Model, and\n  Dataset: https://github.com/DAMO-NLP-SG/Video-LLaMA"},{"id":"http://arxiv.org/abs/2310.16338v1","updated":"2023-10-25T03:40:50Z","published":"2023-10-25T03:40:50Z","title":"Generative Pre-training for Speech with Flow Matching","summary":"  Generative models have gained more and more attention in recent years for\ntheir remarkable success in tasks that required estimating and sampling data\ndistribution to generate high-fidelity synthetic data. In speech,\ntext-to-speech synthesis and neural vocoder are good examples where generative\nmodels have shined. While generative models have been applied to different\napplications in speech, there exists no general-purpose generative model that\nmodels speech directly. In this work, we take a step toward this direction by\nshowing a single pre-trained generative model can be adapted to different\ndownstream tasks with strong performance. Specifically, we pre-trained a\ngenerative model, named SpeechFlow, on 60k hours of untranscribed speech with\nFlow Matching and masked conditions. Experiment results show the pre-trained\ngenerative model can be fine-tuned with task-specific data to match or surpass\nexisting expert models on speech enhancement, separation, and synthesis. Our\nwork suggested a foundational model for generation tasks in speech can be built\nwith generative pre-training.\n","authors":["Alexander H. Liu","Matt Le","Apoorv Vyas","Bowen Shi","Andros Tjandra","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2310.16338v1.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2310.16334v1","updated":"2023-10-25T03:30:37Z","published":"2023-10-25T03:30:37Z","title":"AccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style\n  Transfer and Multi-Track Function Prior","summary":"  We propose AccoMontage-3, a symbolic music automation system capable of\ngenerating multi-track, full-band accompaniment based on the input of a lead\nmelody with chords (i.e., a lead sheet). The system contains three modular\ncomponents, each modelling a vital aspect of full-band composition. The first\ncomponent is a piano arranger that generates piano accompaniment for the lead\nsheet by transferring texture styles to the chords using latent chord-texture\ndisentanglement and heuristic retrieval of texture donors. The second component\norchestrates the piano accompaniment score into full-band arrangement according\nto the orchestration style encoded by individual track functions. The third\ncomponent, which connects the previous two, is a prior model characterizing the\nglobal structure of orchestration style over the whole piece of music. From end\nto end, the system learns to generate full-band accompaniment in a\nself-supervised fashion, applying style transfer at two levels of polyphonic\ncomposition: texture and orchestration. Experiments show that our system\noutperforms the baselines significantly, and the modular design offers\neffective controls in a musically meaningful way.\n","authors":["Jingwei Zhao","Gus Xia","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16287v1","updated":"2023-10-25T01:45:33Z","published":"2023-10-25T01:45:33Z","title":"Towards Streaming Speech-to-Avatar Synthesis","summary":"  Streaming speech-to-avatar synthesis creates real-time animations for a\nvirtual character from audio data. Accurate avatar representations of speech\nare important for the visualization of sound in linguistics, phonetics, and\nphonology, visual feedback to assist second language acquisition, and virtual\nembodiment for paralyzed patients. Previous works have highlighted the\ncapability of deep articulatory inversion to perform high-quality avatar\nanimation using electromagnetic articulography (EMA) features. However, these\nmodels focus on offline avatar synthesis with recordings rather than real-time\naudio, which is necessary for live avatar visualization or embodiment. To\naddress this issue, we propose a method using articulatory inversion for\nstreaming high quality facial and inner-mouth avatar animation from real-time\naudio. Our approach achieves 130ms average streaming latency for every 0.1\nseconds of audio with a 0.792 correlation with ground truth articulations.\nFinally, we show generated mouth and tongue animations to demonstrate the\nefficacy of our methodology.\n","authors":["Tejas S. Prabhune","Peter Wu","Bohan Yu","Gopala K. Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2310.16287v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.17049v1","updated":"2023-10-25T23:21:46Z","published":"2023-10-25T23:21:46Z","title":"Learning Repeatable Speech Embeddings Using An Intra-class Correlation\n  Regularizer","summary":"  A good supervised embedding for a specific machine learning task is only\nsensitive to changes in the label of interest and is invariant to other\nconfounding factors. We leverage the concept of repeatability from measurement\ntheory to describe this property and propose to use the intra-class correlation\ncoefficient (ICC) to evaluate the repeatability of embeddings. We then propose\na novel regularizer, the ICC regularizer, as a complementary component for\ncontrastive losses to guide deep neural networks to produce embeddings with\nhigher repeatability. We use simulated data to explain why the ICC regularizer\nworks better on minimizing the intra-class variance than the contrastive loss\nalone. We implement the ICC regularizer and apply it to three speech tasks:\nspeaker verification, voice style conversion, and a clinical application for\ndetecting dysphonic voice. The experimental results demonstrate that adding an\nICC regularizer can improve the repeatability of learned embeddings compared to\nonly using the contrastive loss; further, these embeddings lead to improved\nperformance in these downstream tasks.\n","authors":["Jianwei Zhang","Suren Jayasuriya","Visar Berisha"],"pdf_url":"https://arxiv.org/pdf/2310.17049v1.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.17004v1","updated":"2023-10-25T21:05:13Z","published":"2023-10-25T21:05:13Z","title":"Improved Panning on Non-Equidistant Loudspeakers with Direct Sound Level\n  Compensation","summary":"  Loudspeaker rendering techniques that create phantom sound sources often\nassume an equidistant loudspeaker layout. Typical home setups might not fulfill\nthis condition as loudspeakers deviate from canonical positions, thus requiring\na corresponding calibration. The standard approach is to compensate for delays\nand to match the loudness of each loudspeaker at the listener's location.It was\nfound that a shift of the phantom image occurs when this calibration procedure\nis applied and one of a pair of loudspeakers is significantly closer to the\nlistener than the other. In this paper, a novel approach to panning on\nnon-equidistant loudspeaker layouts is presented whereby the panning position\nis governed by the direct sound and the perceived loudness is governed by the\nfull impulse response. Subjective listening tests are presented that validate\nthe approach and quantify the perceived effect of the compensation. In a setup\nwhere the standard calibration leads to an average error of 10 degrees, the\nproposed direct sound compensation largely returns the phantom source to its\nintended position.\n","authors":["Jan-Hendrik Hanschke","Daniel Arteaga","Giulio Cengarle","Joshua Lando","Mark R. P. Thomas","Alan Seefeldt"],"pdf_url":"https://arxiv.org/pdf/2310.17004v1.pdf","comment":"10 pages. Accepted for presentation in AES Convention 155 (2023)"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.11069v3","updated":"2023-10-25T06:20:39Z","published":"2023-10-17T08:33:02Z","title":"VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System","summary":"  Arabic is a complex language with many varieties and dialects spoken by over\n450 millions all around the world. Due to the linguistic diversity and\nvariations, it is challenging to build a robust and generalized ASR system for\nArabic. In this work, we address this gap by developing and demoing a system,\ndubbed VoxArabica, for dialect identification (DID) as well as automatic speech\nrecognition (ASR) of Arabic. We train a wide range of models such as HuBERT\n(DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR\ntasks. Our DID models are trained to identify 17 different dialects in addition\nto MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data.\nAdditionally, for the remaining dialects in ASR, we provide the option to\nchoose various models such as Whisper and MMS in a zero-shot setting. We\nintegrate these models into a single web interface with diverse features such\nas audio recording, file upload, model selection, and the option to raise flags\nfor incorrect outputs. Overall, we believe VoxArabica will be useful for a wide\nrange of audiences concerned with Arabic research. Our system is currently\nrunning at https://cdce-206-12-100-168.ngrok.io/.\n","authors":["Abdul Waheed","Bashar Talafha","Peter Suvellin","AbdelRahim Elmadany","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2310.11069v3.pdf","comment":"Accepted at ArabicNLP conference co-located with EMNLP'23. First\n  three authors contributed equally"},{"id":"http://arxiv.org/abs/2310.11954v2","updated":"2023-10-25T13:34:13Z","published":"2023-10-18T13:31:10Z","title":"MusicAgent: An AI Agent for Music Understanding and Generation with\n  Large Language Models","summary":"  AI-empowered music processing is a diverse field that encompasses dozens of\ntasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension\ntasks (e.g., music classification). For developers and amateurs, it is very\ndifficult to grasp all of these task to satisfy their requirements in music\nprocessing, especially considering the huge differences in the representations\nof music data and the model applicability across platforms among various tasks.\nConsequently, it is necessary to build a system to organize and integrate these\ntasks, and thus help practitioners to automatically analyze their demand and\ncall suitable tools as solutions to fulfill their requirements. Inspired by the\nrecent success of large language models (LLMs) in task automation, we develop a\nsystem, named MusicAgent, which integrates numerous music-related tools and an\nautonomous workflow to address user requirements. More specifically, we build\n1) toolset that collects tools from diverse sources, including Hugging Face,\nGitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g.,\nChatGPT) to organize these tools and automatically decompose user requests into\nmultiple sub-tasks and invoke corresponding music tools. The primary goal of\nthis system is to free users from the intricacies of AI-music tools, enabling\nthem to concentrate on the creative aspect. By granting users the freedom to\neffortlessly combine tools, the system offers a seamless and enriching music\nexperience.\n","authors":["Dingyao Yu","Kaitao Song","Peiling Lu","Tianyu He","Xu Tan","Wei Ye","Shikun Zhang","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.11954v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.04921v4","updated":"2023-10-25T17:43:33Z","published":"2022-11-09T14:45:56Z","title":"Global, and Local Optimization Beamforming for Broadband Sources","summary":"  This paper presents an alternative energy function for Global Optimization\n(GO) beamforming, tailored to acoustic broadband sources. Given, that\nproperties such as the source location, multipole rotation, or flow conditions\nare parameterized over the frequency, a CSM-fitting can be performed for all\nfrequencies at once. A numerical analysis shows that the nonlinear energy\nfunction for the standard GO problem is equivalent to the source's Point Spread\nFunction (PSF) and contains local minima at the grating- and side lobes'\nlocations. The energy function is improved with the proposed broadband energy,\nas it averages the PSF. Further, it simplifies the process of identifying\nsources and reconstructing their spectra from the results. The paper shows that\nthe method is superior on synthetic monopoles compared to standard GO and\nCLEAN-SC. For real-world data the results of the proposed method and CLEAN-SC\nare similar, and outperform standard GO. The main difference is that source\nassumption violations cause noisy maps for CLEAN-SC and cause wrong spectral\nestimations of the proposed method. By using reasonable initial values, the GO\nproblem reduces to a Local Optimization problem with similar results. Further,\nthe proposed method is able to identify synthetic multipoles with different\npole amplitudes and unknown pole rotations.\n","authors":["Armin Goudarzi"],"pdf_url":"https://arxiv.org/pdf/2211.04921v4.pdf","comment":"Submitted to JASA"},{"id":"http://arxiv.org/abs/2110.03427v3","updated":"2023-10-25T15:21:08Z","published":"2021-10-05T16:38:57Z","title":"Is Attention always needed? A Case Study on Language Identification from\n  Speech","summary":"  Language Identification (LID) is a crucial preliminary process in the field\nof Automatic Speech Recognition (ASR) that involves the identification of a\nspoken language from audio samples. Contemporary systems that can process\nspeech in multiple languages require users to expressly designate one or more\nlanguages prior to utilization. The LID task assumes a significant role in\nscenarios where ASR systems are unable to comprehend the spoken language in\nmultilingual settings, leading to unsuccessful speech recognition outcomes. The\npresent study introduces convolutional recurrent neural network (CRNN) based\nLID, designed to operate on the Mel-frequency Cepstral Coefficient (MFCC)\ncharacteristics of audio samples. Furthermore, we replicate certain\nstate-of-the-art methodologies, specifically the Convolutional Neural Network\n(CNN) and Attention-based Convolutional Recurrent Neural Network (CRNN with\nattention), and conduct a comparative analysis with our CRNN-based approach. We\nconducted comprehensive evaluations on thirteen distinct Indian languages and\nour model resulted in over 98\\% classification accuracy. The LID model exhibits\nhigh-performance levels ranging from 97% to 100% for languages that are\nlinguistically similar. The proposed LID model exhibits a high degree of\nextensibility to additional languages and demonstrates a strong resistance to\nnoise, achieving 91.2% accuracy in a noisy setting when applied to a European\nLanguage (EU) dataset.\n","authors":["Atanu Mandal","Santanu Pal","Indranil Dutta","Mahidas Bhattacharya","Sudip Kumar Naskar"],"pdf_url":"https://arxiv.org/pdf/2110.03427v3.pdf","comment":"Accepted for publication in Natural Language Engineering"},{"id":"http://arxiv.org/abs/2310.16621v1","updated":"2023-10-25T13:20:54Z","published":"2023-10-25T13:20:54Z","title":"ArTST: Arabic Text and Speech Transformer","summary":"  We present ArTST, a pre-trained Arabic text and speech transformer for\nsupporting open-source speech technologies for the Arabic language. The model\narchitecture follows the unified-modal framework, SpeechT5, that was recently\nreleased for English, and is focused on Modern Standard Arabic (MSA), with\nplans to extend the model for dialectal and code-switched Arabic in future\neditions. We pre-trained the model from scratch on MSA speech and text data,\nand fine-tuned it for the following tasks: Automatic Speech Recognition (ASR),\nText-To-Speech synthesis (TTS), and spoken dialect identification. In our\nexperiments comparing ArTST with SpeechT5, as well as with previously reported\nresults in these tasks, ArTST performs on a par with or exceeding the current\nstate-of-the-art in all three tasks. Moreover, we find that our pre-training is\nconducive for generalization, which is particularly evident in the low-resource\nTTS task. The pre-trained model as well as the fine-tuned ASR and TTS models\nare released for research use.\n","authors":["Hawau Olamide Toyin","Amirbek Djanibekov","Ajinkya Kulkarni","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2310.16621v1.pdf","comment":"11 pages, 1 figure, SIGARAB ArabicNLP 2023"},{"id":"http://arxiv.org/abs/2310.16609v1","updated":"2023-10-25T13:07:07Z","published":"2023-10-25T13:07:07Z","title":"Back Transcription as a Method for Evaluating Robustness of Natural\n  Language Understanding Models to Speech Recognition Errors","summary":"  In a spoken dialogue system, an NLU model is preceded by a speech recognition\nsystem that can deteriorate the performance of natural language understanding.\nThis paper proposes a method for investigating the impact of speech recognition\nerrors on the performance of natural language understanding models. The\nproposed method combines the back transcription procedure with a fine-grained\ntechnique for categorizing the errors that affect the performance of NLU\nmodels. The method relies on the usage of synthesized speech for NLU\nevaluation. We show that the use of synthesized speech in place of audio\nrecording does not change the outcomes of the presented technique in a\nsignificant way.\n","authors":["Marek Kubis","Pawe≈Ç Sk√≥rzewski","Marcin Sowa≈Ñski","Tomasz Ziƒôtkiewicz"],"pdf_url":"https://arxiv.org/pdf/2310.16609v1.pdf","comment":"Accepted to EMNLP 2023 main conference"},{"id":"http://arxiv.org/abs/2310.16550v1","updated":"2023-10-25T11:04:32Z","published":"2023-10-25T11:04:32Z","title":"Dynamic Processing Neural Network Architecture For Hearing Loss\n  Compensation","summary":"  This paper proposes neural networks for compensating sensorineural hearing\nloss. The aim of the hearing loss compensation task is to transform a speech\nsignal to increase speech intelligibility after further processing by a person\nwith a hearing impairment, which is modeled by a hearing loss model. We propose\nan interpretable model called dynamic processing network, which has a structure\nsimilar to band-wise dynamic compressor. The network is differentiable, and\ntherefore allows to learn its parameters to maximize speech intelligibility.\nMore generic models based on convolutional layers were tested as well. The\nperformance of the tested architectures was assessed using spectro-temporal\nobjective index (STOI) with hearing-threshold noise and hearing aid speech\nintelligibility (HASPI) metrics. The dynamic processing network gave a\nsignificant improvement of STOI and HASPI in comparison to popular compressive\ngain prescription rule Camfit. A large enough convolutional network could\noutperform the interpretable model with the cost of larger computational load.\nFinally, a combination of the dynamic processing network with convolutional\nneural network gave the best results in terms of STOI and HASPI.\n","authors":["Szymon Drgas","Lars Bramsl√∏w","Archontis Politis","Gaurav Naithani","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2310.16550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16481v1","updated":"2023-10-25T09:05:48Z","published":"2023-10-25T09:05:48Z","title":"A Novel Approach for Object Based Audio Broadcasting","summary":"  Object Based Audio (OBA) provides a new kind of audio experience, delivered\nto the audience to personalize and customize their experience of listening and\nto give them choice of what and how to hear their audio content. OBA can be\napplied to different platforms such as broadcasting, streaming and cinema\nsound. This paper presents a novel approach for creating object-based audio on\nthe production side. The approach here presents Sample-by-Sample Object Based\nAudio (SSOBA) embedding. SSOBA places audio object samples in such a way that\nallows audiences to easily individualize their chosen audio sources according\nto their interests and needs. SSOBA is an extra service and not an alternative,\nso it is also compliant with legacy audio players. The biggest advantage of\nSSOBA is that it does not require any special additional hardware in the\nbroadcasting chain and it is therefore easy to implement and equip legacy\nplayers and decoders with enhanced ability. Input audio objects, number of\noutput channels and sampling rates are three important factors affecting SSOBA\nperformance and specifying it to be lossless or lossy. SSOBA adopts\ninterpolation at the decoder side to compensate for eliminated samples. Both\nsubjective and objective experiments are carried out to evaluate the output\nresults at each step. MUSHRA subjective experiments conducted after the\nencoding step shows good-quality performance of SSOBA with up to five objects.\nSNR measurements and objective experiments, performed after decoding and\ninterpolation, show significant successful recovery and separation of audio\nobjects. Experimental results show that a minimum sampling rate of 96 kHz is\nindicated to encode up to five objects in a Stereo-mode channel to acquire good\nsubjective and objective results simultaneously.\n","authors":["Mohammad Reza Hasanabadi"],"pdf_url":"https://arxiv.org/pdf/2310.16481v1.pdf","comment":"Accepted in ABU Technical Review Journal 2020/9"},{"id":"http://arxiv.org/abs/2306.02858v4","updated":"2023-10-25T06:23:31Z","published":"2023-06-05T13:17:27Z","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video\n  Understanding","summary":"  We present Video-LLaMA a multi-modal framework that empowers Large Language\nModels (LLMs) with the capability of understanding both visual and auditory\ncontent in the video. Video-LLaMA bootstraps cross-modal training from the\nfrozen pre-trained visual and audio encoders and the frozen LLMs. Unlike\nprevious works that complement LLMs to process the visual or audio signals\nonly, Video-LLaMA enables video comprehension by tackling two challenges: (1)\ncapturing the temporal changes in visual scenes, (2) integrating audio-visual\nsignals. To counter the first challenge, we propose a Video Q-former to\nassemble a pre-trained image encoder into our video encoder and introduce a\nvideo-to-text generation task to learn video-language correspondence. For the\nsecond challenge, we leverage ImageBind, a universal embedding model aligning\nmultiple modalities, as the pre-trained audio encoder and introduce an Audio\nQ-former on top of ImageBind to learn reasonable auditory query embeddings for\nthe LLM module. To align the output of both visual and audio encoders with\nLLM's embedding space, we first train Video-LLaMA on massive\nvideo/image-caption pairs and then tune our model with visual-instruction\ndatasets of moderate amount but higher quality. We found Video-LLaMA shows the\nability to perceive and comprehend video content and generate meaningful\nresponses grounded in the visual and auditory information presented in the\nvideos.\n","authors":["Hang Zhang","Xin Li","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2306.02858v4.pdf","comment":"Accepted by EMNLP 2023's demo track; Code, Pretrained Model, and\n  Dataset: https://github.com/DAMO-NLP-SG/Video-LLaMA"},{"id":"http://arxiv.org/abs/2310.16367v1","updated":"2023-10-25T05:12:05Z","published":"2023-10-25T05:12:05Z","title":"UniX-Encoder: A Universal $X$-Channel Speech Encoder for Ad-Hoc\n  Microphone Array Speech Processing","summary":"  The speech field is evolving to solve more challenging scenarios, such as\nmulti-channel recordings with multiple simultaneous talkers. Given the many\ntypes of microphone setups out there, we present the UniX-Encoder. It's a\nuniversal encoder designed for multiple tasks, and worked with any microphone\narray, in both solo and multi-talker environments. Our research enhances\nprevious multi-channel speech processing efforts in four key areas: 1)\nAdaptability: Contrasting traditional models constrained to certain microphone\narray configurations, our encoder is universally compatible. 2) Multi-Task\nCapability: Beyond the single-task focus of previous systems, UniX-Encoder acts\nas a robust upstream model, adeptly extracting features for diverse tasks\nincluding ASR and speaker recognition. 3) Self-Supervised Training: The encoder\nis trained without requiring labeled multi-channel data. 4) End-to-End\nIntegration: In contrast to models that first beamform then process\nsingle-channels, our encoder offers an end-to-end solution, bypassing explicit\nbeamforming or separation. To validate its effectiveness, we tested the\nUniX-Encoder on a synthetic multi-channel dataset from the LibriSpeech corpus.\nAcross tasks like speech recognition and speaker diarization, our encoder\nconsistently outperformed combinations like the WavLM model with the BeamformIt\nfrontend.\n","authors":["Zili Huang","Yiwen Shao","Shi-Xiong Zhang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2310.16367v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.16338v1","updated":"2023-10-25T03:40:50Z","published":"2023-10-25T03:40:50Z","title":"Generative Pre-training for Speech with Flow Matching","summary":"  Generative models have gained more and more attention in recent years for\ntheir remarkable success in tasks that required estimating and sampling data\ndistribution to generate high-fidelity synthetic data. In speech,\ntext-to-speech synthesis and neural vocoder are good examples where generative\nmodels have shined. While generative models have been applied to different\napplications in speech, there exists no general-purpose generative model that\nmodels speech directly. In this work, we take a step toward this direction by\nshowing a single pre-trained generative model can be adapted to different\ndownstream tasks with strong performance. Specifically, we pre-trained a\ngenerative model, named SpeechFlow, on 60k hours of untranscribed speech with\nFlow Matching and masked conditions. Experiment results show the pre-trained\ngenerative model can be fine-tuned with task-specific data to match or surpass\nexisting expert models on speech enhancement, separation, and synthesis. Our\nwork suggested a foundational model for generation tasks in speech can be built\nwith generative pre-training.\n","authors":["Alexander H. Liu","Matt Le","Apoorv Vyas","Bowen Shi","Andros Tjandra","Wei-Ning Hsu"],"pdf_url":"https://arxiv.org/pdf/2310.16338v1.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2310.16334v1","updated":"2023-10-25T03:30:37Z","published":"2023-10-25T03:30:37Z","title":"AccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style\n  Transfer and Multi-Track Function Prior","summary":"  We propose AccoMontage-3, a symbolic music automation system capable of\ngenerating multi-track, full-band accompaniment based on the input of a lead\nmelody with chords (i.e., a lead sheet). The system contains three modular\ncomponents, each modelling a vital aspect of full-band composition. The first\ncomponent is a piano arranger that generates piano accompaniment for the lead\nsheet by transferring texture styles to the chords using latent chord-texture\ndisentanglement and heuristic retrieval of texture donors. The second component\norchestrates the piano accompaniment score into full-band arrangement according\nto the orchestration style encoded by individual track functions. The third\ncomponent, which connects the previous two, is a prior model characterizing the\nglobal structure of orchestration style over the whole piece of music. From end\nto end, the system learns to generate full-band accompaniment in a\nself-supervised fashion, applying style transfer at two levels of polyphonic\ncomposition: texture and orchestration. Experiments show that our system\noutperforms the baselines significantly, and the modular design offers\neffective controls in a musically meaningful way.\n","authors":["Jingwei Zhao","Gus Xia","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16327v1","updated":"2023-10-25T03:19:08Z","published":"2023-10-25T03:19:08Z","title":"Covariance Blocking and Whitening Method for Successive Relative\n  Transfer Function Vector Estimation in Multi-Speaker Scenarios","summary":"  This paper addresses the challenge of estimating the relative transfer\nfunction (RTF) vectors of multiple speakers in a noisy and reverberant\nenvironment. More specifically, we consider a scenario where two speakers\nactivate successively. In this scenario, the RTF vector of the first speaker\ncan be estimated in a straightforward way and the main challenge lies in\nestimating the RTF vector of the second speaker during segments where both\nspeakers are simultaneously active. To estimate the RTF vector of the second\nspeaker the so-called blind oblique projection (BOP) method determines the\noblique projection operator that optimally blocks the second speaker. Instead\nof blocking the second speaker, in this paper we propose a covariance blocking\nand whitening (CBW) method, which first blocks the first speaker and applies\nwhitening using the estimated noise covariance matrix and then estimates the\nRTF vector of the second speaker based on a singular value decomposition. When\nusing the estimated RTF vectors of both speakers in a linearly constrained\nminimum variance beamformer, simulation results using real-world recordings for\nmultiple speaker positions demonstrate that the proposed CBW method outperforms\nthe conventional BOP and covariance whitening methods in terms of\nsignal-to-interferer-and-noise ratio improvement.\n","authors":["Henri Gode","Simon Doclo"],"pdf_url":"https://arxiv.org/pdf/2310.16327v1.pdf","comment":"IEEE Workshop on Applications of Signal Processing to Audio and\n  Acoustics (WASPAA), New Paltz, NY, USA, Oct 22-25, 2023"},{"id":"http://arxiv.org/abs/2310.16287v1","updated":"2023-10-25T01:45:33Z","published":"2023-10-25T01:45:33Z","title":"Towards Streaming Speech-to-Avatar Synthesis","summary":"  Streaming speech-to-avatar synthesis creates real-time animations for a\nvirtual character from audio data. Accurate avatar representations of speech\nare important for the visualization of sound in linguistics, phonetics, and\nphonology, visual feedback to assist second language acquisition, and virtual\nembodiment for paralyzed patients. Previous works have highlighted the\ncapability of deep articulatory inversion to perform high-quality avatar\nanimation using electromagnetic articulography (EMA) features. However, these\nmodels focus on offline avatar synthesis with recordings rather than real-time\naudio, which is necessary for live avatar visualization or embodiment. To\naddress this issue, we propose a method using articulatory inversion for\nstreaming high quality facial and inner-mouth avatar animation from real-time\naudio. Our approach achieves 130ms average streaming latency for every 0.1\nseconds of audio with a 0.792 correlation with ground truth articulations.\nFinally, we show generated mouth and tongue animations to demonstrate the\nefficacy of our methodology.\n","authors":["Tejas S. Prabhune","Peter Wu","Bohan Yu","Gopala K. Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2310.16287v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.17049v1","updated":"2023-10-25T23:21:46Z","published":"2023-10-25T23:21:46Z","title":"Learning Repeatable Speech Embeddings Using An Intra-class Correlation\n  Regularizer","summary":"  A good supervised embedding for a specific machine learning task is only\nsensitive to changes in the label of interest and is invariant to other\nconfounding factors. We leverage the concept of repeatability from measurement\ntheory to describe this property and propose to use the intra-class correlation\ncoefficient (ICC) to evaluate the repeatability of embeddings. We then propose\na novel regularizer, the ICC regularizer, as a complementary component for\ncontrastive losses to guide deep neural networks to produce embeddings with\nhigher repeatability. We use simulated data to explain why the ICC regularizer\nworks better on minimizing the intra-class variance than the contrastive loss\nalone. We implement the ICC regularizer and apply it to three speech tasks:\nspeaker verification, voice style conversion, and a clinical application for\ndetecting dysphonic voice. The experimental results demonstrate that adding an\nICC regularizer can improve the repeatability of learned embeddings compared to\nonly using the contrastive loss; further, these embeddings lead to improved\nperformance in these downstream tasks.\n","authors":["Jianwei Zhang","Suren Jayasuriya","Visar Berisha"],"pdf_url":"https://arxiv.org/pdf/2310.17049v1.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.17004v1","updated":"2023-10-25T21:05:13Z","published":"2023-10-25T21:05:13Z","title":"Improved Panning on Non-Equidistant Loudspeakers with Direct Sound Level\n  Compensation","summary":"  Loudspeaker rendering techniques that create phantom sound sources often\nassume an equidistant loudspeaker layout. Typical home setups might not fulfill\nthis condition as loudspeakers deviate from canonical positions, thus requiring\na corresponding calibration. The standard approach is to compensate for delays\nand to match the loudness of each loudspeaker at the listener's location.It was\nfound that a shift of the phantom image occurs when this calibration procedure\nis applied and one of a pair of loudspeakers is significantly closer to the\nlistener than the other. In this paper, a novel approach to panning on\nnon-equidistant loudspeaker layouts is presented whereby the panning position\nis governed by the direct sound and the perceived loudness is governed by the\nfull impulse response. Subjective listening tests are presented that validate\nthe approach and quantify the perceived effect of the compensation. In a setup\nwhere the standard calibration leads to an average error of 10 degrees, the\nproposed direct sound compensation largely returns the phantom source to its\nintended position.\n","authors":["Jan-Hendrik Hanschke","Daniel Arteaga","Giulio Cengarle","Joshua Lando","Mark R. P. Thomas","Alan Seefeldt"],"pdf_url":"https://arxiv.org/pdf/2310.17004v1.pdf","comment":"10 pages. Accepted for presentation in AES Convention 155 (2023)"}]},"2023-10-26T00:00:00Z":{"Sound":[{"id":"http://arxiv.org/abs/2310.17558v1","updated":"2023-10-26T16:47:52Z","published":"2023-10-26T16:47:52Z","title":"Towards Matching Phones and Speech Representations","summary":"  Learning phone types from phone instances has been a long-standing problem,\nwhile still being open. In this work, we revisit this problem in the context of\nself-supervised learning, and pose it as the problem of matching cluster\ncentroids to phone embeddings. We study two key properties that enable\nmatching, namely, whether cluster centroids of self-supervised representations\nreduce the variability of phone instances and respect the relationship among\nphones. We then use the matching result to produce pseudo-labels and introduce\na new loss function for improving self-supervised representations. Our\nexperiments show that the matching result captures the relationship among\nphones. Training the new loss function jointly with the regular self-supervised\nlosses, such as APC and CPC, significantly improves the downstream phone\nclassification.\n","authors":["Gene-Ping Yang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2310.17558v1.pdf","comment":"Accepted to ASRU 2023"},{"id":"http://arxiv.org/abs/2310.17502v1","updated":"2023-10-26T15:54:12Z","published":"2023-10-26T15:54:12Z","title":"Controllable Generation of Artificial Speaker Embeddings through\n  Discovery of Principal Directions","summary":"  Customizing voice and speaking style in a speech synthesis system with\nintuitive and fine-grained controls is challenging, given that little data with\nappropriate labels is available. Furthermore, editing an existing human's voice\nalso comes with ethical concerns. In this paper, we propose a method to\ngenerate artificial speaker embeddings that cannot be linked to a real human\nwhile offering intuitive and fine-grained control over the voice and speaking\nstyle of the embeddings, without requiring any labels for speaker or style. The\nartificial and controllable embeddings can be fed to a speech synthesis system,\nconditioned on embeddings of real humans during training, without sacrificing\nprivacy during inference.\n","authors":["Florian Lux","Pascal Tilli","Sarina Meyer","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2310.17502v1.pdf","comment":"Published at ISCA Interspeech 2023\n  https://www.isca-speech.org/archive/interspeech_2023/lux23_interspeech.html"},{"id":"http://arxiv.org/abs/2211.00990v2","updated":"2023-10-26T11:47:25Z","published":"2022-11-02T09:51:15Z","title":"A weighted-variance variational autoencoder model for speech enhancement","summary":"  We address speech enhancement based on variational autoencoders, which\ninvolves learning a speech prior distribution in the time-frequency (TF)\ndomain. A zero-mean complex-valued Gaussian distribution is usually assumed for\nthe generative model, where the speech information is encoded in the variance\nas a function of a latent variable. In contrast to this commonly used approach,\nwe propose a weighted variance generative model, where the contribution of each\nspectrogram time-frame in parameter learning is weighted. We impose a Gamma\nprior distribution on the weights, which would effectively lead to a Student's\nt-distribution instead of Gaussian for speech generative modeling. We develop\nefficient training and speech enhancement algorithms based on the proposed\ngenerative model. Our experimental results on spectrogram auto-encoding and\nspeech enhancement demonstrate the effectiveness and robustness of the proposed\napproach compared to the standard unweighted variance model.\n","authors":["Ali Golmakani","Mostafa Sadeghi","Xavier Alameda-Pineda","Romain Serizel"],"pdf_url":"https://arxiv.org/pdf/2211.00990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17162v1","updated":"2023-10-26T05:24:38Z","published":"2023-10-26T05:24:38Z","title":"Content-based Controls For Music Large Language Modeling","summary":"  Recent years have witnessed a rapid growth of large-scale language models in\nthe domain of music audio. Such models enable end-to-end generation of\nhigher-quality music, and some allow conditioned generation using text\ndescriptions. However, the control power of text controls on music is\nintrinsically limited, as they can only describe music indirectly through\nmeta-data (such as singers and instruments) or high-level representations (such\nas genre and emotion). We aim to further equip the models with direct and\ncontent-based controls on innate music languages such as pitch, chords and drum\ntrack. To this end, we contribute Coco-Mulla, a content-based control method\nfor music large language modeling. It uses a parameter-efficient fine-tuning\n(PEFT) method tailored for Transformer-based audio models. Experiments show\nthat our approach achieved high-quality music generation with low-resource\nsemi-supervised learning, tuning with less than 4% parameters compared to the\noriginal model and training on a small dataset with fewer than 300 songs.\nMoreover, our approach enables effective content-based controls, and we\nillustrate the control power via chords and rhythms, two of the most salient\nfeatures of music audio. Furthermore, we show that by combining content-based\ncontrols and text descriptions, our system achieves flexible music variation\ngeneration and style transfer. Our source codes and demos are available online.\n","authors":["Liwei Lin","Gus Xia","Junyan Jiang","Yixiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.17162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17142v1","updated":"2023-10-26T04:29:27Z","published":"2023-10-26T04:29:27Z","title":"Single channel speech enhancement by colored spectrograms","summary":"  Speech enhancement concerns the processes required to remove unwanted\nbackground sounds from the target speech to improve its quality and\nintelligibility. In this paper, a novel approach for single-channel speech\nenhancement is presented, using colored spectrograms. We propose the use of a\ndeep neural network (DNN) architecture adapted from the pix2pix generative\nadversarial network (GAN) and train it over colored spectrograms of speech to\ndenoise them. After denoising, the colors of spectrograms are translated to\nmagnitudes of short-time Fourier transform (STFT) using a shallow regression\nneural network. These estimated STFT magnitudes are later combined with the\nnoisy phases to obtain an enhanced speech. The results show an improvement of\nalmost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1%\nin the short-term objective intelligibility (STOI) over the unprocessed noisy\ndata. The gain in quality and intelligibility over the unprocessed signal is\nalmost equal to the gain achieved by the baseline methods used for comparison\nwith the proposed model, but at a much reduced computational cost. The proposed\nsolution offers a comparative PESQ score at almost 10 times reduced\ncomputational cost than a similar baseline model that has generated the highest\nPESQ score trained on grayscaled spectrograms, while it provides only a 1%\ndeficit in STOI at 28 times reduced computational cost when compared to another\nbaseline system based on convolutional neural network-GAN (CNN-GAN) that\nproduces the most intelligible speech.\n","authors":["Sania Gul","Muhammad Salman Khan","Muhammad Fazeel"],"pdf_url":"https://arxiv.org/pdf/2310.17142v1.pdf","comment":"18 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2309.08200v2","updated":"2023-10-26T03:24:31Z","published":"2023-09-15T07:05:29Z","title":"TF-SepNet: An Efficient 1D Kernel Design in CNNs for Low-Complexity\n  Acoustic Scene Classification","summary":"  Recent studies focus on developing efficient systems for acoustic scene\nclassification (ASC) using convolutional neural networks (CNNs), which\ntypically consist of consecutive kernels. This paper highlights the benefits of\nusing separate kernels as a more powerful and efficient design approach in ASC\ntasks. Inspired by the time-frequency nature of audio signals, we propose\nTF-SepNet, a CNN architecture that separates the feature processing along the\ntime and frequency dimensions. Features resulted from the separate paths are\nthen merged by channels and directly forwarded to the classifier. Instead of\nthe conventional two dimensional (2D) kernel, TF-SepNet incorporates one\ndimensional (1D) kernels to reduce the computational costs. Experiments have\nbeen conducted using the TAU Urban Acoustic Scene 2022 Mobile development\ndataset. The results show that TF-SepNet outperforms similar state-of-the-arts\nthat use consecutive kernels. A further investigation reveals that the separate\nkernels lead to a larger effective receptive field (ERF), which enables\nTF-SepNet to capture more time-frequency features.\n","authors":["Yiqiang Cai","Peihong Zhang","Shengchen Li"],"pdf_url":"https://arxiv.org/pdf/2309.08200v2.pdf","comment":"Submitted to the 2024 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2024)"},{"id":"http://arxiv.org/abs/2310.17116v1","updated":"2023-10-26T03:05:40Z","published":"2023-10-26T03:05:40Z","title":"Real-time Neonatal Chest Sound Separation using Deep Learning","summary":"  Auscultation for neonates is a simple and non-invasive method of providing\ndiagnosis for cardiovascular and respiratory disease. Such diagnosis often\nrequires high-quality heart and lung sounds to be captured during auscultation.\nHowever, in most cases, obtaining such high-quality sounds is non-trivial due\nto the chest sounds containing a mixture of heart, lung, and noise sounds. As\nsuch, additional preprocessing is needed to separate the chest sounds into\nheart and lung sounds. This paper proposes a novel deep-learning approach to\nseparate such chest sounds into heart and lung sounds. Inspired by the\nConv-TasNet model, the proposed model has an encoder, decoder, and mask\ngenerator. The encoder consists of a 1D convolution model and the decoder\nconsists of a transposed 1D convolution. The mask generator is constructed\nusing stacked 1D convolutions and transformers. The proposed model outperforms\nprevious methods in terms of objective distortion measures by 2.01 dB to 5.06\ndB in the artificial dataset, as well as computation time, with at least a\n17-time improvement. Therefore, our proposed model could be a suitable\npreprocessing step for any phonocardiogram-based health monitoring system.\n","authors":["Yang Yi Poh","Ethan Grooby","Kenneth Tan","Lindsay Zhou","Arrabella King","Ashwin Ramanathan","Atul Malhotra","Mehrtash Harandi","Faezeh Marzbanrad"],"pdf_url":"https://arxiv.org/pdf/2310.17116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17101v1","updated":"2023-10-26T01:58:38Z","published":"2023-10-26T01:58:38Z","title":"Multi-Speaker Expressive Speech Synthesis via Semi-supervised\n  Contrastive Learning","summary":"  This paper aims to build an expressive TTS system for multi-speakers,\nsynthesizing a target speaker's speech with multiple styles and emotions. To\nthis end, we propose a novel contrastive learning-based TTS approach to\ntransfer style and emotion across speakers. Specifically, we construct\npositive-negative sample pairs at both utterance and category (such as\nemotion-happy or style-poet or speaker A) levels and leverage contrastive\nlearning to better extract disentangled style, emotion, and speaker\nrepresentations from speech. Furthermore, we introduce a semi-supervised\ntraining strategy to the proposed approach to effectively leverage multi-domain\ndata, including style-labeled data, emotion-labeled data, and unlabeled data.\nWe integrate the learned representations into an improved VITS model, enabling\nit to synthesize expressive speech with diverse styles and emotions for a\ntarget speaker. Experiments on multi-domain data demonstrate the good design of\nour model.\n","authors":["Xinfa Zhu","Yuke Li","Yi Lei","Ning Jiang","Guoqing Zhao","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2310.17101v1.pdf","comment":"5 pages, 3 figures"}],"Audio and Speech Processing":[{"id":"http://arxiv.org/abs/2310.17558v1","updated":"2023-10-26T16:47:52Z","published":"2023-10-26T16:47:52Z","title":"Towards Matching Phones and Speech Representations","summary":"  Learning phone types from phone instances has been a long-standing problem,\nwhile still being open. In this work, we revisit this problem in the context of\nself-supervised learning, and pose it as the problem of matching cluster\ncentroids to phone embeddings. We study two key properties that enable\nmatching, namely, whether cluster centroids of self-supervised representations\nreduce the variability of phone instances and respect the relationship among\nphones. We then use the matching result to produce pseudo-labels and introduce\na new loss function for improving self-supervised representations. Our\nexperiments show that the matching result captures the relationship among\nphones. Training the new loss function jointly with the regular self-supervised\nlosses, such as APC and CPC, significantly improves the downstream phone\nclassification.\n","authors":["Gene-Ping Yang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2310.17558v1.pdf","comment":"Accepted to ASRU 2023"},{"id":"http://arxiv.org/abs/2310.17502v1","updated":"2023-10-26T15:54:12Z","published":"2023-10-26T15:54:12Z","title":"Controllable Generation of Artificial Speaker Embeddings through\n  Discovery of Principal Directions","summary":"  Customizing voice and speaking style in a speech synthesis system with\nintuitive and fine-grained controls is challenging, given that little data with\nappropriate labels is available. Furthermore, editing an existing human's voice\nalso comes with ethical concerns. In this paper, we propose a method to\ngenerate artificial speaker embeddings that cannot be linked to a real human\nwhile offering intuitive and fine-grained control over the voice and speaking\nstyle of the embeddings, without requiring any labels for speaker or style. The\nartificial and controllable embeddings can be fed to a speech synthesis system,\nconditioned on embeddings of real humans during training, without sacrificing\nprivacy during inference.\n","authors":["Florian Lux","Pascal Tilli","Sarina Meyer","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2310.17502v1.pdf","comment":"Published at ISCA Interspeech 2023\n  https://www.isca-speech.org/archive/interspeech_2023/lux23_interspeech.html"},{"id":"http://arxiv.org/abs/2310.17499v1","updated":"2023-10-26T15:53:29Z","published":"2023-10-26T15:53:29Z","title":"The IMS Toucan System for the Blizzard Challenge 2023","summary":"  For our contribution to the Blizzard Challenge 2023, we improved on the\nsystem we submitted to the Blizzard Challenge 2021. Our approach entails a\nrule-based text-to-phoneme processing system that includes rule-based\ndisambiguation of homographs in the French language. It then transforms the\nphonemes to spectrograms as intermediate representations using a fast and\nefficient non-autoregressive synthesis architecture based on Conformer and\nGlow. A GAN based neural vocoder that combines recent state-of-the-art\napproaches converts the spectrogram to the final wave. We carefully designed\nthe data processing, training, and inference procedures for the challenge data.\nOur system identifier is G. Open source code and demo are available.\n","authors":["Florian Lux","Julia Koch","Sarina Meyer","Thomas Bott","Nadja Schauffler","Pavel Denisov","Antje Schweitzer","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2310.17499v1.pdf","comment":"Published at the Blizzard Challenge Workshop 2023, colocated with the\n  Speech Synthesis Workshop 2023, a sattelite event of the Interspeech 2023"},{"id":"http://arxiv.org/abs/2310.17448v1","updated":"2023-10-26T14:57:08Z","published":"2023-10-26T14:57:08Z","title":"Dialect Adaptation and Data Augmentation for Low-Resource ASR: TalTech\n  Systems for the MADASR 2023 Challenge","summary":"  This paper describes Tallinn University of Technology (TalTech) systems\ndeveloped for the ASRU MADASR 2023 Challenge. The challenge focuses on\nautomatic speech recognition of dialect-rich Indian languages with limited\ntraining audio and text data. TalTech participated in two tracks of the\nchallenge: Track 1 that allowed using only the provided training data and Track\n3 which allowed using additional audio data. In both tracks, we relied on\nwav2vec2.0 models. Our methodology diverges from the traditional procedure of\nfinetuning pretrained wav2vec2.0 models in two key points: firstly, through the\nimplementation of the aligned data augmentation technique to enhance the\nlinguistic diversity of the training data, and secondly, via the application of\ndeep prefix tuning for dialect adaptation of wav2vec2.0 models. In both tracks,\nour approach yielded significant improvements over the provided baselines,\nachieving the lowest word error rates across all participating teams.\n","authors":["Tanel Alum√§e","Jiaming Kong","Daniil Robnikov"],"pdf_url":"https://arxiv.org/pdf/2310.17448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11618v2","updated":"2023-10-26T11:55:28Z","published":"2023-01-27T09:39:50Z","title":"Five ways to recover the symbol of a non-binary localization operator","summary":"  Five constructive methods for recovering the symbol of a time-frequency\nlocalization operator with non-binary symbol are presented, two based on\nearlier work and three novel methods. For the two derivative methods which have\npreviously been applied to binary symbols, we propose a changed symbol\nestimator and provide additional estimates that show how we can recover\nnon-binary symbols. The three novel methods each have their own advantages and\nare all applicable to non-binary symbols. Two of them rely on prescribing the\ninput of the localization operator and examining the output, allowing for\ntargeting of the part of the symbol one wishes to recover while the last one\nrelies on spectral information about the operator. All five methods are also\nimplemented numerically and evaluated with the code available.\n","authors":["Simon Halvdansson"],"pdf_url":"https://arxiv.org/pdf/2301.11618v2.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2211.00990v2","updated":"2023-10-26T11:47:25Z","published":"2022-11-02T09:51:15Z","title":"A weighted-variance variational autoencoder model for speech enhancement","summary":"  We address speech enhancement based on variational autoencoders, which\ninvolves learning a speech prior distribution in the time-frequency (TF)\ndomain. A zero-mean complex-valued Gaussian distribution is usually assumed for\nthe generative model, where the speech information is encoded in the variance\nas a function of a latent variable. In contrast to this commonly used approach,\nwe propose a weighted variance generative model, where the contribution of each\nspectrogram time-frame in parameter learning is weighted. We impose a Gamma\nprior distribution on the weights, which would effectively lead to a Student's\nt-distribution instead of Gaussian for speech generative modeling. We develop\nefficient training and speech enhancement algorithms based on the proposed\ngenerative model. Our experimental results on spectrogram auto-encoding and\nspeech enhancement demonstrate the effectiveness and robustness of the proposed\napproach compared to the standard unweighted variance model.\n","authors":["Ali Golmakani","Mostafa Sadeghi","Xavier Alameda-Pineda","Romain Serizel"],"pdf_url":"https://arxiv.org/pdf/2211.00990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11685v2","updated":"2023-10-26T10:43:07Z","published":"2023-05-19T14:07:43Z","title":"Recycle-and-Distill: Universal Compression Strategy for\n  Transformer-based Speech SSL Models with Attention Map Reusing and Masking\n  Distillation","summary":"  Transformer-based speech self-supervised learning (SSL) models, such as\nHuBERT, show surprising performance in various speech processing tasks.\nHowever, huge number of parameters in speech SSL models necessitate the\ncompression to a more compact model for wider usage in academia or small\ncompanies. In this study, we suggest to reuse attention maps across the\nTransformer layers, so as to remove key and query parameters while retaining\nthe number of layers. Furthermore, we propose a novel masking distillation\nstrategy to improve the student model's speech representation quality. We\nextend the distillation loss to utilize both masked and unmasked speech frames\nto fully leverage the teacher model's high-quality representation. Our\nuniversal compression strategy yields the student model that achieves phoneme\nerror rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB\nbenchmark.\n","authors":["Kangwook Jang","Sungnyun Kim","Se-Young Yun","Hoirin Kim"],"pdf_url":"https://arxiv.org/pdf/2305.11685v2.pdf","comment":"Proceedings of Interspeech 2023. Code URL:\n  https://github.com/sungnyun/ARMHuBERT"},{"id":"http://arxiv.org/abs/2310.17194v1","updated":"2023-10-26T07:20:23Z","published":"2023-10-26T07:20:23Z","title":"Privacy-preserving Representation Learning for Speech Understanding","summary":"  Existing privacy-preserving speech representation learning methods target a\nsingle application domain. In this paper, we present a novel framework to\nanonymize utterance-level speech embeddings generated by pre-trained encoders\nand show its effectiveness for a range of speech classification tasks.\nSpecifically, given the representations from a pre-trained encoder, we train a\nTransformer to estimate the representations for the same utterances spoken by\nother speakers. During inference, the extracted representations can be\nconverted into different identities to preserve privacy. We compare the results\nwith the voice anonymization baselines from the VoicePrivacy 2022 challenge. We\nevaluate our framework on speaker identification for privacy and emotion\nrecognition, depression classification, and intent classification for utility.\nOur method outperforms the baselines on privacy and utility in paralinguistic\ntasks and achieves comparable performance for intent classification.\n","authors":["Minh Tran","Mohammad Soleymani"],"pdf_url":"https://arxiv.org/pdf/2310.17194v1.pdf","comment":"INTERSPEECH 2023"},{"id":"http://arxiv.org/abs/2310.17162v1","updated":"2023-10-26T05:24:38Z","published":"2023-10-26T05:24:38Z","title":"Content-based Controls For Music Large Language Modeling","summary":"  Recent years have witnessed a rapid growth of large-scale language models in\nthe domain of music audio. Such models enable end-to-end generation of\nhigher-quality music, and some allow conditioned generation using text\ndescriptions. However, the control power of text controls on music is\nintrinsically limited, as they can only describe music indirectly through\nmeta-data (such as singers and instruments) or high-level representations (such\nas genre and emotion). We aim to further equip the models with direct and\ncontent-based controls on innate music languages such as pitch, chords and drum\ntrack. To this end, we contribute Coco-Mulla, a content-based control method\nfor music large language modeling. It uses a parameter-efficient fine-tuning\n(PEFT) method tailored for Transformer-based audio models. Experiments show\nthat our approach achieved high-quality music generation with low-resource\nsemi-supervised learning, tuning with less than 4% parameters compared to the\noriginal model and training on a small dataset with fewer than 300 songs.\nMoreover, our approach enables effective content-based controls, and we\nillustrate the control power via chords and rhythms, two of the most salient\nfeatures of music audio. Furthermore, we show that by combining content-based\ncontrols and text descriptions, our system achieves flexible music variation\ngeneration and style transfer. Our source codes and demos are available online.\n","authors":["Liwei Lin","Gus Xia","Junyan Jiang","Yixiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.17162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17142v1","updated":"2023-10-26T04:29:27Z","published":"2023-10-26T04:29:27Z","title":"Single channel speech enhancement by colored spectrograms","summary":"  Speech enhancement concerns the processes required to remove unwanted\nbackground sounds from the target speech to improve its quality and\nintelligibility. In this paper, a novel approach for single-channel speech\nenhancement is presented, using colored spectrograms. We propose the use of a\ndeep neural network (DNN) architecture adapted from the pix2pix generative\nadversarial network (GAN) and train it over colored spectrograms of speech to\ndenoise them. After denoising, the colors of spectrograms are translated to\nmagnitudes of short-time Fourier transform (STFT) using a shallow regression\nneural network. These estimated STFT magnitudes are later combined with the\nnoisy phases to obtain an enhanced speech. The results show an improvement of\nalmost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1%\nin the short-term objective intelligibility (STOI) over the unprocessed noisy\ndata. The gain in quality and intelligibility over the unprocessed signal is\nalmost equal to the gain achieved by the baseline methods used for comparison\nwith the proposed model, but at a much reduced computational cost. The proposed\nsolution offers a comparative PESQ score at almost 10 times reduced\ncomputational cost than a similar baseline model that has generated the highest\nPESQ score trained on grayscaled spectrograms, while it provides only a 1%\ndeficit in STOI at 28 times reduced computational cost when compared to another\nbaseline system based on convolutional neural network-GAN (CNN-GAN) that\nproduces the most intelligible speech.\n","authors":["Sania Gul","Muhammad Salman Khan","Muhammad Fazeel"],"pdf_url":"https://arxiv.org/pdf/2310.17142v1.pdf","comment":"18 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2309.08200v2","updated":"2023-10-26T03:24:31Z","published":"2023-09-15T07:05:29Z","title":"TF-SepNet: An Efficient 1D Kernel Design in CNNs for Low-Complexity\n  Acoustic Scene Classification","summary":"  Recent studies focus on developing efficient systems for acoustic scene\nclassification (ASC) using convolutional neural networks (CNNs), which\ntypically consist of consecutive kernels. This paper highlights the benefits of\nusing separate kernels as a more powerful and efficient design approach in ASC\ntasks. Inspired by the time-frequency nature of audio signals, we propose\nTF-SepNet, a CNN architecture that separates the feature processing along the\ntime and frequency dimensions. Features resulted from the separate paths are\nthen merged by channels and directly forwarded to the classifier. Instead of\nthe conventional two dimensional (2D) kernel, TF-SepNet incorporates one\ndimensional (1D) kernels to reduce the computational costs. Experiments have\nbeen conducted using the TAU Urban Acoustic Scene 2022 Mobile development\ndataset. The results show that TF-SepNet outperforms similar state-of-the-arts\nthat use consecutive kernels. A further investigation reveals that the separate\nkernels lead to a larger effective receptive field (ERF), which enables\nTF-SepNet to capture more time-frequency features.\n","authors":["Yiqiang Cai","Peihong Zhang","Shengchen Li"],"pdf_url":"https://arxiv.org/pdf/2309.08200v2.pdf","comment":"Submitted to the 2024 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2024)"},{"id":"http://arxiv.org/abs/2310.17116v1","updated":"2023-10-26T03:05:40Z","published":"2023-10-26T03:05:40Z","title":"Real-time Neonatal Chest Sound Separation using Deep Learning","summary":"  Auscultation for neonates is a simple and non-invasive method of providing\ndiagnosis for cardiovascular and respiratory disease. Such diagnosis often\nrequires high-quality heart and lung sounds to be captured during auscultation.\nHowever, in most cases, obtaining such high-quality sounds is non-trivial due\nto the chest sounds containing a mixture of heart, lung, and noise sounds. As\nsuch, additional preprocessing is needed to separate the chest sounds into\nheart and lung sounds. This paper proposes a novel deep-learning approach to\nseparate such chest sounds into heart and lung sounds. Inspired by the\nConv-TasNet model, the proposed model has an encoder, decoder, and mask\ngenerator. The encoder consists of a 1D convolution model and the decoder\nconsists of a transposed 1D convolution. The mask generator is constructed\nusing stacked 1D convolutions and transformers. The proposed model outperforms\nprevious methods in terms of objective distortion measures by 2.01 dB to 5.06\ndB in the artificial dataset, as well as computation time, with at least a\n17-time improvement. Therefore, our proposed model could be a suitable\npreprocessing step for any phonocardiogram-based health monitoring system.\n","authors":["Yang Yi Poh","Ethan Grooby","Kenneth Tan","Lindsay Zhou","Arrabella King","Ashwin Ramanathan","Atul Malhotra","Mehrtash Harandi","Faezeh Marzbanrad"],"pdf_url":"https://arxiv.org/pdf/2310.17116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17101v1","updated":"2023-10-26T01:58:38Z","published":"2023-10-26T01:58:38Z","title":"Multi-Speaker Expressive Speech Synthesis via Semi-supervised\n  Contrastive Learning","summary":"  This paper aims to build an expressive TTS system for multi-speakers,\nsynthesizing a target speaker's speech with multiple styles and emotions. To\nthis end, we propose a novel contrastive learning-based TTS approach to\ntransfer style and emotion across speakers. Specifically, we construct\npositive-negative sample pairs at both utterance and category (such as\nemotion-happy or style-poet or speaker A) levels and leverage contrastive\nlearning to better extract disentangled style, emotion, and speaker\nrepresentations from speech. Furthermore, we introduce a semi-supervised\ntraining strategy to the proposed approach to effectively leverage multi-domain\ndata, including style-labeled data, emotion-labeled data, and unlabeled data.\nWe integrate the learned representations into an improved VITS model, enabling\nit to synthesize expressive speech with diverse styles and emotions for a\ntarget speaker. Experiments on multi-domain data demonstrate the good design of\nour model.\n","authors":["Xinfa Zhu","Yuke Li","Yi Lei","Ning Jiang","Guoqing Zhao","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2310.17101v1.pdf","comment":"5 pages, 3 figures"}]}}